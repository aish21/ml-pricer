{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74f79724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras tensorflow --upgrade\n",
    "!{sys.executable} -m pip install keras_tuner\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f994c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/raw/training_data.npz\"\n",
    "SCALER_DIR = \"../data/processed/scalers\"\n",
    "MODEL_DIR = \"../src/models\"\n",
    "FIGURE_DIR = \"../src/visualization/plots\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17623456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: X shape (450000, 15), y shape (450000,)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path=DATA_PATH):\n",
    "    \"\"\"Load dataset from NPZ file.\"\"\"\n",
    "    data = np.load(path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    print(f\"Loaded dataset: X shape {X.shape}, y shape {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X, y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9110167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (360000, 15), Test set: (90000, 15)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits into train/test and normalizes features.\n",
    "    Prefix prices + numeric features are scaled.\n",
    "    opt_flag is kept as-is (categorical 0/1).\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Separate opt_flag (last column)\n",
    "    X_train_prefix = X_train[:, :-1]\n",
    "    X_test_prefix = X_test[:, :-1]\n",
    "\n",
    "    opt_flag_train = X_train[:, -1].reshape(-1, 1)\n",
    "    opt_flag_test = X_test[:, -1].reshape(-1, 1)\n",
    "\n",
    "    # Scale everything except opt_flag\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_prefix)\n",
    "    X_test_scaled = scaler.transform(X_test_prefix)\n",
    "\n",
    "    # Reattach opt_flag\n",
    "    X_train_final = np.hstack([X_train_scaled, opt_flag_train])\n",
    "    X_test_final = np.hstack([X_test_scaled, opt_flag_test])\n",
    "\n",
    "    # Save scaler for later inference\n",
    "    os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "    joblib.dump(scaler, os.path.join(SCALER_DIR, \"feature_scaler.pkl\"))\n",
    "\n",
    "    print(f\"Train set: {X_train_final.shape}, Test set: {X_test_final.shape}\")\n",
    "    return X_train_final, X_test_final, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cef6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1f90b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history, title, filename):\n",
    "    \"\"\"Save training curves for loss + MAE + RMSE.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "    plt.title(f\"{title} - MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    # RMSE plot (computed from loss)\n",
    "    train_rmse = np.sqrt(history.history[\"loss\"])\n",
    "    val_rmse = np.sqrt(history.history[\"val_loss\"])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_rmse, label=\"Train RMSE\")\n",
    "    plt.plot(val_rmse, label=\"Val RMSE\")\n",
    "    plt.title(f\"{title} - RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root MSE\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURE_DIR, filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0165c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - loss: 95.7670 - mae: 6.1171 - val_loss: 91.8340 - val_mae: 5.8506\n",
      "Epoch 2/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 738us/step - loss: 95.7670 - mae: 6.1171 - val_loss: 91.8340 - val_mae: 5.8506\n",
      "Epoch 2/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 721us/step - loss: 91.3797 - mae: 5.8531 - val_loss: 92.1265 - val_mae: 5.7729\n",
      "Epoch 3/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 721us/step - loss: 91.3797 - mae: 5.8531 - val_loss: 92.1265 - val_mae: 5.7729\n",
      "Epoch 3/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 705us/step - loss: 91.1778 - mae: 5.8427 - val_loss: 91.6412 - val_mae: 5.8700\n",
      "Epoch 4/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 705us/step - loss: 91.1778 - mae: 5.8427 - val_loss: 91.6412 - val_mae: 5.8700\n",
      "Epoch 4/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 719us/step - loss: 91.0327 - mae: 5.8291 - val_loss: 92.0664 - val_mae: 5.8646\n",
      "Epoch 5/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 719us/step - loss: 91.0327 - mae: 5.8291 - val_loss: 92.0664 - val_mae: 5.8646\n",
      "Epoch 5/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 722us/step - loss: 90.9774 - mae: 5.8278 - val_loss: 91.7659 - val_mae: 5.7835\n",
      "Epoch 6/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 722us/step - loss: 90.9774 - mae: 5.8278 - val_loss: 91.7659 - val_mae: 5.7835\n",
      "Epoch 6/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 719us/step - loss: 90.8755 - mae: 5.8186 - val_loss: 91.7280 - val_mae: 5.8050\n",
      "Epoch 7/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 719us/step - loss: 90.8755 - mae: 5.8186 - val_loss: 91.7280 - val_mae: 5.8050\n",
      "Epoch 7/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 715us/step - loss: 90.8496 - mae: 5.8162 - val_loss: 91.2763 - val_mae: 5.8235\n",
      "Epoch 8/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 715us/step - loss: 90.8496 - mae: 5.8162 - val_loss: 91.2763 - val_mae: 5.8235\n",
      "Epoch 8/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 721us/step - loss: 90.8194 - mae: 5.8150 - val_loss: 91.3873 - val_mae: 5.8949\n",
      "Epoch 9/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 721us/step - loss: 90.8194 - mae: 5.8150 - val_loss: 91.3873 - val_mae: 5.8949\n",
      "Epoch 9/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713us/step - loss: 90.8329 - mae: 5.8146 - val_loss: 91.1975 - val_mae: 5.8116\n",
      "Epoch 10/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713us/step - loss: 90.8329 - mae: 5.8146 - val_loss: 91.1975 - val_mae: 5.8116\n",
      "Epoch 10/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 725us/step - loss: 90.7524 - mae: 5.8125 - val_loss: 91.3137 - val_mae: 5.8443\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 725us/step - loss: 90.7524 - mae: 5.8125 - val_loss: 91.3137 - val_mae: 5.8443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 348us/step\n",
      "Evaluation on Test Set -> MAE: 5.8443, RMSE: 9.5558\n",
      "Evaluation on Test Set -> MAE: 5.8443, RMSE: 9.5558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(5.844283566354557), np.float64(9.555823780126518))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_quick_mlp(input_dim):\n",
    "    \"\"\"Small, quick baseline MLP.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Compute MAE and RMSE on test set.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred.flatten()) ** 2))\n",
    "    print(f\"Evaluation on Test Set -> MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "quick_mlp = build_quick_mlp(input_dim)\n",
    "history_quick = quick_mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "quick_mlp.save(os.path.join(MODEL_DIR, \"mlp_quick.h5\"))\n",
    "plot_training(history_quick, \"Quick MLP\", \"training_quick_run_2.png\")\n",
    "evaluate_model(quick_mlp, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc39133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_large_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Number of hidden layers\n",
    "    num_layers = hp.Int(\"num_layers\", min_value=2, max_value=6, step=1)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32)\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\", \"mse\", \"accuracy\"]  # include accuracy for monitoring\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed55014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_tuning\\tuner0.json\n",
      "  - num_layers: 3\n",
      "  - units_0: 96\n",
      "  - activation: relu\n",
      "  - units_1: 192\n",
      "  - optimizer: sgd\n",
      "  - lr: 0.0005822751269209618\n",
      "  - units_2: 352\n",
      "  - units_3: 320\n",
      "  - units_4: 384\n",
      "  - units_5: 352\n",
      "  - tuner/epochs: 6\n",
      "  - tuner/initial_epoch: 0\n",
      "  - tuner/bracket: 2\n",
      "  - tuner/round: 0\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_large_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=50,  # upper bound for epochs\n",
    "    factor=3,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_tuning\"\n",
    ")\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=kt.HyperParameters().Int(\"batch_size\", min_value=32, max_value=256, step=32),\n",
    "    epochs=50\n",
    ")\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "for hp in best_hps.values.keys():\n",
    "    print(f\"  - {hp}: {best_hps.get(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f0db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 992us/step - accuracy: 0.1544 - loss: 94.8747 - mae: 5.9900 - mse: 94.8747 - val_accuracy: 0.1673 - val_loss: 91.9165 - val_mae: 5.8146 - val_mse: 91.9165\n",
      "Epoch 2/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 992us/step - accuracy: 0.1544 - loss: 94.8747 - mae: 5.9900 - mse: 94.8747 - val_accuracy: 0.1673 - val_loss: 91.9165 - val_mae: 5.8146 - val_mse: 91.9165\n",
      "Epoch 2/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 963us/step - accuracy: 0.1728 - loss: 91.6572 - mae: 5.8233 - mse: 91.6572 - val_accuracy: 0.1582 - val_loss: 94.4077 - val_mae: 6.0406 - val_mse: 94.4077\n",
      "Epoch 3/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 963us/step - accuracy: 0.1728 - loss: 91.6572 - mae: 5.8233 - mse: 91.6572 - val_accuracy: 0.1582 - val_loss: 94.4077 - val_mae: 6.0406 - val_mse: 94.4077\n",
      "Epoch 3/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.1749 - loss: 91.4241 - mae: 5.8152 - mse: 91.4241 - val_accuracy: 0.1753 - val_loss: 91.2064 - val_mae: 5.8519 - val_mse: 91.2064\n",
      "Epoch 4/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - accuracy: 0.1749 - loss: 91.4241 - mae: 5.8152 - mse: 91.4241 - val_accuracy: 0.1753 - val_loss: 91.2064 - val_mae: 5.8519 - val_mse: 91.2064\n",
      "Epoch 4/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1751 - loss: 91.2857 - mae: 5.8084 - mse: 91.2857 - val_accuracy: 0.1582 - val_loss: 91.5130 - val_mae: 5.9205 - val_mse: 91.5130\n",
      "Epoch 5/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1751 - loss: 91.2857 - mae: 5.8084 - mse: 91.2857 - val_accuracy: 0.1582 - val_loss: 91.5130 - val_mae: 5.9205 - val_mse: 91.5130\n",
      "Epoch 5/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1751 - loss: 91.1868 - mae: 5.8080 - mse: 91.1868 - val_accuracy: 0.1650 - val_loss: 94.5511 - val_mae: 6.0227 - val_mse: 94.5511\n",
      "Epoch 6/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1751 - loss: 91.1868 - mae: 5.8080 - mse: 91.1868 - val_accuracy: 0.1650 - val_loss: 94.5511 - val_mae: 6.0227 - val_mse: 94.5511\n",
      "Epoch 6/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1740 - loss: 91.1457 - mae: 5.8060 - mse: 91.1457 - val_accuracy: 0.1649 - val_loss: 91.5588 - val_mae: 5.8595 - val_mse: 91.5588\n",
      "Epoch 7/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1740 - loss: 91.1457 - mae: 5.8060 - mse: 91.1457 - val_accuracy: 0.1649 - val_loss: 91.5588 - val_mae: 5.8595 - val_mse: 91.5588\n",
      "Epoch 7/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1729 - loss: 91.0799 - mae: 5.8083 - mse: 91.0799 - val_accuracy: 0.1993 - val_loss: 92.3915 - val_mae: 5.7803 - val_mse: 92.3915\n",
      "Epoch 8/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1729 - loss: 91.0799 - mae: 5.8083 - mse: 91.0799 - val_accuracy: 0.1993 - val_loss: 92.3915 - val_mae: 5.7803 - val_mse: 92.3915\n",
      "Epoch 8/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1736 - loss: 91.0017 - mae: 5.8036 - mse: 91.0017 - val_accuracy: 0.1808 - val_loss: 92.2592 - val_mae: 5.8710 - val_mse: 92.2592\n",
      "Epoch 9/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1736 - loss: 91.0017 - mae: 5.8036 - mse: 91.0017 - val_accuracy: 0.1808 - val_loss: 92.2592 - val_mae: 5.8710 - val_mse: 92.2592\n",
      "Epoch 9/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1720 - loss: 90.9857 - mae: 5.8037 - mse: 90.9857 - val_accuracy: 0.1773 - val_loss: 91.2392 - val_mae: 5.8444 - val_mse: 91.2392\n",
      "Epoch 10/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1720 - loss: 90.9857 - mae: 5.8037 - mse: 90.9857 - val_accuracy: 0.1773 - val_loss: 91.2392 - val_mae: 5.8444 - val_mse: 91.2392\n",
      "Epoch 10/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1735 - loss: 91.0111 - mae: 5.8049 - mse: 91.0111 - val_accuracy: 0.1507 - val_loss: 92.3978 - val_mae: 5.8890 - val_mse: 92.3978\n",
      "Epoch 11/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1735 - loss: 91.0111 - mae: 5.8049 - mse: 91.0111 - val_accuracy: 0.1507 - val_loss: 92.3978 - val_mae: 5.8890 - val_mse: 92.3978\n",
      "Epoch 11/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1726 - loss: 90.9320 - mae: 5.8047 - mse: 90.9320 - val_accuracy: 0.1827 - val_loss: 91.2177 - val_mae: 5.8374 - val_mse: 91.2177\n",
      "Epoch 12/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1726 - loss: 90.9320 - mae: 5.8047 - mse: 90.9320 - val_accuracy: 0.1827 - val_loss: 91.2177 - val_mae: 5.8374 - val_mse: 91.2177\n",
      "Epoch 12/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1747 - loss: 90.8961 - mae: 5.8003 - mse: 90.8961 - val_accuracy: 0.1612 - val_loss: 91.1181 - val_mae: 5.8182 - val_mse: 91.1181\n",
      "Epoch 13/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1747 - loss: 90.8961 - mae: 5.8003 - mse: 90.8961 - val_accuracy: 0.1612 - val_loss: 91.1181 - val_mae: 5.8182 - val_mse: 91.1181\n",
      "Epoch 13/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1727 - loss: 90.8720 - mae: 5.8042 - mse: 90.8720 - val_accuracy: 0.1886 - val_loss: 91.2865 - val_mae: 5.8368 - val_mse: 91.2865\n",
      "Epoch 14/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1727 - loss: 90.8720 - mae: 5.8042 - mse: 90.8720 - val_accuracy: 0.1886 - val_loss: 91.2865 - val_mae: 5.8368 - val_mse: 91.2865\n",
      "Epoch 14/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1731 - loss: 90.8518 - mae: 5.8003 - mse: 90.8518 - val_accuracy: 0.1628 - val_loss: 92.0938 - val_mae: 5.8670 - val_mse: 92.0938\n",
      "Epoch 15/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1731 - loss: 90.8518 - mae: 5.8003 - mse: 90.8518 - val_accuracy: 0.1628 - val_loss: 92.0938 - val_mae: 5.8670 - val_mse: 92.0938\n",
      "Epoch 15/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1735 - loss: 90.7902 - mae: 5.8024 - mse: 90.7902 - val_accuracy: 0.1972 - val_loss: 91.1200 - val_mae: 5.7734 - val_mse: 91.1200\n",
      "Epoch 16/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1735 - loss: 90.7902 - mae: 5.8024 - mse: 90.7902 - val_accuracy: 0.1972 - val_loss: 91.1200 - val_mae: 5.7734 - val_mse: 91.1200\n",
      "Epoch 16/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1721 - loss: 90.7605 - mae: 5.8012 - mse: 90.7605 - val_accuracy: 0.1865 - val_loss: 91.3970 - val_mae: 5.7991 - val_mse: 91.3970\n",
      "Epoch 17/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1721 - loss: 90.7605 - mae: 5.8012 - mse: 90.7605 - val_accuracy: 0.1865 - val_loss: 91.3970 - val_mae: 5.7991 - val_mse: 91.3970\n",
      "Epoch 17/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1718 - loss: 90.7620 - mae: 5.8021 - mse: 90.7620 - val_accuracy: 0.1636 - val_loss: 91.1440 - val_mae: 5.8607 - val_mse: 91.1440\n",
      "Epoch 18/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1718 - loss: 90.7620 - mae: 5.8021 - mse: 90.7620 - val_accuracy: 0.1636 - val_loss: 91.1440 - val_mae: 5.8607 - val_mse: 91.1440\n",
      "Epoch 18/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1724 - loss: 90.7483 - mae: 5.8019 - mse: 90.7483 - val_accuracy: 0.1661 - val_loss: 91.4906 - val_mae: 5.8730 - val_mse: 91.4906\n",
      "Epoch 19/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1724 - loss: 90.7483 - mae: 5.8019 - mse: 90.7483 - val_accuracy: 0.1661 - val_loss: 91.4906 - val_mae: 5.8730 - val_mse: 91.4906\n",
      "Epoch 19/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1709 - loss: 90.7511 - mae: 5.8026 - mse: 90.7511 - val_accuracy: 0.1816 - val_loss: 91.4209 - val_mae: 5.8285 - val_mse: 91.4209\n",
      "Epoch 20/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1709 - loss: 90.7511 - mae: 5.8026 - mse: 90.7511 - val_accuracy: 0.1816 - val_loss: 91.4209 - val_mae: 5.8285 - val_mse: 91.4209\n",
      "Epoch 20/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1720 - loss: 90.6882 - mae: 5.7997 - mse: 90.6882 - val_accuracy: 0.1715 - val_loss: 91.5172 - val_mae: 5.8382 - val_mse: 91.5172\n",
      "Epoch 21/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1720 - loss: 90.6882 - mae: 5.7997 - mse: 90.6882 - val_accuracy: 0.1715 - val_loss: 91.5172 - val_mae: 5.8382 - val_mse: 91.5172\n",
      "Epoch 21/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.1708 - loss: 90.6871 - mae: 5.8006 - mse: 90.6871 - val_accuracy: 0.1639 - val_loss: 91.2442 - val_mae: 5.8485 - val_mse: 91.2442\n",
      "Epoch 22/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.1708 - loss: 90.6871 - mae: 5.8006 - mse: 90.6871 - val_accuracy: 0.1639 - val_loss: 91.2442 - val_mae: 5.8485 - val_mse: 91.2442\n",
      "Epoch 22/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.1710 - loss: 90.6176 - mae: 5.7997 - mse: 90.6176 - val_accuracy: 0.1398 - val_loss: 91.1974 - val_mae: 5.8729 - val_mse: 91.1974\n",
      "Epoch 23/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - accuracy: 0.1710 - loss: 90.6176 - mae: 5.7997 - mse: 90.6176 - val_accuracy: 0.1398 - val_loss: 91.1974 - val_mae: 5.8729 - val_mse: 91.1974\n",
      "Epoch 23/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1688 - loss: 90.6271 - mae: 5.8010 - mse: 90.6271 - val_accuracy: 0.1802 - val_loss: 91.3568 - val_mae: 5.8067 - val_mse: 91.3568\n",
      "Epoch 24/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1688 - loss: 90.6271 - mae: 5.8010 - mse: 90.6271 - val_accuracy: 0.1802 - val_loss: 91.3568 - val_mae: 5.8067 - val_mse: 91.3568\n",
      "Epoch 24/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1695 - loss: 90.5805 - mae: 5.8011 - mse: 90.5805 - val_accuracy: 0.1648 - val_loss: 91.8310 - val_mae: 5.8156 - val_mse: 91.8310\n",
      "Epoch 25/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1695 - loss: 90.5805 - mae: 5.8011 - mse: 90.5805 - val_accuracy: 0.1648 - val_loss: 91.8310 - val_mae: 5.8156 - val_mse: 91.8310\n",
      "Epoch 25/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.6449 - mae: 5.8003 - mse: 90.6449 - val_accuracy: 0.1798 - val_loss: 92.2924 - val_mae: 5.7620 - val_mse: 92.2924\n",
      "Epoch 26/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.6449 - mae: 5.8003 - mse: 90.6449 - val_accuracy: 0.1798 - val_loss: 92.2924 - val_mae: 5.7620 - val_mse: 92.2924\n",
      "Epoch 26/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1699 - loss: 90.5839 - mae: 5.7973 - mse: 90.5839 - val_accuracy: 0.1463 - val_loss: 91.4915 - val_mae: 5.8615 - val_mse: 91.4915\n",
      "Epoch 27/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1699 - loss: 90.5839 - mae: 5.7973 - mse: 90.5839 - val_accuracy: 0.1463 - val_loss: 91.4915 - val_mae: 5.8615 - val_mse: 91.4915\n",
      "Epoch 27/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1697 - loss: 90.5643 - mae: 5.7974 - mse: 90.5643 - val_accuracy: 0.1618 - val_loss: 91.3520 - val_mae: 5.8521 - val_mse: 91.3520\n",
      "Epoch 28/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1697 - loss: 90.5643 - mae: 5.7974 - mse: 90.5643 - val_accuracy: 0.1618 - val_loss: 91.3520 - val_mae: 5.8521 - val_mse: 91.3520\n",
      "Epoch 28/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.5257 - mae: 5.7965 - mse: 90.5257 - val_accuracy: 0.1458 - val_loss: 91.3418 - val_mae: 5.8155 - val_mse: 91.3418\n",
      "Epoch 29/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.5257 - mae: 5.7965 - mse: 90.5257 - val_accuracy: 0.1458 - val_loss: 91.3418 - val_mae: 5.8155 - val_mse: 91.3418\n",
      "Epoch 29/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1703 - loss: 90.4897 - mae: 5.7945 - mse: 90.4897 - val_accuracy: 0.1817 - val_loss: 91.3313 - val_mae: 5.8415 - val_mse: 91.3313\n",
      "Epoch 30/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1703 - loss: 90.4897 - mae: 5.7945 - mse: 90.4897 - val_accuracy: 0.1817 - val_loss: 91.3313 - val_mae: 5.8415 - val_mse: 91.3313\n",
      "Epoch 30/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1706 - loss: 90.4961 - mae: 5.7966 - mse: 90.4961 - val_accuracy: 0.1941 - val_loss: 91.7639 - val_mae: 5.7892 - val_mse: 91.7639\n",
      "Epoch 31/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1706 - loss: 90.4961 - mae: 5.7966 - mse: 90.4961 - val_accuracy: 0.1941 - val_loss: 91.7639 - val_mae: 5.7892 - val_mse: 91.7639\n",
      "Epoch 31/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1715 - loss: 90.4641 - mae: 5.7943 - mse: 90.4641 - val_accuracy: 0.1677 - val_loss: 91.4853 - val_mae: 5.8554 - val_mse: 91.4853\n",
      "Epoch 32/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1715 - loss: 90.4641 - mae: 5.7943 - mse: 90.4641 - val_accuracy: 0.1677 - val_loss: 91.4853 - val_mae: 5.8554 - val_mse: 91.4853\n",
      "Epoch 32/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1702 - loss: 90.4264 - mae: 5.7936 - mse: 90.4264 - val_accuracy: 0.1704 - val_loss: 91.3535 - val_mae: 5.7708 - val_mse: 91.3535\n",
      "Epoch 33/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1702 - loss: 90.4264 - mae: 5.7936 - mse: 90.4264 - val_accuracy: 0.1704 - val_loss: 91.3535 - val_mae: 5.7708 - val_mse: 91.3535\n",
      "Epoch 33/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1712 - loss: 90.4050 - mae: 5.7940 - mse: 90.4050 - val_accuracy: 0.1815 - val_loss: 91.8573 - val_mae: 5.8138 - val_mse: 91.8573\n",
      "Epoch 34/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1712 - loss: 90.4050 - mae: 5.7940 - mse: 90.4050 - val_accuracy: 0.1815 - val_loss: 91.8573 - val_mae: 5.8138 - val_mse: 91.8573\n",
      "Epoch 34/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1706 - loss: 90.3653 - mae: 5.7915 - mse: 90.3653 - val_accuracy: 0.1697 - val_loss: 91.3245 - val_mae: 5.8549 - val_mse: 91.3245\n",
      "Epoch 35/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1706 - loss: 90.3653 - mae: 5.7915 - mse: 90.3653 - val_accuracy: 0.1697 - val_loss: 91.3245 - val_mae: 5.8549 - val_mse: 91.3245\n",
      "Epoch 35/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1710 - loss: 90.3397 - mae: 5.7920 - mse: 90.3397 - val_accuracy: 0.1350 - val_loss: 91.4284 - val_mae: 5.8870 - val_mse: 91.4284\n",
      "Epoch 36/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1710 - loss: 90.3397 - mae: 5.7920 - mse: 90.3397 - val_accuracy: 0.1350 - val_loss: 91.4284 - val_mae: 5.8870 - val_mse: 91.4284\n",
      "Epoch 36/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1697 - loss: 90.2853 - mae: 5.7902 - mse: 90.2853 - val_accuracy: 0.1639 - val_loss: 91.2297 - val_mae: 5.8420 - val_mse: 91.2297\n",
      "Epoch 37/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1697 - loss: 90.2853 - mae: 5.7902 - mse: 90.2853 - val_accuracy: 0.1639 - val_loss: 91.2297 - val_mae: 5.8420 - val_mse: 91.2297\n",
      "Epoch 37/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.3079 - mae: 5.7909 - mse: 90.3079 - val_accuracy: 0.1855 - val_loss: 91.5723 - val_mae: 5.8126 - val_mse: 91.5723\n",
      "Epoch 38/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1701 - loss: 90.3079 - mae: 5.7909 - mse: 90.3079 - val_accuracy: 0.1855 - val_loss: 91.5723 - val_mae: 5.8126 - val_mse: 91.5723\n",
      "Epoch 38/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1696 - loss: 90.2278 - mae: 5.7909 - mse: 90.2278 - val_accuracy: 0.1681 - val_loss: 91.4711 - val_mae: 5.8210 - val_mse: 91.4711\n",
      "Epoch 39/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1696 - loss: 90.2278 - mae: 5.7909 - mse: 90.2278 - val_accuracy: 0.1681 - val_loss: 91.4711 - val_mae: 5.8210 - val_mse: 91.4711\n",
      "Epoch 39/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1700 - loss: 90.2392 - mae: 5.7903 - mse: 90.2392 - val_accuracy: 0.1896 - val_loss: 91.4198 - val_mae: 5.7878 - val_mse: 91.4198\n",
      "Epoch 40/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1700 - loss: 90.2392 - mae: 5.7903 - mse: 90.2392 - val_accuracy: 0.1896 - val_loss: 91.4198 - val_mae: 5.7878 - val_mse: 91.4198\n",
      "Epoch 40/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1677 - loss: 90.1981 - mae: 5.7913 - mse: 90.1981 - val_accuracy: 0.1804 - val_loss: 91.7743 - val_mae: 5.8382 - val_mse: 91.7743\n",
      "Epoch 41/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1677 - loss: 90.1981 - mae: 5.7913 - mse: 90.1981 - val_accuracy: 0.1804 - val_loss: 91.7743 - val_mae: 5.8382 - val_mse: 91.7743\n",
      "Epoch 41/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1703 - loss: 90.1470 - mae: 5.7880 - mse: 90.1470 - val_accuracy: 0.1553 - val_loss: 91.4133 - val_mae: 5.8439 - val_mse: 91.4133\n",
      "Epoch 42/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1703 - loss: 90.1470 - mae: 5.7880 - mse: 90.1470 - val_accuracy: 0.1553 - val_loss: 91.4133 - val_mae: 5.8439 - val_mse: 91.4133\n",
      "Epoch 42/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1683 - loss: 90.1478 - mae: 5.7896 - mse: 90.1478 - val_accuracy: 0.1578 - val_loss: 91.6973 - val_mae: 5.8624 - val_mse: 91.6973\n",
      "Epoch 43/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1683 - loss: 90.1478 - mae: 5.7896 - mse: 90.1478 - val_accuracy: 0.1578 - val_loss: 91.6973 - val_mae: 5.8624 - val_mse: 91.6973\n",
      "Epoch 43/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1687 - loss: 90.0793 - mae: 5.7894 - mse: 90.0793 - val_accuracy: 0.1516 - val_loss: 91.8102 - val_mae: 5.8631 - val_mse: 91.8102\n",
      "Epoch 44/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1687 - loss: 90.0793 - mae: 5.7894 - mse: 90.0793 - val_accuracy: 0.1516 - val_loss: 91.8102 - val_mae: 5.8631 - val_mse: 91.8102\n",
      "Epoch 44/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1700 - loss: 90.0464 - mae: 5.7881 - mse: 90.0464 - val_accuracy: 0.1419 - val_loss: 91.5744 - val_mae: 5.9016 - val_mse: 91.5744\n",
      "Epoch 45/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1700 - loss: 90.0464 - mae: 5.7881 - mse: 90.0464 - val_accuracy: 0.1419 - val_loss: 91.5744 - val_mae: 5.9016 - val_mse: 91.5744\n",
      "Epoch 45/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 90.0193 - mae: 5.7854 - mse: 90.0193 - val_accuracy: 0.1458 - val_loss: 91.8069 - val_mae: 5.9231 - val_mse: 91.8069\n",
      "Epoch 46/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 90.0193 - mae: 5.7854 - mse: 90.0193 - val_accuracy: 0.1458 - val_loss: 91.8069 - val_mae: 5.9231 - val_mse: 91.8069\n",
      "Epoch 46/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 90.0137 - mae: 5.7879 - mse: 90.0137 - val_accuracy: 0.1563 - val_loss: 91.6871 - val_mae: 5.8503 - val_mse: 91.6871\n",
      "Epoch 47/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 90.0137 - mae: 5.7879 - mse: 90.0137 - val_accuracy: 0.1563 - val_loss: 91.6871 - val_mae: 5.8503 - val_mse: 91.6871\n",
      "Epoch 47/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1694 - loss: 89.8984 - mae: 5.7837 - mse: 89.8984 - val_accuracy: 0.1684 - val_loss: 91.8330 - val_mae: 5.8667 - val_mse: 91.8330\n",
      "Epoch 48/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1694 - loss: 89.8984 - mae: 5.7837 - mse: 89.8984 - val_accuracy: 0.1684 - val_loss: 91.8330 - val_mae: 5.8667 - val_mse: 91.8330\n",
      "Epoch 48/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 89.9082 - mae: 5.7849 - mse: 89.9082 - val_accuracy: 0.1377 - val_loss: 91.5757 - val_mae: 5.9049 - val_mse: 91.5757\n",
      "Epoch 49/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1704 - loss: 89.9082 - mae: 5.7849 - mse: 89.9082 - val_accuracy: 0.1377 - val_loss: 91.5757 - val_mae: 5.9049 - val_mse: 91.5757\n",
      "Epoch 49/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1698 - loss: 89.8264 - mae: 5.7844 - mse: 89.8264 - val_accuracy: 0.1562 - val_loss: 93.1858 - val_mae: 5.9942 - val_mse: 93.1858\n",
      "Epoch 50/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1698 - loss: 89.8264 - mae: 5.7844 - mse: 89.8264 - val_accuracy: 0.1562 - val_loss: 93.1858 - val_mae: 5.9942 - val_mse: 93.1858\n",
      "Epoch 50/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1707 - loss: 89.8788 - mae: 5.7836 - mse: 89.8788 - val_accuracy: 0.1509 - val_loss: 91.7168 - val_mae: 5.8631 - val_mse: 91.7168\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.1707 - loss: 89.8788 - mae: 5.7836 - mse: 89.8788 - val_accuracy: 0.1509 - val_loss: 91.7168 - val_mae: 5.8631 - val_mse: 91.7168\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeeaee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation:\n",
      "  MAE: 5.8631\n",
      "  RMSE: 9.5769\n",
      "  Accuracy: 0.1773\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "acc = np.mean(np.isclose(np.round(y_test), np.round(y_pred)))  # crude regression accuracy\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation:\\n  MAE: {mae:.4f}\\n  RMSE: {rmse:.4f}\\n  Accuracy: {acc:.4f}\")\n",
    "model.save(os.path.join(MODEL_DIR, \"mlp_large_tuned.h5\"))\n",
    "plot_training(history, \"Large MLP (Tuned)\", \"training_large_tuned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab703a",
   "metadata": {},
   "source": [
    "Not the best numbers, next few steps - \n",
    "1. Baseline from mean of y_train -> check wif the NN is beating this\n",
    "2. Try different batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a8f394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (mean predictor):\n",
      "  MAE: 0.7706\n",
      "  RMSE: 1.0066\n",
      "  R²: -0.0001\n"
     ]
    }
   ],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "y_baseline = np.full_like(y_test_scaled, fill_value=np.mean(y_train_scaled))\n",
    "baseline_mae = mean_absolute_error(y_test_scaled, y_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_scaled, y_baseline))\n",
    "baseline_r2 = r2_score(y_test_scaled, y_baseline)\n",
    "\n",
    "print(f\"\\nBaseline (mean predictor):\\n\"\n",
    "      f\"  MAE: {baseline_mae:.4f}\\n\"\n",
    "      f\"  RMSE: {baseline_rmse:.4f}\\n\"\n",
    "      f\"  R²: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edf5d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03e3b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.9074 - mae: 0.7331 - mse: 0.9074 - val_accuracy: 0.0000e+00 - val_loss: 0.8311 - val_mae: 0.6863 - val_mse: 0.8311\n",
      "Epoch 2/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.9074 - mae: 0.7331 - mse: 0.9074 - val_accuracy: 0.0000e+00 - val_loss: 0.8311 - val_mae: 0.6863 - val_mse: 0.8311\n",
      "Epoch 2/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.7310 - mae: 0.6201 - mse: 0.7310 - val_accuracy: 0.0000e+00 - val_loss: 0.6555 - val_mae: 0.5594 - val_mse: 0.6555\n",
      "Epoch 3/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.7310 - mae: 0.6201 - mse: 0.7310 - val_accuracy: 0.0000e+00 - val_loss: 0.6555 - val_mae: 0.5594 - val_mse: 0.6555\n",
      "Epoch 3/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.6096 - mae: 0.5175 - mse: 0.6096 - val_accuracy: 0.0000e+00 - val_loss: 0.5935 - val_mae: 0.4989 - val_mse: 0.5935\n",
      "Epoch 4/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.6096 - mae: 0.5175 - mse: 0.6096 - val_accuracy: 0.0000e+00 - val_loss: 0.5935 - val_mae: 0.4989 - val_mse: 0.5935\n",
      "Epoch 4/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5812 - mae: 0.4912 - mse: 0.5812 - val_accuracy: 0.0000e+00 - val_loss: 0.5812 - val_mae: 0.4907 - val_mse: 0.5812\n",
      "Epoch 5/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5812 - mae: 0.4912 - mse: 0.5812 - val_accuracy: 0.0000e+00 - val_loss: 0.5812 - val_mae: 0.4907 - val_mse: 0.5812\n",
      "Epoch 5/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5736 - mae: 0.4826 - mse: 0.5736 - val_accuracy: 0.0000e+00 - val_loss: 0.5768 - val_mae: 0.4882 - val_mse: 0.5768\n",
      "Epoch 6/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5736 - mae: 0.4826 - mse: 0.5736 - val_accuracy: 0.0000e+00 - val_loss: 0.5768 - val_mae: 0.4882 - val_mse: 0.5768\n",
      "Epoch 6/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5694 - mae: 0.4773 - mse: 0.5694 - val_accuracy: 0.0000e+00 - val_loss: 0.5727 - val_mae: 0.4791 - val_mse: 0.5727\n",
      "Epoch 7/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5694 - mae: 0.4773 - mse: 0.5694 - val_accuracy: 0.0000e+00 - val_loss: 0.5727 - val_mae: 0.4791 - val_mse: 0.5727\n",
      "Epoch 7/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5666 - mae: 0.4736 - mse: 0.5666 - val_accuracy: 0.0000e+00 - val_loss: 0.5706 - val_mae: 0.4762 - val_mse: 0.5706\n",
      "Epoch 8/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5666 - mae: 0.4736 - mse: 0.5666 - val_accuracy: 0.0000e+00 - val_loss: 0.5706 - val_mae: 0.4762 - val_mse: 0.5706\n",
      "Epoch 8/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5646 - mae: 0.4709 - mse: 0.5646 - val_accuracy: 0.0000e+00 - val_loss: 0.5689 - val_mae: 0.4708 - val_mse: 0.5689\n",
      "Epoch 9/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5646 - mae: 0.4709 - mse: 0.5646 - val_accuracy: 0.0000e+00 - val_loss: 0.5689 - val_mae: 0.4708 - val_mse: 0.5689\n",
      "Epoch 9/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5629 - mae: 0.4685 - mse: 0.5629 - val_accuracy: 0.0000e+00 - val_loss: 0.5671 - val_mae: 0.4702 - val_mse: 0.5671\n",
      "Epoch 10/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5629 - mae: 0.4685 - mse: 0.5629 - val_accuracy: 0.0000e+00 - val_loss: 0.5671 - val_mae: 0.4702 - val_mse: 0.5671\n",
      "Epoch 10/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5616 - mae: 0.4668 - mse: 0.5616 - val_accuracy: 0.0000e+00 - val_loss: 0.5659 - val_mae: 0.4691 - val_mse: 0.5659\n",
      "Epoch 11/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5616 - mae: 0.4668 - mse: 0.5616 - val_accuracy: 0.0000e+00 - val_loss: 0.5659 - val_mae: 0.4691 - val_mse: 0.5659\n",
      "Epoch 11/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5606 - mae: 0.4654 - mse: 0.5606 - val_accuracy: 0.0000e+00 - val_loss: 0.5654 - val_mae: 0.4654 - val_mse: 0.5654\n",
      "Epoch 12/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5606 - mae: 0.4654 - mse: 0.5606 - val_accuracy: 0.0000e+00 - val_loss: 0.5654 - val_mae: 0.4654 - val_mse: 0.5654\n",
      "Epoch 12/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5597 - mae: 0.4641 - mse: 0.5597 - val_accuracy: 0.0000e+00 - val_loss: 0.5641 - val_mae: 0.4682 - val_mse: 0.5641\n",
      "Epoch 13/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5597 - mae: 0.4641 - mse: 0.5597 - val_accuracy: 0.0000e+00 - val_loss: 0.5641 - val_mae: 0.4682 - val_mse: 0.5641\n",
      "Epoch 13/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5589 - mae: 0.4631 - mse: 0.5589 - val_accuracy: 0.0000e+00 - val_loss: 0.5636 - val_mae: 0.4701 - val_mse: 0.5636\n",
      "Epoch 14/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5589 - mae: 0.4631 - mse: 0.5589 - val_accuracy: 0.0000e+00 - val_loss: 0.5636 - val_mae: 0.4701 - val_mse: 0.5636\n",
      "Epoch 14/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5583 - mae: 0.4624 - mse: 0.5583 - val_accuracy: 0.0000e+00 - val_loss: 0.5627 - val_mae: 0.4666 - val_mse: 0.5627\n",
      "Epoch 15/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5583 - mae: 0.4624 - mse: 0.5583 - val_accuracy: 0.0000e+00 - val_loss: 0.5627 - val_mae: 0.4666 - val_mse: 0.5627\n",
      "Epoch 15/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5577 - mae: 0.4616 - mse: 0.5577 - val_accuracy: 0.0000e+00 - val_loss: 0.5622 - val_mae: 0.4655 - val_mse: 0.5622\n",
      "Epoch 16/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5577 - mae: 0.4616 - mse: 0.5577 - val_accuracy: 0.0000e+00 - val_loss: 0.5622 - val_mae: 0.4655 - val_mse: 0.5622\n",
      "Epoch 16/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5570 - mae: 0.4609 - mse: 0.5570 - val_accuracy: 0.0000e+00 - val_loss: 0.5624 - val_mae: 0.4621 - val_mse: 0.5624\n",
      "Epoch 17/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5570 - mae: 0.4609 - mse: 0.5570 - val_accuracy: 0.0000e+00 - val_loss: 0.5624 - val_mae: 0.4621 - val_mse: 0.5624\n",
      "Epoch 17/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5567 - mae: 0.4603 - mse: 0.5567 - val_accuracy: 0.0000e+00 - val_loss: 0.5615 - val_mae: 0.4623 - val_mse: 0.5615\n",
      "Epoch 18/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5567 - mae: 0.4603 - mse: 0.5567 - val_accuracy: 0.0000e+00 - val_loss: 0.5615 - val_mae: 0.4623 - val_mse: 0.5615\n",
      "Epoch 18/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5562 - mae: 0.4599 - mse: 0.5562 - val_accuracy: 0.0000e+00 - val_loss: 0.5610 - val_mae: 0.4621 - val_mse: 0.5610\n",
      "Epoch 19/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5562 - mae: 0.4599 - mse: 0.5562 - val_accuracy: 0.0000e+00 - val_loss: 0.5610 - val_mae: 0.4621 - val_mse: 0.5610\n",
      "Epoch 19/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5558 - mae: 0.4594 - mse: 0.5558 - val_accuracy: 0.0000e+00 - val_loss: 0.5606 - val_mae: 0.4619 - val_mse: 0.5606\n",
      "Epoch 20/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5558 - mae: 0.4594 - mse: 0.5558 - val_accuracy: 0.0000e+00 - val_loss: 0.5606 - val_mae: 0.4619 - val_mse: 0.5606\n",
      "Epoch 20/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5554 - mae: 0.4590 - mse: 0.5554 - val_accuracy: 0.0000e+00 - val_loss: 0.5606 - val_mae: 0.4612 - val_mse: 0.5606\n",
      "Epoch 21/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5554 - mae: 0.4590 - mse: 0.5554 - val_accuracy: 0.0000e+00 - val_loss: 0.5606 - val_mae: 0.4612 - val_mse: 0.5606\n",
      "Epoch 21/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5550 - mae: 0.4586 - mse: 0.5550 - val_accuracy: 0.0000e+00 - val_loss: 0.5598 - val_mae: 0.4638 - val_mse: 0.5598\n",
      "Epoch 22/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5550 - mae: 0.4586 - mse: 0.5550 - val_accuracy: 0.0000e+00 - val_loss: 0.5598 - val_mae: 0.4638 - val_mse: 0.5598\n",
      "Epoch 22/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5547 - mae: 0.4583 - mse: 0.5547 - val_accuracy: 0.0000e+00 - val_loss: 0.5603 - val_mae: 0.4589 - val_mse: 0.5603\n",
      "Epoch 23/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5547 - mae: 0.4583 - mse: 0.5547 - val_accuracy: 0.0000e+00 - val_loss: 0.5603 - val_mae: 0.4589 - val_mse: 0.5603\n",
      "Epoch 23/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5545 - mae: 0.4581 - mse: 0.5545 - val_accuracy: 0.0000e+00 - val_loss: 0.5594 - val_mae: 0.4594 - val_mse: 0.5594\n",
      "Epoch 24/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5545 - mae: 0.4581 - mse: 0.5545 - val_accuracy: 0.0000e+00 - val_loss: 0.5594 - val_mae: 0.4594 - val_mse: 0.5594\n",
      "Epoch 24/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5541 - mae: 0.4577 - mse: 0.5541 - val_accuracy: 0.0000e+00 - val_loss: 0.5591 - val_mae: 0.4598 - val_mse: 0.5591\n",
      "Epoch 25/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5541 - mae: 0.4577 - mse: 0.5541 - val_accuracy: 0.0000e+00 - val_loss: 0.5591 - val_mae: 0.4598 - val_mse: 0.5591\n",
      "Epoch 25/25\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5539 - mae: 0.4574 - mse: 0.5539 - val_accuracy: 0.0000e+00 - val_loss: 0.5589 - val_mae: 0.4603 - val_mse: 0.5589\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5539 - mae: 0.4574 - mse: 0.5539 - val_accuracy: 0.0000e+00 - val_loss: 0.5589 - val_mae: 0.4603 - val_mse: 0.5589\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87addde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step\n",
      "\n",
      "Final Model Evaluation (scaled):\n",
      "  MAE: 0.4603\n",
      "  RMSE: 0.7476\n",
      "  R²: 0.4484\n",
      "\n",
      "Final Model Evaluation (scaled):\n",
      "  MAE: 0.4603\n",
      "  RMSE: 0.7476\n",
      "  R²: 0.4484\n"
     ]
    }
   ],
   "source": [
    "y_pred_scaled = model.predict(X_test_scaled).flatten()\n",
    "mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_scaled, y_pred_scaled))\n",
    "r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation (scaled):\\n\"\n",
    "      f\"  MAE: {mae:.4f}\\n\"\n",
    "      f\"  RMSE: {rmse:.4f}\\n\"\n",
    "      f\"  R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7fc00",
   "metadata": {},
   "source": [
    "- Model seems to be learning from data, with drops in RMSE and MAE\n",
    "- Still improvements to be made\n",
    "- Explore feature engineering, more complex models regularization, CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3cc30",
   "metadata": {},
   "source": [
    "## Further Improvements: Feature Engineering & Advanced Tuning\n",
    "We'll now try to improve the large MLP by adding feature engineering and using more advanced hyperparameter search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b28edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered shapes: X_train_fe (360000, 135), X_test_fe (90000, 135)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Add polynomial and interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_fe = poly.fit_transform(X_train)\n",
    "X_test_fe = poly.transform(X_test)\n",
    "\n",
    "print(f\"Feature engineered shapes: X_train_fe {X_train_fe.shape}, X_test_fe {X_test_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6164bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_bayes_tuning\\tuner0.json\n",
      "  - num_layers: 3\n",
      "  - units_0: 64\n",
      "  - activation_0: relu\n",
      "  - units_1: 352\n",
      "  - activation_1: relu\n",
      "  - optimizer: adam\n",
      "  - lr: 0.0016580818738343003\n",
      "  - units_2: 160\n",
      "  - activation_2: tanh\n",
      "  - units_3: 64\n",
      "  - activation_3: tanh\n",
      "  - units_4: 64\n",
      "  - activation_4: relu\n"
     ]
    }
   ],
   "source": [
    "# Advanced Hyperparameter Tuning: Bayesian Optimization\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_advanced_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train_fe.shape[1]\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    num_layers = hp.Int(\"num_layers\", 2, 6)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", 32, 512, step=32)\n",
    "        activation = hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\", \"rmsprop\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"mse\"])\n",
    "    return model\n",
    "\n",
    "bayes_tuner = BayesianOptimization(\n",
    "    build_advanced_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=20,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_bayes_tuning\"\n",
    ")\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)\n",
    "bayes_tuner.search(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=64,\n",
    "    epochs=40\n",
    ")\n",
    "best_bayes_hps = bayes_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "for hp in best_bayes_hps.values.keys():\n",
    "    print(f\"  - {hp}: {best_bayes_hps.get(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c883fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 94.3186 - mae: 5.9856 - mse: 94.3186 - val_loss: 93.8325 - val_mae: 5.9454 - val_mse: 93.8325\n",
      "Epoch 2/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 94.3186 - mae: 5.9856 - mse: 94.3186 - val_loss: 93.8325 - val_mae: 5.9454 - val_mse: 93.8325\n",
      "Epoch 2/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 92.1812 - mae: 5.8809 - mse: 92.1812 - val_loss: 92.1603 - val_mae: 5.8819 - val_mse: 92.1603\n",
      "Epoch 3/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 92.1812 - mae: 5.8809 - mse: 92.1812 - val_loss: 92.1603 - val_mae: 5.8819 - val_mse: 92.1603\n",
      "Epoch 3/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.7463 - mae: 5.8470 - mse: 91.7463 - val_loss: 92.4359 - val_mae: 5.8334 - val_mse: 92.4359\n",
      "Epoch 4/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.7463 - mae: 5.8470 - mse: 91.7463 - val_loss: 92.4359 - val_mae: 5.8334 - val_mse: 92.4359\n",
      "Epoch 4/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.5186 - mae: 5.8330 - mse: 91.5186 - val_loss: 92.1027 - val_mae: 5.9577 - val_mse: 92.1027\n",
      "Epoch 5/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.5186 - mae: 5.8330 - mse: 91.5186 - val_loss: 92.1027 - val_mae: 5.9577 - val_mse: 92.1027\n",
      "Epoch 5/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.3199 - mae: 5.8290 - mse: 91.3199 - val_loss: 91.8223 - val_mae: 5.9105 - val_mse: 91.8223\n",
      "Epoch 6/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.3199 - mae: 5.8290 - mse: 91.3199 - val_loss: 91.8223 - val_mae: 5.9105 - val_mse: 91.8223\n",
      "Epoch 6/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.2450 - mae: 5.8281 - mse: 91.2450 - val_loss: 91.7264 - val_mae: 5.8984 - val_mse: 91.7264\n",
      "Epoch 7/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.2450 - mae: 5.8281 - mse: 91.2450 - val_loss: 91.7264 - val_mae: 5.8984 - val_mse: 91.7264\n",
      "Epoch 7/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.0605 - mae: 5.8208 - mse: 91.0605 - val_loss: 92.1917 - val_mae: 5.8236 - val_mse: 92.1917\n",
      "Epoch 8/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 91.0605 - mae: 5.8208 - mse: 91.0605 - val_loss: 92.1917 - val_mae: 5.8236 - val_mse: 92.1917\n",
      "Epoch 8/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.8890 - mae: 5.8150 - mse: 90.8890 - val_loss: 92.3036 - val_mae: 5.9933 - val_mse: 92.3036\n",
      "Epoch 9/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.8890 - mae: 5.8150 - mse: 90.8890 - val_loss: 92.3036 - val_mae: 5.9933 - val_mse: 92.3036\n",
      "Epoch 9/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.8417 - mae: 5.8192 - mse: 90.8417 - val_loss: 92.1856 - val_mae: 5.8434 - val_mse: 92.1856\n",
      "Epoch 10/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.8417 - mae: 5.8192 - mse: 90.8417 - val_loss: 92.1856 - val_mae: 5.8434 - val_mse: 92.1856\n",
      "Epoch 10/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.6999 - mae: 5.8095 - mse: 90.6999 - val_loss: 92.2285 - val_mae: 5.9239 - val_mse: 92.2285\n",
      "Epoch 11/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.6999 - mae: 5.8095 - mse: 90.6999 - val_loss: 92.2285 - val_mae: 5.9239 - val_mse: 92.2285\n",
      "Epoch 11/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.6340 - mae: 5.8098 - mse: 90.6340 - val_loss: 92.6537 - val_mae: 6.2804 - val_mse: 92.6537\n",
      "Epoch 12/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.6340 - mae: 5.8098 - mse: 90.6340 - val_loss: 92.6537 - val_mae: 6.2804 - val_mse: 92.6537\n",
      "Epoch 12/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.4261 - mae: 5.8053 - mse: 90.4261 - val_loss: 92.2735 - val_mae: 5.9916 - val_mse: 92.2735\n",
      "Epoch 13/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.4261 - mae: 5.8053 - mse: 90.4261 - val_loss: 92.2735 - val_mae: 5.9916 - val_mse: 92.2735\n",
      "Epoch 13/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.3574 - mae: 5.8055 - mse: 90.3574 - val_loss: 92.3709 - val_mae: 5.8661 - val_mse: 92.3709\n",
      "Epoch 14/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.3574 - mae: 5.8055 - mse: 90.3574 - val_loss: 92.3709 - val_mae: 5.8661 - val_mse: 92.3709\n",
      "Epoch 14/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.2576 - mae: 5.8018 - mse: 90.2576 - val_loss: 92.1608 - val_mae: 5.7894 - val_mse: 92.1608\n",
      "Epoch 15/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.2576 - mae: 5.8018 - mse: 90.2576 - val_loss: 92.1608 - val_mae: 5.7894 - val_mse: 92.1608\n",
      "Epoch 15/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.1823 - mae: 5.7993 - mse: 90.1823 - val_loss: 92.1884 - val_mae: 5.7717 - val_mse: 92.1884\n",
      "Epoch 16/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.1823 - mae: 5.7993 - mse: 90.1823 - val_loss: 92.1884 - val_mae: 5.7717 - val_mse: 92.1884\n",
      "Epoch 16/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.0510 - mae: 5.7996 - mse: 90.0510 - val_loss: 92.3128 - val_mae: 5.9981 - val_mse: 92.3128\n",
      "Epoch 17/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.0510 - mae: 5.7996 - mse: 90.0510 - val_loss: 92.3128 - val_mae: 5.9981 - val_mse: 92.3128\n",
      "Epoch 17/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.0005 - mae: 5.7930 - mse: 90.0005 - val_loss: 92.7017 - val_mae: 5.9366 - val_mse: 92.7017\n",
      "Epoch 18/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 90.0005 - mae: 5.7930 - mse: 90.0005 - val_loss: 92.7017 - val_mae: 5.9366 - val_mse: 92.7017\n",
      "Epoch 18/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.9363 - mae: 5.7971 - mse: 89.9363 - val_loss: 92.3044 - val_mae: 5.8016 - val_mse: 92.3044\n",
      "Epoch 19/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.9363 - mae: 5.7971 - mse: 89.9363 - val_loss: 92.3044 - val_mae: 5.8016 - val_mse: 92.3044\n",
      "Epoch 19/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.8226 - mae: 5.7887 - mse: 89.8226 - val_loss: 93.1075 - val_mae: 5.9854 - val_mse: 93.1075\n",
      "Epoch 20/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.8226 - mae: 5.7887 - mse: 89.8226 - val_loss: 93.1075 - val_mae: 5.9854 - val_mse: 93.1075\n",
      "Epoch 20/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.6057 - mae: 5.7869 - mse: 89.6057 - val_loss: 93.2721 - val_mae: 5.9734 - val_mse: 93.2721\n",
      "Epoch 21/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.6057 - mae: 5.7869 - mse: 89.6057 - val_loss: 93.2721 - val_mae: 5.9734 - val_mse: 93.2721\n",
      "Epoch 21/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.5407 - mae: 5.7848 - mse: 89.5407 - val_loss: 92.5509 - val_mae: 6.0007 - val_mse: 92.5509\n",
      "Epoch 22/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.5407 - mae: 5.7848 - mse: 89.5407 - val_loss: 92.5509 - val_mae: 6.0007 - val_mse: 92.5509\n",
      "Epoch 22/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.3617 - mae: 5.7823 - mse: 89.3617 - val_loss: 92.5915 - val_mae: 5.8177 - val_mse: 92.5915\n",
      "Epoch 23/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.3617 - mae: 5.7823 - mse: 89.3617 - val_loss: 92.5915 - val_mae: 5.8177 - val_mse: 92.5915\n",
      "Epoch 23/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.2577 - mae: 5.7780 - mse: 89.2577 - val_loss: 92.8893 - val_mae: 5.7386 - val_mse: 92.8893\n",
      "Epoch 24/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.2577 - mae: 5.7780 - mse: 89.2577 - val_loss: 92.8893 - val_mae: 5.7386 - val_mse: 92.8893\n",
      "Epoch 24/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.2579 - mae: 5.7842 - mse: 89.2579 - val_loss: 93.3315 - val_mae: 5.9400 - val_mse: 93.3315\n",
      "Epoch 25/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.2579 - mae: 5.7842 - mse: 89.2579 - val_loss: 93.3315 - val_mae: 5.9400 - val_mse: 93.3315\n",
      "Epoch 25/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.3350 - mae: 5.7758 - mse: 89.3350 - val_loss: 93.3981 - val_mae: 6.0333 - val_mse: 93.3981\n",
      "Epoch 26/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.3350 - mae: 5.7758 - mse: 89.3350 - val_loss: 93.3981 - val_mae: 6.0333 - val_mse: 93.3981\n",
      "Epoch 26/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.0605 - mae: 5.7763 - mse: 89.0605 - val_loss: 93.7367 - val_mae: 6.2804 - val_mse: 93.7367\n",
      "Epoch 27/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.0605 - mae: 5.7763 - mse: 89.0605 - val_loss: 93.7367 - val_mae: 6.2804 - val_mse: 93.7367\n",
      "Epoch 27/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.0098 - mae: 5.7711 - mse: 89.0098 - val_loss: 93.3400 - val_mae: 5.9488 - val_mse: 93.3400\n",
      "Epoch 28/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 89.0098 - mae: 5.7711 - mse: 89.0098 - val_loss: 93.3400 - val_mae: 5.9488 - val_mse: 93.3400\n",
      "Epoch 28/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.9056 - mae: 5.7719 - mse: 88.9056 - val_loss: 93.7894 - val_mae: 5.7810 - val_mse: 93.7894\n",
      "Epoch 29/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.9056 - mae: 5.7719 - mse: 88.9056 - val_loss: 93.7894 - val_mae: 5.7810 - val_mse: 93.7894\n",
      "Epoch 29/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7527 - mae: 5.7713 - mse: 88.7527 - val_loss: 93.6270 - val_mae: 5.8374 - val_mse: 93.6270\n",
      "Epoch 30/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7527 - mae: 5.7713 - mse: 88.7527 - val_loss: 93.6270 - val_mae: 5.8374 - val_mse: 93.6270\n",
      "Epoch 30/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7410 - mae: 5.7659 - mse: 88.7410 - val_loss: 93.1625 - val_mae: 5.8278 - val_mse: 93.1625\n",
      "Epoch 31/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7410 - mae: 5.7659 - mse: 88.7410 - val_loss: 93.1625 - val_mae: 5.8278 - val_mse: 93.1625\n",
      "Epoch 31/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7433 - mae: 5.7681 - mse: 88.7433 - val_loss: 93.4426 - val_mae: 5.8741 - val_mse: 93.4426\n",
      "Epoch 32/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.7433 - mae: 5.7681 - mse: 88.7433 - val_loss: 93.4426 - val_mae: 5.8741 - val_mse: 93.4426\n",
      "Epoch 32/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.6371 - mae: 5.7643 - mse: 88.6371 - val_loss: 93.6747 - val_mae: 5.8216 - val_mse: 93.6747\n",
      "Epoch 33/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.6371 - mae: 5.7643 - mse: 88.6371 - val_loss: 93.6747 - val_mae: 5.8216 - val_mse: 93.6747\n",
      "Epoch 33/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.4831 - mae: 5.7578 - mse: 88.4831 - val_loss: 93.7832 - val_mae: 6.1068 - val_mse: 93.7832\n",
      "Epoch 34/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.4831 - mae: 5.7578 - mse: 88.4831 - val_loss: 93.7832 - val_mae: 6.1068 - val_mse: 93.7832\n",
      "Epoch 34/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.5000 - mae: 5.7682 - mse: 88.5000 - val_loss: 93.7139 - val_mae: 5.8012 - val_mse: 93.7139\n",
      "Epoch 35/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.5000 - mae: 5.7682 - mse: 88.5000 - val_loss: 93.7139 - val_mae: 5.8012 - val_mse: 93.7139\n",
      "Epoch 35/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.4076 - mae: 5.7522 - mse: 88.4076 - val_loss: 94.1004 - val_mae: 5.8027 - val_mse: 94.1004\n",
      "Epoch 36/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.4076 - mae: 5.7522 - mse: 88.4076 - val_loss: 94.1004 - val_mae: 5.8027 - val_mse: 94.1004\n",
      "Epoch 36/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.2397 - mae: 5.7528 - mse: 88.2397 - val_loss: 94.1844 - val_mae: 5.8434 - val_mse: 94.1844\n",
      "Epoch 37/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.2397 - mae: 5.7528 - mse: 88.2397 - val_loss: 94.1844 - val_mae: 5.8434 - val_mse: 94.1844\n",
      "Epoch 37/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.2039 - mae: 5.7479 - mse: 88.2039 - val_loss: 93.9547 - val_mae: 5.9332 - val_mse: 93.9547\n",
      "Epoch 38/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.2039 - mae: 5.7479 - mse: 88.2039 - val_loss: 93.9547 - val_mae: 5.9332 - val_mse: 93.9547\n",
      "Epoch 38/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 87.9856 - mae: 5.7496 - mse: 87.9856 - val_loss: 94.6536 - val_mae: 6.0385 - val_mse: 94.6536\n",
      "Epoch 39/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 87.9856 - mae: 5.7496 - mse: 87.9856 - val_loss: 94.6536 - val_mae: 6.0385 - val_mse: 94.6536\n",
      "Epoch 39/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.0245 - mae: 5.7419 - mse: 88.0245 - val_loss: 94.4730 - val_mae: 6.0537 - val_mse: 94.4730\n",
      "Epoch 40/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.0245 - mae: 5.7419 - mse: 88.0245 - val_loss: 94.4730 - val_mae: 6.0537 - val_mse: 94.4730\n",
      "Epoch 40/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.0173 - mae: 5.7509 - mse: 88.0173 - val_loss: 94.4831 - val_mae: 6.0651 - val_mse: 94.4831\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 88.0173 - mae: 5.7509 - mse: 88.0173 - val_loss: 94.4831 - val_mae: 6.0651 - val_mse: 94.4831\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 493us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Model Evaluation:\n",
      "  MAE: 6.0651\n",
      "  RMSE: 9.7202\n",
      "  R²: 0.4336\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate improved model\n",
    "advanced_model = bayes_tuner.hypermodel.build(best_bayes_hps)\n",
    "history_adv = advanced_model.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_adv = advanced_model.predict(X_test_fe).flatten()\n",
    "mae_adv = mean_absolute_error(y_test, y_pred_adv)\n",
    "rmse_adv = np.sqrt(mean_squared_error(y_test, y_pred_adv))\n",
    "r2_adv = r2_score(y_test, y_pred_adv)\n",
    "\n",
    "print(f\"\\nImproved Model Evaluation:\\n  MAE: {mae_adv:.4f}\\n  RMSE: {rmse_adv:.4f}\\n  R²: {r2_adv:.4f}\")\n",
    "advanced_model.save(os.path.join(MODEL_DIR, \"mlp_large_advanced_tuned.h5\"))\n",
    "plot_training(history_adv, \"Large MLP (Advanced)\", \"training_large_advanced_tuned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e440d5",
   "metadata": {},
   "source": [
    "## Advanced Model Improvements\n",
    "Let's try more advanced deep learning techniques: residual connections, dropout, batch normalization, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "156d30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 96.8087 - mae: 6.1104 - mse: 95.7270 - val_loss: 94.3517 - val_mae: 6.0304 - val_mse: 93.2012 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - loss: 96.8087 - mae: 6.1104 - mse: 95.7270 - val_loss: 94.3517 - val_mae: 6.0304 - val_mse: 93.2012 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 93.5180 - mae: 5.9446 - mse: 92.4254 - val_loss: 92.9527 - val_mae: 5.8540 - val_mse: 91.9389 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 93.5180 - mae: 5.9446 - mse: 92.4254 - val_loss: 92.9527 - val_mae: 5.8540 - val_mse: 91.9389 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.8391 - mae: 5.9062 - mse: 91.8947 - val_loss: 93.5872 - val_mae: 5.8761 - val_mse: 92.7123 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.8391 - mae: 5.9062 - mse: 91.8947 - val_loss: 93.5872 - val_mae: 5.8761 - val_mse: 92.7123 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.2974 - mae: 5.8830 - mse: 91.4651 - val_loss: 92.5504 - val_mae: 5.8705 - val_mse: 91.7621 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.2974 - mae: 5.8830 - mse: 91.4651 - val_loss: 92.5504 - val_mae: 5.8705 - val_mse: 91.7621 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.0253 - mae: 5.8700 - mse: 91.2731 - val_loss: 92.1564 - val_mae: 5.8713 - val_mse: 91.4489 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 92.0253 - mae: 5.8700 - mse: 91.2731 - val_loss: 92.1564 - val_mae: 5.8713 - val_mse: 91.4489 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.9044 - mae: 5.8585 - mse: 91.2175 - val_loss: 92.1168 - val_mae: 5.8307 - val_mse: 91.4462 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.9044 - mae: 5.8585 - mse: 91.2175 - val_loss: 92.1168 - val_mae: 5.8307 - val_mse: 91.4462 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.6879 - mae: 5.8497 - mse: 91.0486 - val_loss: 92.8431 - val_mae: 5.9547 - val_mse: 92.2383 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.6879 - mae: 5.8497 - mse: 91.0486 - val_loss: 92.8431 - val_mae: 5.9547 - val_mse: 92.2383 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.5104 - mae: 5.8448 - mse: 90.9197 - val_loss: 92.5566 - val_mae: 5.8375 - val_mse: 92.0010 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.5104 - mae: 5.8448 - mse: 90.9197 - val_loss: 92.5566 - val_mae: 5.8375 - val_mse: 92.0010 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.3486 - mae: 5.8403 - mse: 90.8205 - val_loss: 92.3126 - val_mae: 5.8390 - val_mse: 91.8176 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.3486 - mae: 5.8403 - mse: 90.8205 - val_loss: 92.3126 - val_mae: 5.8390 - val_mse: 91.8176 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.2609 - mae: 5.8351 - mse: 90.7841 - val_loss: 92.9061 - val_mae: 5.7961 - val_mse: 92.4424 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 91.2609 - mae: 5.8351 - mse: 90.7841 - val_loss: 92.9061 - val_mae: 5.7961 - val_mse: 92.4424 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 91.1552 - mae: 5.8300 - mse: 90.7035 - val_loss: 92.4031 - val_mae: 5.9675 - val_mse: 91.9669 - learning_rate: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 91.1552 - mae: 5.8300 - mse: 90.7035 - val_loss: 92.4031 - val_mae: 5.9675 - val_mse: 91.9669 - learning_rate: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 90.7379 - mae: 5.8118 - mse: 90.3252 - val_loss: 91.9128 - val_mae: 5.9008 - val_mse: 91.5132 - learning_rate: 5.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 90.7379 - mae: 5.8118 - mse: 90.3252 - val_loss: 91.9128 - val_mae: 5.9008 - val_mse: 91.5132 - learning_rate: 5.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 90.6599 - mae: 5.8082 - mse: 90.2695 - val_loss: 91.8155 - val_mae: 5.8250 - val_mse: 91.4318 - learning_rate: 5.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 90.6599 - mae: 5.8082 - mse: 90.2695 - val_loss: 91.8155 - val_mae: 5.8250 - val_mse: 91.4318 - learning_rate: 5.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5701 - mae: 5.8029 - mse: 90.1898 - val_loss: 91.8487 - val_mae: 5.8351 - val_mse: 91.4725 - learning_rate: 5.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5701 - mae: 5.8029 - mse: 90.1898 - val_loss: 91.8487 - val_mae: 5.8351 - val_mse: 91.4725 - learning_rate: 5.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5505 - mae: 5.8074 - mse: 90.1777 - val_loss: 91.7306 - val_mae: 5.8119 - val_mse: 91.3600 - learning_rate: 5.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5505 - mae: 5.8074 - mse: 90.1777 - val_loss: 91.7306 - val_mae: 5.8119 - val_mse: 91.3600 - learning_rate: 5.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5026 - mae: 5.8037 - mse: 90.1356 - val_loss: 91.7111 - val_mae: 5.8356 - val_mse: 91.3457 - learning_rate: 5.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.5026 - mae: 5.8037 - mse: 90.1356 - val_loss: 91.7111 - val_mae: 5.8356 - val_mse: 91.3457 - learning_rate: 5.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.4934 - mae: 5.8030 - mse: 90.1297 - val_loss: 91.8571 - val_mae: 5.8239 - val_mse: 91.4947 - learning_rate: 5.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.4934 - mae: 5.8030 - mse: 90.1297 - val_loss: 91.8571 - val_mae: 5.8239 - val_mse: 91.4947 - learning_rate: 5.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.4460 - mae: 5.8028 - mse: 90.0839 - val_loss: 91.8198 - val_mae: 5.8277 - val_mse: 91.4583 - learning_rate: 5.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.4460 - mae: 5.8028 - mse: 90.0839 - val_loss: 91.8198 - val_mae: 5.8277 - val_mse: 91.4583 - learning_rate: 5.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3760 - mae: 5.7987 - mse: 90.0147 - val_loss: 92.0315 - val_mae: 5.8197 - val_mse: 91.6719 - learning_rate: 5.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3760 - mae: 5.7987 - mse: 90.0147 - val_loss: 92.0315 - val_mae: 5.8197 - val_mse: 91.6719 - learning_rate: 5.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3847 - mae: 5.7953 - mse: 90.0244 - val_loss: 91.8788 - val_mae: 5.8534 - val_mse: 91.5176 - learning_rate: 5.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3847 - mae: 5.7953 - mse: 90.0244 - val_loss: 91.8788 - val_mae: 5.8534 - val_mse: 91.5176 - learning_rate: 5.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3376 - mae: 5.7945 - mse: 89.9758 - val_loss: 91.9595 - val_mae: 5.7963 - val_mse: 91.5970 - learning_rate: 5.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 90.3376 - mae: 5.7945 - mse: 89.9758 - val_loss: 91.9595 - val_mae: 5.7963 - val_mse: 91.5970 - learning_rate: 5.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 90.0611 - mae: 5.7870 - mse: 89.7030 - val_loss: 91.7869 - val_mae: 5.7979 - val_mse: 91.4322 - learning_rate: 2.5000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 90.0611 - mae: 5.7870 - mse: 89.7030 - val_loss: 91.7869 - val_mae: 5.7979 - val_mse: 91.4322 - learning_rate: 2.5000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 89.9748 - mae: 5.7823 - mse: 89.6204 - val_loss: 91.9024 - val_mae: 5.8179 - val_mse: 91.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 89.9748 - mae: 5.7823 - mse: 89.6204 - val_loss: 91.9024 - val_mae: 5.8179 - val_mse: 91.5490 - learning_rate: 2.5000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 89.9239 - mae: 5.7825 - mse: 89.5705 - val_loss: 92.0181 - val_mae: 5.8672 - val_mse: 91.6642 - learning_rate: 2.5000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - loss: 89.9239 - mae: 5.7825 - mse: 89.5705 - val_loss: 92.0181 - val_mae: 5.8672 - val_mse: 91.6642 - learning_rate: 2.5000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.8845 - mae: 5.7814 - mse: 89.5312 - val_loss: 91.9199 - val_mae: 5.8404 - val_mse: 91.5664 - learning_rate: 2.5000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.8845 - mae: 5.7814 - mse: 89.5312 - val_loss: 91.9199 - val_mae: 5.8404 - val_mse: 91.5664 - learning_rate: 2.5000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.7830 - mae: 5.7793 - mse: 89.4294 - val_loss: 91.9491 - val_mae: 5.8030 - val_mse: 91.5956 - learning_rate: 2.5000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.7830 - mae: 5.7793 - mse: 89.4294 - val_loss: 91.9491 - val_mae: 5.8030 - val_mse: 91.5956 - learning_rate: 2.5000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.5745 - mae: 5.7706 - mse: 89.2224 - val_loss: 92.1092 - val_mae: 5.8398 - val_mse: 91.7575 - learning_rate: 1.2500e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.5745 - mae: 5.7706 - mse: 89.2224 - val_loss: 92.1092 - val_mae: 5.8398 - val_mse: 91.7575 - learning_rate: 1.2500e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.5218 - mae: 5.7727 - mse: 89.1704 - val_loss: 92.1057 - val_mae: 5.8063 - val_mse: 91.7549 - learning_rate: 1.2500e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.5218 - mae: 5.7727 - mse: 89.1704 - val_loss: 92.1057 - val_mae: 5.8063 - val_mse: 91.7549 - learning_rate: 1.2500e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.4739 - mae: 5.7701 - mse: 89.1228 - val_loss: 92.1731 - val_mae: 5.8503 - val_mse: 91.8218 - learning_rate: 1.2500e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.4739 - mae: 5.7701 - mse: 89.1228 - val_loss: 92.1731 - val_mae: 5.8503 - val_mse: 91.8218 - learning_rate: 1.2500e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.3681 - mae: 5.7675 - mse: 89.0169 - val_loss: 92.3614 - val_mae: 5.8617 - val_mse: 92.0099 - learning_rate: 1.2500e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.3681 - mae: 5.7675 - mse: 89.0169 - val_loss: 92.3614 - val_mae: 5.8617 - val_mse: 92.0099 - learning_rate: 1.2500e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.3119 - mae: 5.7665 - mse: 88.9602 - val_loss: 92.3058 - val_mae: 5.8365 - val_mse: 91.9540 - learning_rate: 1.2500e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.3119 - mae: 5.7665 - mse: 88.9602 - val_loss: 92.3058 - val_mae: 5.8365 - val_mse: 91.9540 - learning_rate: 1.2500e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.1634 - mae: 5.7626 - mse: 88.8121 - val_loss: 92.4991 - val_mae: 5.8065 - val_mse: 92.1484 - learning_rate: 6.2500e-05\n",
      "Epoch 33/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.1634 - mae: 5.7626 - mse: 88.8121 - val_loss: 92.4991 - val_mae: 5.8065 - val_mse: 92.1484 - learning_rate: 6.2500e-05\n",
      "Epoch 33/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.1367 - mae: 5.7594 - mse: 88.7860 - val_loss: 92.5173 - val_mae: 5.8384 - val_mse: 92.1667 - learning_rate: 6.2500e-05\n",
      "Epoch 34/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.1367 - mae: 5.7594 - mse: 88.7860 - val_loss: 92.5173 - val_mae: 5.8384 - val_mse: 92.1667 - learning_rate: 6.2500e-05\n",
      "Epoch 34/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.0957 - mae: 5.7607 - mse: 88.7449 - val_loss: 92.7447 - val_mae: 5.8436 - val_mse: 92.3940 - learning_rate: 6.2500e-05\n",
      "Epoch 35/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.0957 - mae: 5.7607 - mse: 88.7449 - val_loss: 92.7447 - val_mae: 5.8436 - val_mse: 92.3940 - learning_rate: 6.2500e-05\n",
      "Epoch 35/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.0011 - mae: 5.7596 - mse: 88.6502 - val_loss: 92.7024 - val_mae: 5.8512 - val_mse: 92.3517 - learning_rate: 6.2500e-05\n",
      "Epoch 36/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 89.0011 - mae: 5.7596 - mse: 88.6502 - val_loss: 92.7024 - val_mae: 5.8512 - val_mse: 92.3517 - learning_rate: 6.2500e-05\n",
      "Epoch 36/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.9939 - mae: 5.7594 - mse: 88.6433 - val_loss: 92.8235 - val_mae: 5.8185 - val_mse: 92.4726 - learning_rate: 6.2500e-05\n",
      "Epoch 37/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.9939 - mae: 5.7594 - mse: 88.6433 - val_loss: 92.8235 - val_mae: 5.8185 - val_mse: 92.4726 - learning_rate: 6.2500e-05\n",
      "Epoch 37/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.8582 - mae: 5.7491 - mse: 88.5075 - val_loss: 92.9188 - val_mae: 5.8529 - val_mse: 92.5681 - learning_rate: 3.1250e-05\n",
      "Epoch 38/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.8582 - mae: 5.7491 - mse: 88.5075 - val_loss: 92.9188 - val_mae: 5.8529 - val_mse: 92.5681 - learning_rate: 3.1250e-05\n",
      "Epoch 38/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.8225 - mae: 5.7560 - mse: 88.4719 - val_loss: 93.0848 - val_mae: 5.8401 - val_mse: 92.7341 - learning_rate: 3.1250e-05\n",
      "Epoch 39/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.8225 - mae: 5.7560 - mse: 88.4719 - val_loss: 93.0848 - val_mae: 5.8401 - val_mse: 92.7341 - learning_rate: 3.1250e-05\n",
      "Epoch 39/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.7963 - mae: 5.7536 - mse: 88.4458 - val_loss: 93.1202 - val_mae: 5.8379 - val_mse: 92.7698 - learning_rate: 3.1250e-05\n",
      "Epoch 40/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.7963 - mae: 5.7536 - mse: 88.4458 - val_loss: 93.1202 - val_mae: 5.8379 - val_mse: 92.7698 - learning_rate: 3.1250e-05\n",
      "Epoch 40/40\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.7427 - mae: 5.7535 - mse: 88.3920 - val_loss: 93.2314 - val_mae: 5.8378 - val_mse: 92.8808 - learning_rate: 3.1250e-05\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - loss: 88.7427 - mae: 5.7535 - mse: 88.3920 - val_loss: 93.2314 - val_mae: 5.8378 - val_mse: 92.8808 - learning_rate: 3.1250e-05\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 672us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 672us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Residual MLP Evaluation:\n",
      "  MAE: 5.8378\n",
      "  RMSE: 9.6375\n",
      "  R²: 0.4432\n"
     ]
    }
   ],
   "source": [
    "# Residual MLP with Dropout and BatchNorm\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_residual_mlp(input_dim, n_layers=4, units=128, dropout_rate=0.2, l2_reg=1e-4):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    for i in range(n_layers):\n",
    "        shortcut = x\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([x, shortcut])  # Residual connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "res_mlp = build_residual_mlp(X_train_fe.shape[1], n_layers=3, units=256, dropout_rate=0.3, l2_reg=1e-3)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history_res = res_mlp.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "res_pred = res_mlp.predict(X_test_fe).flatten()\n",
    "mae_res = mean_absolute_error(y_test, res_pred)\n",
    "rmse_res = np.sqrt(mean_squared_error(y_test, res_pred))\n",
    "r2_res = r2_score(y_test, res_pred)\n",
    "\n",
    "print(f\"\\nResidual MLP Evaluation:\\n  MAE: {mae_res:.4f}\\n  RMSE: {rmse_res:.4f}\\n  R²: {r2_res:.4f}\")\n",
    "res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_residual_tuned.h5\"))\n",
    "plot_training(history_res, \"Residual MLP\", \"training_residual_mlp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86780e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Model Evaluation:\n",
      "  MAE: 5.9232\n",
      "  RMSE: 9.6243\n",
      "  R²: 0.4447\n"
     ]
    }
   ],
   "source": [
    "# Ensemble: Average predictions from best models\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predict with advanced and residual models\n",
    "ensemble_preds = (y_pred_adv + res_pred) / 2\n",
    "mae_ensemble = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_test, ensemble_preds))\n",
    "r2_ensemble = r2_score(y_test, ensemble_preds)\n",
    "\n",
    "print(f\"\\nEnsemble Model Evaluation:\\n  MAE: {mae_ensemble:.4f}\\n  RMSE: {rmse_ensemble:.4f}\\n  R²: {r2_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944f8f2",
   "metadata": {},
   "source": [
    "## Next-Level Model Improvements\n",
    "We'll now try several advanced deep learning and ML techniques to further boost performance:\n",
    "- Deeper residual networks with multi-level skip connections\n",
    "- More regularization: Dropout, L1/L2, Gaussian noise\n",
    "- Advanced optimizers: AdamW, Nadam, Lookahead\n",
    "- Learning rate warmup and cyclical schedules\n",
    "- Feature selection/dimensionality reduction (PCA)\n",
    "- Ensemble stacking (combine multiple models)\n",
    "- Data augmentation (if feasible for tabular data)\n",
    "- Model uncertainty estimation (MC Dropout, quantile regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b02e8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 99.8228 - mae: 6.0948 - mse: 95.7783 - val_loss: 95.8885 - val_mae: 5.9922 - val_mse: 92.3501 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 94.7495 - mae: 5.9057 - mse: 92.1526 - val_loss: 94.2381 - val_mae: 5.9298 - val_mse: 92.2835 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 93.3804 - mae: 5.8713 - mse: 91.7001 - val_loss: 93.5348 - val_mae: 5.8081 - val_mse: 92.0518 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 92.7688 - mae: 5.8654 - mse: 91.4322 - val_loss: 93.1828 - val_mae: 5.8160 - val_mse: 91.8870 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 92.4510 - mae: 5.8504 - mse: 91.2488 - val_loss: 93.3022 - val_mae: 5.9720 - val_mse: 92.2163 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 92.2102 - mae: 5.8473 - mse: 91.1684 - val_loss: 92.6732 - val_mae: 5.9574 - val_mse: 91.6410 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.9738 - mae: 5.8437 - mse: 91.0684 - val_loss: 92.6029 - val_mae: 5.8323 - val_mse: 91.7949 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.7903 - mae: 5.8422 - mse: 91.0468 - val_loss: 92.3787 - val_mae: 5.8954 - val_mse: 91.5691 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.6538 - mae: 5.8403 - mse: 90.9701 - val_loss: 92.2698 - val_mae: 5.8432 - val_mse: 91.6619 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.4867 - mae: 5.8374 - mse: 90.9115 - val_loss: 91.9189 - val_mae: 5.8768 - val_mse: 91.4147 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 91.3705 - mae: 5.8332 - mse: 90.8123 - val_loss: 91.9394 - val_mae: 5.8184 - val_mse: 91.4568 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.2590 - mae: 5.8318 - mse: 90.7734 - val_loss: 92.0443 - val_mae: 5.8769 - val_mse: 91.5817 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.2243 - mae: 5.8299 - mse: 90.7779 - val_loss: 91.9199 - val_mae: 5.8142 - val_mse: 91.4795 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 91.0932 - mae: 5.8291 - mse: 90.6608 - val_loss: 92.3505 - val_mae: 5.8537 - val_mse: 91.9236 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0771 - mae: 5.8305 - mse: 90.6405 - val_loss: 91.7918 - val_mae: 5.8488 - val_mse: 91.3731 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0469 - mae: 5.8283 - mse: 90.6400 - val_loss: 91.9302 - val_mae: 5.8494 - val_mse: 91.5212 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0550 - mae: 5.8286 - mse: 90.6329 - val_loss: 92.6815 - val_mae: 5.8003 - val_mse: 92.2657 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.9794 - mae: 5.8255 - mse: 90.5882 - val_loss: 92.0936 - val_mae: 5.8497 - val_mse: 91.6808 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.9652 - mae: 5.8258 - mse: 90.5800 - val_loss: 91.8916 - val_mae: 5.8489 - val_mse: 91.5127 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.9206 - mae: 5.8250 - mse: 90.5436 - val_loss: 91.8778 - val_mae: 5.8699 - val_mse: 91.5219 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.5216 - mae: 5.8073 - mse: 90.1888 - val_loss: 91.7146 - val_mae: 5.8184 - val_mse: 91.3898 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.4718 - mae: 5.8043 - mse: 90.1488 - val_loss: 91.7472 - val_mae: 5.8467 - val_mse: 91.4325 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.4629 - mae: 5.8047 - mse: 90.1487 - val_loss: 91.6612 - val_mae: 5.8342 - val_mse: 91.3434 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.4034 - mae: 5.8032 - mse: 90.0940 - val_loss: 92.0491 - val_mae: 5.8698 - val_mse: 91.7452 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3867 - mae: 5.8033 - mse: 90.0812 - val_loss: 91.7574 - val_mae: 5.8759 - val_mse: 91.4517 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3542 - mae: 5.8014 - mse: 90.0484 - val_loss: 91.8765 - val_mae: 5.8342 - val_mse: 91.5735 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3272 - mae: 5.8032 - mse: 90.0264 - val_loss: 91.7452 - val_mae: 5.8469 - val_mse: 91.4528 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.3457 - mae: 5.8009 - mse: 90.0519 - val_loss: 92.0543 - val_mae: 5.8166 - val_mse: 91.7579 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0788 - mae: 5.7910 - mse: 89.7942 - val_loss: 91.6858 - val_mae: 5.8530 - val_mse: 91.4043 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0472 - mae: 5.7909 - mse: 89.7666 - val_loss: 91.6735 - val_mae: 5.8186 - val_mse: 91.3946 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0227 - mae: 5.7900 - mse: 89.7430 - val_loss: 91.7160 - val_mae: 5.8241 - val_mse: 91.4404 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0209 - mae: 5.7884 - mse: 89.7454 - val_loss: 91.6743 - val_mae: 5.8582 - val_mse: 91.3991 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0281 - mae: 5.7909 - mse: 89.7550 - val_loss: 91.7191 - val_mae: 5.8431 - val_mse: 91.4483 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8660 - mae: 5.7825 - mse: 89.5978 - val_loss: 91.6468 - val_mae: 5.8159 - val_mse: 91.3794 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8371 - mae: 5.7804 - mse: 89.5712 - val_loss: 91.7240 - val_mae: 5.8037 - val_mse: 91.4591 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8425 - mae: 5.7804 - mse: 89.5782 - val_loss: 91.6936 - val_mae: 5.8360 - val_mse: 91.4287 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8249 - mae: 5.7804 - mse: 89.5614 - val_loss: 91.6772 - val_mae: 5.8195 - val_mse: 91.4144 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.8216 - mae: 5.7810 - mse: 89.5588 - val_loss: 91.6856 - val_mae: 5.8170 - val_mse: 91.4242 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8219 - mae: 5.7817 - mse: 89.5605 - val_loss: 91.6556 - val_mae: 5.8246 - val_mse: 91.3946 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7228 - mae: 5.7761 - mse: 89.4631 - val_loss: 91.6753 - val_mae: 5.8217 - val_mse: 91.4170 - learning_rate: 6.2500e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7001 - mae: 5.7765 - mse: 89.4419 - val_loss: 91.6795 - val_mae: 5.8244 - val_mse: 91.4217 - learning_rate: 6.2500e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7079 - mae: 5.7769 - mse: 89.4500 - val_loss: 91.6992 - val_mae: 5.8156 - val_mse: 91.4416 - learning_rate: 6.2500e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7070 - mae: 5.7765 - mse: 89.4493 - val_loss: 91.7197 - val_mae: 5.8024 - val_mse: 91.4626 - learning_rate: 6.2500e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6715 - mae: 5.7757 - mse: 89.4144 - val_loss: 91.6938 - val_mae: 5.8337 - val_mse: 91.4371 - learning_rate: 6.2500e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.6555 - mae: 5.7786 - mse: 89.4000 - val_loss: 91.7080 - val_mae: 5.8045 - val_mse: 91.4532 - learning_rate: 3.1250e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.6257 - mae: 5.7741 - mse: 89.3705 - val_loss: 91.6895 - val_mae: 5.8232 - val_mse: 91.4347 - learning_rate: 3.1250e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6201 - mae: 5.7728 - mse: 89.3657 - val_loss: 91.7196 - val_mae: 5.8160 - val_mse: 91.4655 - learning_rate: 3.1250e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6351 - mae: 5.7756 - mse: 89.3811 - val_loss: 91.6984 - val_mae: 5.8398 - val_mse: 91.4444 - learning_rate: 3.1250e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.5880 - mae: 5.7716 - mse: 89.3343 - val_loss: 91.7016 - val_mae: 5.8337 - val_mse: 91.4479 - learning_rate: 3.1250e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.5943 - mae: 5.7743 - mse: 89.3414 - val_loss: 91.7121 - val_mae: 5.8221 - val_mse: 91.4594 - learning_rate: 1.5625e-05\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 720us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deeper Residual MLP Evaluation:\n",
      "  MAE: 5.8221\n",
      "  RMSE: 9.5634\n",
      "  R²: 0.4517\n"
     ]
    }
   ],
   "source": [
    "# Deeper Residual Network with Multi-Level Skip Connections and More Regularization\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers import AdamW, Nadam\n",
    "\n",
    "def build_deep_residual_mlp(input_dim, n_layers=6, units=128, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-4, noise_std=0.05):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = GaussianNoise(noise_std)(inputs)\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "    skip = x\n",
    "    for i in range(n_layers):\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        if i % 2 == 1:\n",
    "            x = layers.Add()([x, skip])  # Multi-level skip connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=AdamW(), loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Train deeper residual model\n",
    "advanced_res_mlp = build_deep_residual_mlp(X_train_fe.shape[1], n_layers=6, units=256, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-3, noise_std=0.05)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history_adv_res = advanced_res_mlp.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "adv_res_pred = advanced_res_mlp.predict(X_test_fe).flatten()\n",
    "mae_adv_res = mean_absolute_error(y_test, adv_res_pred)\n",
    "rmse_adv_res = np.sqrt(mean_squared_error(y_test, adv_res_pred))\n",
    "r2_adv_res = r2_score(y_test, adv_res_pred)\n",
    "\n",
    "print(f\"\\nDeeper Residual MLP Evaluation:\\n  MAE: {mae_adv_res:.4f}\\n  RMSE: {rmse_adv_res:.4f}\\n  R²: {r2_adv_res:.4f}\")\n",
    "advanced_res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_deep_residual_tuned.h5\"))\n",
    "plot_training(history_adv_res, \"Deep Residual MLP\", \"training_deep_residual_mlp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "886424dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Learning Rate Callback\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=1e-4, max_lr=1e-2, step_size=2000):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.iterations = 0\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        cycle = np.floor(1 + self.iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.iterations / self.step_size - 2 * cycle + 1)\n",
    "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        self.iterations += 1\n",
    "\n",
    "# Example usage:\n",
    "# clr = CyclicLR(base_lr=1e-4, max_lr=1e-2, step_size=1000)\n",
    "# history = model.fit(..., callbacks=[clr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1fbc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reduced shape: X_train_pca (360000, 38), X_test_pca (90000, 38)\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection: Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_fe)\n",
    "X_test_pca = pca.transform(X_test_fe)\n",
    "\n",
    "print(f\"PCA reduced shape: X_train_pca {X_train_pca.shape}, X_test_pca {X_test_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "055af79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked Ensemble Evaluation:\n",
      "  MAE: 5.8465\n",
      "  RMSE: 9.5624\n",
      "  R²: 0.4518\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Stacking: Combine Multiple Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume you have predictions from several models\n",
    "# y_pred_adv, res_pred, adv_res_pred (from previous cells)\n",
    "\n",
    "stacked_preds = np.vstack([\n",
    "    y_pred_adv,\n",
    "    res_pred,\n",
    "    adv_res_pred\n",
    "]).T\n",
    "\n",
    "stacker = LinearRegression()\n",
    "stacker.fit(stacked_preds, y_test)\n",
    "ensemble_stacked = stacker.predict(stacked_preds)\n",
    "\n",
    "mae_stacked = mean_absolute_error(y_test, ensemble_stacked)\n",
    "rmse_stacked = np.sqrt(mean_squared_error(y_test, ensemble_stacked))\n",
    "r2_stacked = r2_score(y_test, ensemble_stacked)\n",
    "\n",
    "print(f\"\\nStacked Ensemble Evaluation:\\n  MAE: {mae_stacked:.4f}\\n  RMSE: {rmse_stacked:.4f}\\n  R²: {r2_stacked:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf889bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce8ebe60",
   "metadata": {},
   "source": [
    "### Advanced Deep Learning Improvements\n",
    "We'll now apply:\n",
    "- MC Dropout for uncertainty estimation\n",
    "- Quantile regression for predictive intervals\n",
    "- Mixup data augmentation for tabular data\n",
    "- Learning rate warmup/scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bc196ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Dropout mean prediction: [0.06772503 0.17885922 7.824484   2.438419   0.09934095]\n",
      "MC Dropout std (uncertainty): [0.22354934 0.2279192  0.34892038 0.3731246  0.2561742 ]\n"
     ]
    }
   ],
   "source": [
    "# MC Dropout for Uncertainty Estimation\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def mc_dropout_predict(model, X, n_iter=100):\n",
    "    # Enable dropout at inference\n",
    "    f = Model(model.input, model.output)\n",
    "    preds = [f(X, training=True).numpy().flatten() for _ in range(n_iter)]\n",
    "    preds = np.array(preds)\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    std_pred = preds.std(axis=0)\n",
    "    return mean_pred, std_pred\n",
    "\n",
    "# Example usage:\n",
    "mean_pred, std_pred = mc_dropout_predict(advanced_res_mlp, X_test_fe, n_iter=100)\n",
    "print(f\"MC Dropout mean prediction: {mean_pred[:5]}\")\n",
    "print(f\"MC Dropout std (uncertainty): {std_pred[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6351b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step\n",
      "Quantile interval example: low=[1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06], median=[-1.5293405e-04 -1.5293405e-04  4.4251113e+00 -1.5293405e-04\n",
      " -1.5293405e-04], high=[1.2684676e-03 1.2684676e-03 1.9968006e+01 9.7142715e+00 1.2684676e-03]\n",
      "Quantile interval example: low=[1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06], median=[-1.5293405e-04 -1.5293405e-04  4.4251113e+00 -1.5293405e-04\n",
      " -1.5293405e-04], high=[1.2684676e-03 1.2684676e-03 1.9968006e+01 9.7142715e+00 1.2684676e-03]\n"
     ]
    }
   ],
   "source": [
    "# Quantile Regression for Predictive Intervals\n",
    "# Use only tensorflow.keras for compatibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "\n",
    "# Quantile loss function for Keras\n",
    "# K.maximum and K.mean are from tensorflow.keras.backend\n",
    "\n",
    "def quantile_loss(q):\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * e, (q - 1) * e), axis=-1)\n",
    "    return loss\n",
    "\n",
    "# Build quantile model (e.g., 0.1, 0.5, 0.9)\n",
    "def build_quantile_mlp(input_dim, quantile):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss=quantile_loss(quantile))\n",
    "    return model\n",
    "\n",
    "# Train quantile models\n",
    "q_low, q_med, q_high = 0.1, 0.5, 0.9\n",
    "model_low = build_quantile_mlp(X_train_fe.shape[1], q_low)\n",
    "model_med = build_quantile_mlp(X_train_fe.shape[1], q_med)\n",
    "model_high = build_quantile_mlp(X_train_fe.shape[1], q_high)\n",
    "\n",
    "model_low.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_med.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_high.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "\n",
    "# Predict intervals\n",
    "pred_low = model_low.predict(X_test_fe).flatten()\n",
    "pred_med = model_med.predict(X_test_fe).flatten()\n",
    "pred_high = model_high.predict(X_test_fe).flatten()\n",
    "\n",
    "print(f\"Quantile interval example: low={pred_low[:5]}, median={pred_med[:5]}, high={pred_high[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0fe898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixup sample shapes: (360000, 135), (360000,)\n",
      "Epoch 1/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 85.3159 - mae: 5.7766 - mse: 82.7681 - val_loss: 95.4293 - val_mae: 6.0405 - val_mse: 93.0159\n",
      "Epoch 2/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 85.3159 - mae: 5.7766 - mse: 82.7681 - val_loss: 95.4293 - val_mae: 6.0405 - val_mse: 93.0159\n",
      "Epoch 2/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 81.4110 - mae: 5.5964 - mse: 79.4167 - val_loss: 97.4179 - val_mae: 6.1284 - val_mse: 95.8057\n",
      "Epoch 3/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 81.4110 - mae: 5.5964 - mse: 79.4167 - val_loss: 97.4179 - val_mae: 6.1284 - val_mse: 95.8057\n",
      "Epoch 3/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 80.3429 - mae: 5.5628 - mse: 78.9570 - val_loss: 94.6987 - val_mae: 6.0738 - val_mse: 93.4098\n",
      "Epoch 4/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 80.3429 - mae: 5.5628 - mse: 78.9570 - val_loss: 94.6987 - val_mae: 6.0738 - val_mse: 93.4098\n",
      "Epoch 4/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.8443 - mae: 5.5474 - mse: 78.6696 - val_loss: 92.9281 - val_mae: 5.9283 - val_mse: 91.8011\n",
      "Epoch 5/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.8443 - mae: 5.5474 - mse: 78.6696 - val_loss: 92.9281 - val_mae: 5.9283 - val_mse: 91.8011\n",
      "Epoch 5/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.5674 - mae: 5.5401 - mse: 78.5042 - val_loss: 92.8102 - val_mae: 5.8325 - val_mse: 91.7410\n",
      "Epoch 6/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.5674 - mae: 5.5401 - mse: 78.5042 - val_loss: 92.8102 - val_mae: 5.8325 - val_mse: 91.7410\n",
      "Epoch 6/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.2646 - mae: 5.5338 - mse: 78.3859 - val_loss: 92.4000 - val_mae: 5.8472 - val_mse: 91.5401\n",
      "Epoch 7/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.2646 - mae: 5.5338 - mse: 78.3859 - val_loss: 92.4000 - val_mae: 5.8472 - val_mse: 91.5401\n",
      "Epoch 7/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.0615 - mae: 5.5290 - mse: 78.2568 - val_loss: 92.4259 - val_mae: 5.8737 - val_mse: 91.6549\n",
      "Epoch 8/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.0615 - mae: 5.5290 - mse: 78.2568 - val_loss: 92.4259 - val_mae: 5.8737 - val_mse: 91.6549\n",
      "Epoch 8/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.8362 - mae: 5.5277 - mse: 78.1400 - val_loss: 92.6366 - val_mae: 5.8481 - val_mse: 91.9183\n",
      "Epoch 9/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.8362 - mae: 5.5277 - mse: 78.1400 - val_loss: 92.6366 - val_mae: 5.8481 - val_mse: 91.9183\n",
      "Epoch 9/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.7466 - mae: 5.5276 - mse: 78.0876 - val_loss: 92.6769 - val_mae: 5.9848 - val_mse: 92.0112\n",
      "Epoch 10/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.7466 - mae: 5.5276 - mse: 78.0876 - val_loss: 92.6769 - val_mae: 5.9848 - val_mse: 92.0112\n",
      "Epoch 10/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.5818 - mae: 5.5225 - mse: 77.9399 - val_loss: 92.3640 - val_mae: 5.9420 - val_mse: 91.7404\n",
      "Epoch 11/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.5818 - mae: 5.5225 - mse: 77.9399 - val_loss: 92.3640 - val_mae: 5.9420 - val_mse: 91.7404\n",
      "Epoch 11/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.4267 - mae: 5.5189 - mse: 77.8541 - val_loss: 92.7150 - val_mae: 5.9120 - val_mse: 92.2120\n",
      "Epoch 12/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.4267 - mae: 5.5189 - mse: 77.8541 - val_loss: 92.7150 - val_mae: 5.9120 - val_mse: 92.2120\n",
      "Epoch 12/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.2213 - mae: 5.5132 - mse: 77.7259 - val_loss: 92.6047 - val_mae: 5.8272 - val_mse: 92.1385\n",
      "Epoch 13/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.2213 - mae: 5.5132 - mse: 77.7259 - val_loss: 92.6047 - val_mae: 5.8272 - val_mse: 92.1385\n",
      "Epoch 13/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.1383 - mae: 5.5126 - mse: 77.6697 - val_loss: 92.6841 - val_mae: 5.8005 - val_mse: 92.2377\n",
      "Epoch 14/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.1383 - mae: 5.5126 - mse: 77.6697 - val_loss: 92.6841 - val_mae: 5.8005 - val_mse: 92.2377\n",
      "Epoch 14/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.9559 - mae: 5.5095 - mse: 77.4966 - val_loss: 92.5615 - val_mae: 5.8853 - val_mse: 92.0937\n",
      "Epoch 15/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.9559 - mae: 5.5095 - mse: 77.4966 - val_loss: 92.5615 - val_mae: 5.8853 - val_mse: 92.0937\n",
      "Epoch 15/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.8510 - mae: 5.5076 - mse: 77.4042 - val_loss: 92.5182 - val_mae: 5.8942 - val_mse: 92.0935\n",
      "Epoch 16/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.8510 - mae: 5.5076 - mse: 77.4042 - val_loss: 92.5182 - val_mae: 5.8942 - val_mse: 92.0935\n",
      "Epoch 16/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.7806 - mae: 5.5064 - mse: 77.3441 - val_loss: 92.4383 - val_mae: 5.9071 - val_mse: 91.9713\n",
      "Epoch 17/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.7806 - mae: 5.5064 - mse: 77.3441 - val_loss: 92.4383 - val_mae: 5.9071 - val_mse: 91.9713\n",
      "Epoch 17/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6870 - mae: 5.5052 - mse: 77.2584 - val_loss: 92.5702 - val_mae: 5.8549 - val_mse: 92.1644\n",
      "Epoch 18/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6870 - mae: 5.5052 - mse: 77.2584 - val_loss: 92.5702 - val_mae: 5.8549 - val_mse: 92.1644\n",
      "Epoch 18/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6162 - mae: 5.5032 - mse: 77.1851 - val_loss: 92.8932 - val_mae: 5.9707 - val_mse: 92.4700\n",
      "Epoch 19/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6162 - mae: 5.5032 - mse: 77.1851 - val_loss: 92.8932 - val_mae: 5.9707 - val_mse: 92.4700\n",
      "Epoch 19/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.4621 - mae: 5.4967 - mse: 77.0256 - val_loss: 92.9643 - val_mae: 5.9739 - val_mse: 92.5410\n",
      "Epoch 20/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.4621 - mae: 5.4967 - mse: 77.0256 - val_loss: 92.9643 - val_mae: 5.9739 - val_mse: 92.5410\n",
      "Epoch 20/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.4020 - mae: 5.4954 - mse: 76.9423 - val_loss: 92.9174 - val_mae: 5.9209 - val_mse: 92.4612\n",
      "Epoch 21/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.4020 - mae: 5.4954 - mse: 76.9423 - val_loss: 92.9174 - val_mae: 5.9209 - val_mse: 92.4612\n",
      "Epoch 21/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.3191 - mae: 5.4932 - mse: 76.8789 - val_loss: 93.3338 - val_mae: 5.8462 - val_mse: 92.9052\n",
      "Epoch 22/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.3191 - mae: 5.4932 - mse: 76.8789 - val_loss: 93.3338 - val_mae: 5.8462 - val_mse: 92.9052\n",
      "Epoch 22/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.2090 - mae: 5.4923 - mse: 76.7815 - val_loss: 92.9022 - val_mae: 6.0293 - val_mse: 92.4709\n",
      "Epoch 23/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.2090 - mae: 5.4923 - mse: 76.7815 - val_loss: 92.9022 - val_mae: 6.0293 - val_mse: 92.4709\n",
      "Epoch 23/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.1022 - mae: 5.4887 - mse: 76.6751 - val_loss: 92.9234 - val_mae: 5.8257 - val_mse: 92.5149\n",
      "Epoch 24/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.1022 - mae: 5.4887 - mse: 76.6751 - val_loss: 92.9234 - val_mae: 5.8257 - val_mse: 92.5149\n",
      "Epoch 24/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.0270 - mae: 5.4900 - mse: 76.6172 - val_loss: 92.9699 - val_mae: 5.8482 - val_mse: 92.5620\n",
      "Epoch 25/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.0270 - mae: 5.4900 - mse: 76.6172 - val_loss: 92.9699 - val_mae: 5.8482 - val_mse: 92.5620\n",
      "Epoch 25/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.0064 - mae: 5.4867 - mse: 76.5931 - val_loss: 93.3832 - val_mae: 5.8459 - val_mse: 92.9711\n",
      "Epoch 26/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.0064 - mae: 5.4867 - mse: 76.5931 - val_loss: 93.3832 - val_mae: 5.8459 - val_mse: 92.9711\n",
      "Epoch 26/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 76.9397 - mae: 5.4835 - mse: 76.5212 - val_loss: 93.3414 - val_mae: 5.9000 - val_mse: 92.9221\n",
      "Epoch 27/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 76.9397 - mae: 5.4835 - mse: 76.5212 - val_loss: 93.3414 - val_mae: 5.9000 - val_mse: 92.9221\n",
      "Epoch 27/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.8812 - mae: 5.4833 - mse: 76.4612 - val_loss: 93.3887 - val_mae: 5.9511 - val_mse: 92.9733\n",
      "Epoch 28/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.8812 - mae: 5.4833 - mse: 76.4612 - val_loss: 93.3887 - val_mae: 5.9511 - val_mse: 92.9733\n",
      "Epoch 28/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7457 - mae: 5.4796 - mse: 76.3395 - val_loss: 93.1661 - val_mae: 5.9000 - val_mse: 92.7570\n",
      "Epoch 29/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7457 - mae: 5.4796 - mse: 76.3395 - val_loss: 93.1661 - val_mae: 5.9000 - val_mse: 92.7570\n",
      "Epoch 29/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7468 - mae: 5.4783 - mse: 76.3430 - val_loss: 93.2295 - val_mae: 5.9006 - val_mse: 92.8319\n",
      "Epoch 30/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7468 - mae: 5.4783 - mse: 76.3430 - val_loss: 93.2295 - val_mae: 5.9006 - val_mse: 92.8319\n",
      "Epoch 30/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 76.7161 - mae: 5.4782 - mse: 76.3096 - val_loss: 93.8141 - val_mae: 5.9384 - val_mse: 93.4154\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 76.7161 - mae: 5.4782 - mse: 76.3096 - val_loss: 93.8141 - val_mae: 5.9384 - val_mse: 93.4154\n"
     ]
    }
   ],
   "source": [
    "# Mixup Data Augmentation for Tabular Data\n",
    "import numpy as np\n",
    "\n",
    "def mixup(X, y, alpha=0.2):\n",
    "    '''Mixup augmentation for tabular data.'''\n",
    "    n_samples = X.shape[0]\n",
    "    lam = np.random.beta(alpha, alpha, n_samples)\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X_mix = lam[:, None] * X + (1 - lam)[:, None] * X[idx]\n",
    "    y_mix = lam * y + (1 - lam) * y[idx]\n",
    "    return X_mix, y_mix\n",
    "\n",
    "# Example usage:\n",
    "X_train_mix, y_train_mix = mixup(X_train_fe, y_train, alpha=0.2)\n",
    "print(f\"Mixup sample shapes: {X_train_mix.shape}, {y_train_mix.shape}\")\n",
    "# Train model on mixup data\n",
    "mixup_model = build_deep_residual_mlp(X_train_mix.shape[1], n_layers=6, units=256)\n",
    "history_mixup = mixup_model.fit(X_train_mix, y_train_mix, validation_data=(X_test_fe, y_test), epochs=30, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c5b7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 716us/step\n",
      "\n",
      "Mixup Model Evaluation:\n",
      "  MAE: 5.9384\n",
      "  RMSE: 9.6652\n",
      "  R²: 0.4400\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Mixup Model Performance\n",
    "y_pred_mixup = mixup_model.predict(X_test_fe).flatten()\n",
    "mae_mixup = mean_absolute_error(y_test, y_pred_mixup)\n",
    "rmse_mixup = np.sqrt(mean_squared_error(y_test, y_pred_mixup))\n",
    "r2_mixup = r2_score(y_test, y_pred_mixup)\n",
    "\n",
    "print(f\"\\nMixup Model Evaluation:\\n  MAE: {mae_mixup:.4f}\\n  RMSE: {rmse_mixup:.4f}\\n  R²: {r2_mixup:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neural-pricer venv)",
   "language": "python",
   "name": "neural-pricer-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
