{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f79724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting keras_tuner\n",
      "  Using cached keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Collecting kt-legacy (from keras_tuner)\n",
      "  Using cached kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Using cached keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "Using cached kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras_tuner\n",
      "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras tensorflow --upgrade\n",
    "!{sys.executable} -m pip install keras_tuner\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f994c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/raw/training_data.npz\"\n",
    "SCALER_DIR = \"../data/processed/scalers\"\n",
    "MODEL_DIR = \"../src/models\"\n",
    "FIGURE_DIR = \"../src/visualization/plots\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17623456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: X shape (450000, 15), y shape (450000,)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path=DATA_PATH):\n",
    "    \"\"\"Load dataset from NPZ file.\"\"\"\n",
    "    data = np.load(path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    print(f\"Loaded dataset: X shape {X.shape}, y shape {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X, y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9110167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (360000, 15), Test set: (90000, 15)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits into train/test and normalizes features.\n",
    "    Prefix prices + numeric features are scaled.\n",
    "    opt_flag is kept as-is (categorical 0/1).\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Separate opt_flag (last column)\n",
    "    X_train_prefix = X_train[:, :-1]\n",
    "    X_test_prefix = X_test[:, :-1]\n",
    "\n",
    "    opt_flag_train = X_train[:, -1].reshape(-1, 1)\n",
    "    opt_flag_test = X_test[:, -1].reshape(-1, 1)\n",
    "\n",
    "    # Scale everything except opt_flag\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_prefix)\n",
    "    X_test_scaled = scaler.transform(X_test_prefix)\n",
    "\n",
    "    # Reattach opt_flag\n",
    "    X_train_final = np.hstack([X_train_scaled, opt_flag_train])\n",
    "    X_test_final = np.hstack([X_test_scaled, opt_flag_test])\n",
    "\n",
    "    # Save scaler for later inference\n",
    "    os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "    joblib.dump(scaler, os.path.join(SCALER_DIR, \"feature_scaler.pkl\"))\n",
    "\n",
    "    print(f\"Train set: {X_train_final.shape}, Test set: {X_test_final.shape}\")\n",
    "    return X_train_final, X_test_final, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cef6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f90b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history, title, filename):\n",
    "    \"\"\"Save training curves for loss + MAE + RMSE.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "    plt.title(f\"{title} - MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    # RMSE plot (computed from loss)\n",
    "    train_rmse = np.sqrt(history.history[\"loss\"])\n",
    "    val_rmse = np.sqrt(history.history[\"val_loss\"])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_rmse, label=\"Train RMSE\")\n",
    "    plt.plot(val_rmse, label=\"Val RMSE\")\n",
    "    plt.title(f\"{title} - RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root MSE\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURE_DIR, filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0165c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 761us/step - loss: 95.4658 - mae: 6.0945 - val_loss: 91.7508 - val_mae: 5.9288\n",
      "Epoch 2/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 742us/step - loss: 91.3731 - mae: 5.8502 - val_loss: 91.5185 - val_mae: 5.9363\n",
      "Epoch 3/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 735us/step - loss: 91.1271 - mae: 5.8367 - val_loss: 91.7755 - val_mae: 5.9679\n",
      "Epoch 4/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 760us/step - loss: 91.0568 - mae: 5.8293 - val_loss: 92.8673 - val_mae: 6.0017\n",
      "Epoch 5/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 740us/step - loss: 91.0015 - mae: 5.8252 - val_loss: 91.1833 - val_mae: 5.8462\n",
      "Epoch 6/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 740us/step - loss: 90.8985 - mae: 5.8194 - val_loss: 92.1096 - val_mae: 5.9459\n",
      "Epoch 7/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 776us/step - loss: 90.8768 - mae: 5.8172 - val_loss: 91.5798 - val_mae: 5.8945\n",
      "Epoch 8/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 792us/step - loss: 90.8268 - mae: 5.8146 - val_loss: 91.4256 - val_mae: 5.8467\n",
      "Epoch 9/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 749us/step - loss: 90.7690 - mae: 5.8155 - val_loss: 91.5163 - val_mae: 5.8198\n",
      "Epoch 10/10\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 758us/step - loss: 90.7354 - mae: 5.8103 - val_loss: 91.2678 - val_mae: 5.8318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 369us/step\n",
      "ðŸŽ¯ Evaluation on Test Set -> MAE: 5.8318, RMSE: 9.5534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(5.831829638353275), np.float64(9.553417440466928))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_quick_mlp(input_dim):\n",
    "    \"\"\"Small, quick baseline MLP.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Compute MAE and RMSE on test set.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred.flatten()) ** 2))\n",
    "    print(f\"ðŸŽ¯ Evaluation on Test Set -> MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "quick_mlp = build_quick_mlp(input_dim)\n",
    "history_quick = quick_mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "quick_mlp.save(os.path.join(MODEL_DIR, \"mlp_quick.h5\"))\n",
    "plot_training(history_quick, \"Quick MLP\", \"training_quick_run_2.png\")\n",
    "evaluate_model(quick_mlp, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc39133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_large_mlp(hp, input_dim):\n",
    "    \"\"\"Hyperparameter-tunable large MLP.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Number of layers\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 6)):\n",
    "        units = hp.Int(f\"units_{i}\", 32, 512, step=32)\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        model.add(keras.layers.Dense(units, activation=activation))\n",
    "\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    lr = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    \n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"linear\"))\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed55014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 Complete [00h 01m 45s]\n",
      "val_mae: 5.781039714813232\n",
      "\n",
      "Best val_mae So Far: 5.71572732925415\n",
      "Total elapsed time: 01h 21m 49s\n",
      "\n",
      "âœ… Best Hyperparameters found:\n",
      "  num_layers: 6\n",
      "  units_0: 352\n",
      "  activation: relu\n",
      "  units_1: 224\n",
      "  optimizer: adam\n",
      "  lr: 0.0004816603466007292\n",
      "  units_2: 448\n",
      "  units_3: 192\n",
      "  units_4: 224\n",
      "  units_5: 32\n",
      "  tuner/epochs: 10\n",
      "  tuner/initial_epoch: 0\n",
      "  tuner/bracket: 1\n",
      "  tuner/round: 0\n"
     ]
    }
   ],
   "source": [
    "# Tuner setup\n",
    "tuner = kt.Hyperband(\n",
    "    lambda hp: build_large_mlp(hp, input_dim=X_train.shape[1]),\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=30,\n",
    "    factor=3,\n",
    "    directory=\"tuner_dir\",\n",
    "    project_name=\"large_mlp\"\n",
    ")\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,                # will be tuned by Hyperband\n",
    "    batch_size=kt.HyperParameters().Int(\"batch_size\", min_value=32, max_value=256, step=32),\n",
    "    callbacks=[stop_early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters found:\")\n",
    "for param, value in best_hps.values.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Build best model\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20f0db27",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HyperParameters.get() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train final model with best params\u001b[39;00m\n\u001b[32m      2\u001b[39m history = model.fit(\n\u001b[32m      3\u001b[39m     X_train, y_train,\n\u001b[32m      4\u001b[39m     validation_data=(X_test, y_test),\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     epochs=\u001b[43mbest_hps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m      6\u001b[39m     batch_size=best_hps.get(\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m64\u001b[39m),\n\u001b[32m      7\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Evaluate final model\u001b[39;00m\n\u001b[32m     11\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[31mTypeError\u001b[39m: HyperParameters.get() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Train final model with best params\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=best_hps.get(\"max_epochs\", 50),\n",
    "    batch_size=best_hps.get(\"batch_size\", 64),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred.flatten())**2))\n",
    "mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "print(f\"\\nðŸŽ¯ Final Model Evaluation:\\n  MAE: {mae:.4f}\\n  RMSE: {rmse:.4f}\")\n",
    "\n",
    "model.save(os.path.join(MODEL_DIR, \"mlp_large_tuned.h5\"))\n",
    "plot_training(history, \"Large MLP (Tuned)\", \"training_large_tuned.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeaee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
