{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f79724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras tensorflow --upgrade\n",
    "!{sys.executable} -m pip install keras_tuner\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f994c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: X shape (50000, 20), y shape (50000,)\n",
      "Train set: (40000, 20), Test set: (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Loading and Preprocessing\n",
    "# Load the training data, apply robust scaling based on EDA, and split into train/test sets. The scaler is saved for future inference.\n",
    "DATA_PATH = \"../data/raw/training_data.npz\"\n",
    "SCALER_DIR = \"../data/processed/scalers\"\n",
    "MODEL_DIR = \"../src/models\"\n",
    "FIGURE_DIR = \"../src/visualization/plots\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)\n",
    "\n",
    "def load_and_preprocess_data(path=DATA_PATH, test_size=0.2, random_state=42, scaler_type='standard'):\n",
    "    data = np.load(path)\n",
    "    X, y = data['X'], data['y']\n",
    "    print(f\"Loaded dataset: X shape {X.shape}, y shape {y.shape}\")\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    # Separate opt_flag (last column)\n",
    "    X_train_prefix = X_train[:, :-1]\n",
    "    X_test_prefix = X_test[:, :-1]\n",
    "    opt_flag_train = X_train[:, -1].reshape(-1, 1)\n",
    "    opt_flag_test = X_test[:, -1].reshape(-1, 1)\n",
    "    # Choose scaler based on EDA\n",
    "    if scaler_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_prefix)\n",
    "    X_test_scaled = scaler.transform(X_test_prefix)\n",
    "    # Reattach opt_flag\n",
    "    X_train_final = np.hstack([X_train_scaled, opt_flag_train])\n",
    "    X_test_final = np.hstack([X_test_scaled, opt_flag_test])\n",
    "    # Save scaler for later inference\n",
    "    os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "    joblib.dump(scaler, os.path.join(SCALER_DIR, \"feature_scaler.pkl\"))\n",
    "    print(f\"Train set: {X_train_final.shape}, Test set: {X_test_final.shape}\")\n",
    "    return X_train_final, X_test_final, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_and_preprocess_data(scaler_type='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1f90b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Training Metrics Plotting\n",
    "# This function plots and saves Loss, MAE, and RMSE curves for each model during training.\n",
    "def plot_training_metrics(history, title, filename):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "    plt.title(f\"{title} - MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    # RMSE plot (computed from loss)\n",
    "    train_rmse = np.sqrt(history.history[\"loss\"])\n",
    "    val_rmse = np.sqrt(history.history[\"val_loss\"])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_rmse, label=\"Train RMSE\")\n",
    "    plt.plot(val_rmse, label=\"Val RMSE\")\n",
    "    plt.title(f\"{title} - RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root MSE\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURE_DIR, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc39133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_tuning\\tuner0.json\n",
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.0176 - mae: 8.8824 - val_loss: 92.4278 - val_mae: 6.6603\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.0176 - mae: 8.8824 - val_loss: 92.4278 - val_mae: 6.6603\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 81.7370 - mae: 6.0119 - val_loss: 54.1210 - val_mae: 4.9909\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 81.7370 - mae: 6.0119 - val_loss: 54.1210 - val_mae: 4.9909\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 21.0402 - mae: 2.7185 - val_loss: 3.3074 - val_mae: 1.2200\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 21.0402 - mae: 2.7185 - val_loss: 3.3074 - val_mae: 1.2200\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 2.7573 - mae: 1.0833 - val_loss: 1.7785 - val_mae: 0.8863\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 2.7573 - mae: 1.0833 - val_loss: 1.7785 - val_mae: 0.8863\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 1.8711 - mae: 0.8607 - val_loss: 2.0922 - val_mae: 0.9210\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 1.8711 - mae: 0.8607 - val_loss: 2.0922 - val_mae: 0.9210\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.5967 - mae: 0.7705 - val_loss: 2.0207 - val_mae: 0.9048\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.5967 - mae: 0.7705 - val_loss: 2.0207 - val_mae: 0.9048\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 1.4398 - mae: 0.7169 - val_loss: 1.6469 - val_mae: 0.7651\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 1.4398 - mae: 0.7169 - val_loss: 1.6469 - val_mae: 0.7651\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.3610 - mae: 0.6892 - val_loss: 1.3763 - val_mae: 0.7011\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.3610 - mae: 0.6892 - val_loss: 1.3763 - val_mae: 0.7011\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.3855 - mae: 0.6833 - val_loss: 1.3282 - val_mae: 0.6645\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.3855 - mae: 0.6833 - val_loss: 1.3282 - val_mae: 0.6645\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 1.2586 - mae: 0.6540 - val_loss: 1.4895 - val_mae: 0.7183\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 1.2586 - mae: 0.6540 - val_loss: 1.4895 - val_mae: 0.7183\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - loss: 1.2843 - mae: 0.6601 - val_loss: 1.5627 - val_mae: 0.7510\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - loss: 1.2843 - mae: 0.6601 - val_loss: 1.5627 - val_mae: 0.7510\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.2113 - mae: 0.6388 - val_loss: 1.1803 - val_mae: 0.6194\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.2113 - mae: 0.6388 - val_loss: 1.1803 - val_mae: 0.6194\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.1983 - mae: 0.6330 - val_loss: 1.2921 - val_mae: 0.6482\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.1983 - mae: 0.6330 - val_loss: 1.2921 - val_mae: 0.6482\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.2304 - mae: 0.6376 - val_loss: 1.3049 - val_mae: 0.6477\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.2304 - mae: 0.6376 - val_loss: 1.3049 - val_mae: 0.6477\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.1886 - mae: 0.6249 - val_loss: 1.2100 - val_mae: 0.6271\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.1886 - mae: 0.6249 - val_loss: 1.2100 - val_mae: 0.6271\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 1.1673 - mae: 0.6195 - val_loss: 1.3001 - val_mae: 0.6373\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 1.1673 - mae: 0.6195 - val_loss: 1.3001 - val_mae: 0.6373\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 1.1562 - mae: 0.6144 - val_loss: 1.3544 - val_mae: 0.6610\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 1.1562 - mae: 0.6144 - val_loss: 1.3544 - val_mae: 0.6610\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1657 - mae: 0.6143 - val_loss: 1.3075 - val_mae: 0.6364\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1657 - mae: 0.6143 - val_loss: 1.3075 - val_mae: 0.6364\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1416 - mae: 0.6082 - val_loss: 1.4621 - val_mae: 0.6627\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1416 - mae: 0.6082 - val_loss: 1.4621 - val_mae: 0.6627\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1251 - mae: 0.6012 - val_loss: 1.2508 - val_mae: 0.6262\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1251 - mae: 0.6012 - val_loss: 1.2508 - val_mae: 0.6262\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1314 - mae: 0.6027 - val_loss: 1.2768 - val_mae: 0.6344\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1314 - mae: 0.6027 - val_loss: 1.2768 - val_mae: 0.6344\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.1215 - mae: 0.6012 - val_loss: 1.1653 - val_mae: 0.6070\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.1215 - mae: 0.6012 - val_loss: 1.1653 - val_mae: 0.6070\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.1096 - mae: 0.5944 - val_loss: 1.2754 - val_mae: 0.6330\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.1096 - mae: 0.5944 - val_loss: 1.2754 - val_mae: 0.6330\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - loss: 1.1197 - mae: 0.5958 - val_loss: 1.1382 - val_mae: 0.6018\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - loss: 1.1197 - mae: 0.5958 - val_loss: 1.1382 - val_mae: 0.6018\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 1.1089 - mae: 0.5928 - val_loss: 1.0924 - val_mae: 0.5854\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 1.1089 - mae: 0.5928 - val_loss: 1.0924 - val_mae: 0.5854\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 930us/step - loss: 1.1131 - mae: 0.5925 - val_loss: 1.1501 - val_mae: 0.6010\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 930us/step - loss: 1.1131 - mae: 0.5925 - val_loss: 1.1501 - val_mae: 0.6010\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.1062 - mae: 0.5908 - val_loss: 1.1019 - val_mae: 0.5891\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.1062 - mae: 0.5908 - val_loss: 1.1019 - val_mae: 0.5891\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 1.0894 - mae: 0.5877 - val_loss: 1.2529 - val_mae: 0.6219\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 1.0894 - mae: 0.5877 - val_loss: 1.2529 - val_mae: 0.6219\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 1.0851 - mae: 0.5849 - val_loss: 1.1392 - val_mae: 0.5902\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 1.0851 - mae: 0.5849 - val_loss: 1.1392 - val_mae: 0.5902\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 1.0784 - mae: 0.5804 - val_loss: 1.4350 - val_mae: 0.6652\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 1.0784 - mae: 0.5804 - val_loss: 1.4350 - val_mae: 0.6652\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.0741 - mae: 0.5799 - val_loss: 1.1340 - val_mae: 0.5935\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.0741 - mae: 0.5799 - val_loss: 1.1340 - val_mae: 0.5935\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1.0699 - mae: 0.5790 - val_loss: 1.1686 - val_mae: 0.6036\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1.0699 - mae: 0.5790 - val_loss: 1.1686 - val_mae: 0.6036\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 1.0578 - mae: 0.5741 - val_loss: 1.2206 - val_mae: 0.6238\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 1.0578 - mae: 0.5741 - val_loss: 1.2206 - val_mae: 0.6238\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.0687 - mae: 0.5767 - val_loss: 1.4599 - val_mae: 0.6813\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.0687 - mae: 0.5767 - val_loss: 1.4599 - val_mae: 0.6813\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0602 - mae: 0.5717 - val_loss: 1.1219 - val_mae: 0.5911\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0602 - mae: 0.5717 - val_loss: 1.1219 - val_mae: 0.5911\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.0655 - mae: 0.5733 - val_loss: 1.0948 - val_mae: 0.5768\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.0655 - mae: 0.5733 - val_loss: 1.0948 - val_mae: 0.5768\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0751 - mae: 0.5766 - val_loss: 1.4900 - val_mae: 0.6971\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0751 - mae: 0.5766 - val_loss: 1.4900 - val_mae: 0.6971\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0554 - mae: 0.5710 - val_loss: 1.1238 - val_mae: 0.5882\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0554 - mae: 0.5710 - val_loss: 1.1238 - val_mae: 0.5882\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.0595 - mae: 0.5727 - val_loss: 1.1022 - val_mae: 0.5794\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.0595 - mae: 0.5727 - val_loss: 1.1022 - val_mae: 0.5794\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.0575 - mae: 0.5713 - val_loss: 1.2664 - val_mae: 0.6363\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.0575 - mae: 0.5713 - val_loss: 1.2664 - val_mae: 0.6363\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
      "Large MLP Evaluation: MAE=0.6363, RMSE=1.1254Large MLP Evaluation: MAE=0.6363, RMSE=1.1254\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_tuning\\tuner0.json\n",
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.0176 - mae: 8.8824 - val_loss: 92.4278 - val_mae: 6.6603\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.0176 - mae: 8.8824 - val_loss: 92.4278 - val_mae: 6.6603\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 81.7370 - mae: 6.0119 - val_loss: 54.1210 - val_mae: 4.9909\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 949us/step - loss: 81.7370 - mae: 6.0119 - val_loss: 54.1210 - val_mae: 4.9909\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 21.0402 - mae: 2.7185 - val_loss: 3.3074 - val_mae: 1.2200\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 21.0402 - mae: 2.7185 - val_loss: 3.3074 - val_mae: 1.2200\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 2.7573 - mae: 1.0833 - val_loss: 1.7785 - val_mae: 0.8863\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 2.7573 - mae: 1.0833 - val_loss: 1.7785 - val_mae: 0.8863\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 1.8711 - mae: 0.8607 - val_loss: 2.0922 - val_mae: 0.9210\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 1.8711 - mae: 0.8607 - val_loss: 2.0922 - val_mae: 0.9210\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.5967 - mae: 0.7705 - val_loss: 2.0207 - val_mae: 0.9048\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.5967 - mae: 0.7705 - val_loss: 2.0207 - val_mae: 0.9048\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 1.4398 - mae: 0.7169 - val_loss: 1.6469 - val_mae: 0.7651\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step - loss: 1.4398 - mae: 0.7169 - val_loss: 1.6469 - val_mae: 0.7651\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.3610 - mae: 0.6892 - val_loss: 1.3763 - val_mae: 0.7011\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.3610 - mae: 0.6892 - val_loss: 1.3763 - val_mae: 0.7011\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.3855 - mae: 0.6833 - val_loss: 1.3282 - val_mae: 0.6645\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.3855 - mae: 0.6833 - val_loss: 1.3282 - val_mae: 0.6645\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 1.2586 - mae: 0.6540 - val_loss: 1.4895 - val_mae: 0.7183\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 937us/step - loss: 1.2586 - mae: 0.6540 - val_loss: 1.4895 - val_mae: 0.7183\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - loss: 1.2843 - mae: 0.6601 - val_loss: 1.5627 - val_mae: 0.7510\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - loss: 1.2843 - mae: 0.6601 - val_loss: 1.5627 - val_mae: 0.7510\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.2113 - mae: 0.6388 - val_loss: 1.1803 - val_mae: 0.6194\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.2113 - mae: 0.6388 - val_loss: 1.1803 - val_mae: 0.6194\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.1983 - mae: 0.6330 - val_loss: 1.2921 - val_mae: 0.6482\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.1983 - mae: 0.6330 - val_loss: 1.2921 - val_mae: 0.6482\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.2304 - mae: 0.6376 - val_loss: 1.3049 - val_mae: 0.6477\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.2304 - mae: 0.6376 - val_loss: 1.3049 - val_mae: 0.6477\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.1886 - mae: 0.6249 - val_loss: 1.2100 - val_mae: 0.6271\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.1886 - mae: 0.6249 - val_loss: 1.2100 - val_mae: 0.6271\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 1.1673 - mae: 0.6195 - val_loss: 1.3001 - val_mae: 0.6373\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944us/step - loss: 1.1673 - mae: 0.6195 - val_loss: 1.3001 - val_mae: 0.6373\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 1.1562 - mae: 0.6144 - val_loss: 1.3544 - val_mae: 0.6610\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 1.1562 - mae: 0.6144 - val_loss: 1.3544 - val_mae: 0.6610\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1657 - mae: 0.6143 - val_loss: 1.3075 - val_mae: 0.6364\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1657 - mae: 0.6143 - val_loss: 1.3075 - val_mae: 0.6364\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1416 - mae: 0.6082 - val_loss: 1.4621 - val_mae: 0.6627\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1416 - mae: 0.6082 - val_loss: 1.4621 - val_mae: 0.6627\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1251 - mae: 0.6012 - val_loss: 1.2508 - val_mae: 0.6262\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 941us/step - loss: 1.1251 - mae: 0.6012 - val_loss: 1.2508 - val_mae: 0.6262\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1314 - mae: 0.6027 - val_loss: 1.2768 - val_mae: 0.6344\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 946us/step - loss: 1.1314 - mae: 0.6027 - val_loss: 1.2768 - val_mae: 0.6344\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.1215 - mae: 0.6012 - val_loss: 1.1653 - val_mae: 0.6070\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 940us/step - loss: 1.1215 - mae: 0.6012 - val_loss: 1.1653 - val_mae: 0.6070\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.1096 - mae: 0.5944 - val_loss: 1.2754 - val_mae: 0.6330\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.1096 - mae: 0.5944 - val_loss: 1.2754 - val_mae: 0.6330\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - loss: 1.1197 - mae: 0.5958 - val_loss: 1.1382 - val_mae: 0.6018\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step - loss: 1.1197 - mae: 0.5958 - val_loss: 1.1382 - val_mae: 0.6018\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 1.1089 - mae: 0.5928 - val_loss: 1.0924 - val_mae: 0.5854\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 948us/step - loss: 1.1089 - mae: 0.5928 - val_loss: 1.0924 - val_mae: 0.5854\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 930us/step - loss: 1.1131 - mae: 0.5925 - val_loss: 1.1501 - val_mae: 0.6010\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 930us/step - loss: 1.1131 - mae: 0.5925 - val_loss: 1.1501 - val_mae: 0.6010\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.1062 - mae: 0.5908 - val_loss: 1.1019 - val_mae: 0.5891\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938us/step - loss: 1.1062 - mae: 0.5908 - val_loss: 1.1019 - val_mae: 0.5891\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 1.0894 - mae: 0.5877 - val_loss: 1.2529 - val_mae: 0.6219\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 1.0894 - mae: 0.5877 - val_loss: 1.2529 - val_mae: 0.6219\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 1.0851 - mae: 0.5849 - val_loss: 1.1392 - val_mae: 0.5902\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 1.0851 - mae: 0.5849 - val_loss: 1.1392 - val_mae: 0.5902\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 1.0784 - mae: 0.5804 - val_loss: 1.4350 - val_mae: 0.6652\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 965us/step - loss: 1.0784 - mae: 0.5804 - val_loss: 1.4350 - val_mae: 0.6652\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.0741 - mae: 0.5799 - val_loss: 1.1340 - val_mae: 0.5935\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 1.0741 - mae: 0.5799 - val_loss: 1.1340 - val_mae: 0.5935\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1.0699 - mae: 0.5790 - val_loss: 1.1686 - val_mae: 0.6036\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 1.0699 - mae: 0.5790 - val_loss: 1.1686 - val_mae: 0.6036\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 1.0578 - mae: 0.5741 - val_loss: 1.2206 - val_mae: 0.6238\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 1.0578 - mae: 0.5741 - val_loss: 1.2206 - val_mae: 0.6238\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.0687 - mae: 0.5767 - val_loss: 1.4599 - val_mae: 0.6813\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951us/step - loss: 1.0687 - mae: 0.5767 - val_loss: 1.4599 - val_mae: 0.6813\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0602 - mae: 0.5717 - val_loss: 1.1219 - val_mae: 0.5911\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0602 - mae: 0.5717 - val_loss: 1.1219 - val_mae: 0.5911\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.0655 - mae: 0.5733 - val_loss: 1.0948 - val_mae: 0.5768\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.0655 - mae: 0.5733 - val_loss: 1.0948 - val_mae: 0.5768\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0751 - mae: 0.5766 - val_loss: 1.4900 - val_mae: 0.6971\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0751 - mae: 0.5766 - val_loss: 1.4900 - val_mae: 0.6971\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0554 - mae: 0.5710 - val_loss: 1.1238 - val_mae: 0.5882\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 1.0554 - mae: 0.5710 - val_loss: 1.1238 - val_mae: 0.5882\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.0595 - mae: 0.5727 - val_loss: 1.1022 - val_mae: 0.5794\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - loss: 1.0595 - mae: 0.5727 - val_loss: 1.1022 - val_mae: 0.5794\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.0575 - mae: 0.5713 - val_loss: 1.2664 - val_mae: 0.6363\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - loss: 1.0575 - mae: 0.5713 - val_loss: 1.2664 - val_mae: 0.6363\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step\n",
      "Large MLP Evaluation: MAE=0.6363, RMSE=1.1254Large MLP Evaluation: MAE=0.6363, RMSE=1.1254\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Large MLP Model (Hyperband Tuning)\n",
    "# This model uses Keras Tuner's Hyperband to optimize architecture and learning rate. It serves as a strong baseline for tabular option pricing.\n",
    "def build_large_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train.shape[1]\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    num_layers = hp.Int(\"num_layers\", min_value=2, max_value=6, step=1)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32)\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_large_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=40,\n",
    "    factor=3,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_tuning\"\n",
    "    )\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=kt.HyperParameters().Int(\"batch_size\", min_value=32, max_value=256, step=32),\n",
    "    epochs=40\n",
    "    )\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "model_large = tuner.hypermodel.build(best_hps)\n",
    "history_large = model_large.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    "    )\n",
    "y_pred_large = model_large.predict(X_test).flatten()\n",
    "mae_large = mean_absolute_error(y_test, y_pred_large)\n",
    "rmse_large = np.sqrt(mean_squared_error(y_test, y_pred_large))\n",
    "print(f\"Large MLP Evaluation: MAE={mae_large:.4f}, RMSE={rmse_large:.4f}\")\n",
    "plot_training_metrics(history_large, \"Large MLP (Tuned)\", \"training_large_tuned.png\")\n",
    "model_large.save(os.path.join(MODEL_DIR, \"mlp_large_tuned.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab703a",
   "metadata": {},
   "source": [
    "# Neural Network Training Workflow\n",
    "- Robust preprocessing based on EDA and scaling analysis\n",
    "- Multiple NN architectures: Large MLP (Hyperband), Residual MLP, Advanced MLP (Bayesian Optimization)\n",
    "- Hyperparameter tuning for each model\n",
    "- Evaluation using RMSE, MAE, Loss; metrics plotted and saved\n",
    "- Each trained model is saved for future inference\n",
    "- Ensemble model combines predictions for improved accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b28edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered shapes: X_train_fe (40000, 230), X_test_fe (10000, 230)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Add polynomial and interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_fe = poly.fit_transform(X_train)\n",
    "X_test_fe = poly.transform(X_test)\n",
    "\n",
    "print(f\"Feature engineered shapes: X_train_fe {X_train_fe.shape}, X_test_fe {X_test_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6164bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_bayes_tuning\\tuner0.json\n",
      "Epoch 1/40\n",
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7113 - mae: 5.5656 - val_loss: 4.6096 - val_mae: 1.4736\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7113 - mae: 5.5656 - val_loss: 4.6096 - val_mae: 1.4736\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 2.7609 - mae: 1.0896 - val_loss: 2.0525 - val_mae: 0.9176\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 2.7609 - mae: 1.0896 - val_loss: 2.0525 - val_mae: 0.9176\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.7429 - mae: 0.8246 - val_loss: 1.8845 - val_mae: 0.8732\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.7429 - mae: 0.8246 - val_loss: 1.8845 - val_mae: 0.8732\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 1.5057 - mae: 0.7407 - val_loss: 1.3025 - val_mae: 0.6708\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 1.5057 - mae: 0.7407 - val_loss: 1.3025 - val_mae: 0.6708\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.3841 - mae: 0.6933 - val_loss: 1.4468 - val_mae: 0.6830\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.3841 - mae: 0.6933 - val_loss: 1.4468 - val_mae: 0.6830\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.3258 - mae: 0.6680 - val_loss: 1.3531 - val_mae: 0.6430\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.3258 - mae: 0.6680 - val_loss: 1.3531 - val_mae: 0.6430\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 1.2919 - mae: 0.6600 - val_loss: 1.3166 - val_mae: 0.6600\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 1.2919 - mae: 0.6600 - val_loss: 1.3166 - val_mae: 0.6600\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.2960 - mae: 0.6561 - val_loss: 1.4172 - val_mae: 0.6903\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.2960 - mae: 0.6561 - val_loss: 1.4172 - val_mae: 0.6903\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 1.2239 - mae: 0.6374 - val_loss: 1.2312 - val_mae: 0.6211\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 1.2239 - mae: 0.6374 - val_loss: 1.2312 - val_mae: 0.6211\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1.2174 - mae: 0.6312 - val_loss: 1.3238 - val_mae: 0.6273\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1.2174 - mae: 0.6312 - val_loss: 1.3238 - val_mae: 0.6273\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.2002 - mae: 0.6267 - val_loss: 1.3012 - val_mae: 0.6239\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.2002 - mae: 0.6267 - val_loss: 1.3012 - val_mae: 0.6239\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1900 - mae: 0.6184 - val_loss: 1.5149 - val_mae: 0.7216\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1900 - mae: 0.6184 - val_loss: 1.5149 - val_mae: 0.7216\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 1.1514 - mae: 0.6105 - val_loss: 1.1925 - val_mae: 0.6194\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 1.1514 - mae: 0.6105 - val_loss: 1.1925 - val_mae: 0.6194\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.1522 - mae: 0.6134 - val_loss: 1.2523 - val_mae: 0.6655\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.1522 - mae: 0.6134 - val_loss: 1.2523 - val_mae: 0.6655\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 1.1583 - mae: 0.6090 - val_loss: 1.2846 - val_mae: 0.6168\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 1.1583 - mae: 0.6090 - val_loss: 1.2846 - val_mae: 0.6168\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1.0963 - mae: 0.5826 - val_loss: 1.1075 - val_mae: 0.5740\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1.0963 - mae: 0.5826 - val_loss: 1.1075 - val_mae: 0.5740\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0893 - mae: 0.5884 - val_loss: 1.1828 - val_mae: 0.6186\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0893 - mae: 0.5884 - val_loss: 1.1828 - val_mae: 0.6186\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 1.8539 - mae: 0.6946 - val_loss: 1.1109 - val_mae: 0.5955\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 1.8539 - mae: 0.6946 - val_loss: 1.1109 - val_mae: 0.5955\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1.0405 - mae: 0.5640 - val_loss: 1.1219 - val_mae: 0.5895\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1.0405 - mae: 0.5640 - val_loss: 1.1219 - val_mae: 0.5895\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.0148 - mae: 0.5547 - val_loss: 1.1095 - val_mae: 0.5555\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.0148 - mae: 0.5547 - val_loss: 1.1095 - val_mae: 0.5555\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0125 - mae: 0.5539 - val_loss: 1.2913 - val_mae: 0.6607\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0125 - mae: 0.5539 - val_loss: 1.2913 - val_mae: 0.6607\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1.0081 - mae: 0.5576 - val_loss: 1.2125 - val_mae: 0.6430\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1.0081 - mae: 0.5576 - val_loss: 1.2125 - val_mae: 0.6430\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 1.0064 - mae: 0.5519 - val_loss: 1.0716 - val_mae: 0.5506\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 1.0064 - mae: 0.5519 - val_loss: 1.0716 - val_mae: 0.5506\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - loss: 0.9944 - mae: 0.5483 - val_loss: 1.1268 - val_mae: 0.5905\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - loss: 0.9944 - mae: 0.5483 - val_loss: 1.1268 - val_mae: 0.5905\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0086 - mae: 0.5529 - val_loss: 1.0678 - val_mae: 0.5534\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0086 - mae: 0.5529 - val_loss: 1.0678 - val_mae: 0.5534\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 1.0654 - mae: 0.5684 - val_loss: 1.9083 - val_mae: 0.7470\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 1.0654 - mae: 0.5684 - val_loss: 1.9083 - val_mae: 0.7470\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 1.5259 - mae: 0.6203 - val_loss: 1.0960 - val_mae: 0.5991\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 1.5259 - mae: 0.6203 - val_loss: 1.0960 - val_mae: 0.5991\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.9794 - mae: 0.5447 - val_loss: 1.0510 - val_mae: 0.5621\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.9794 - mae: 0.5447 - val_loss: 1.0510 - val_mae: 0.5621\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.9586 - mae: 0.5322 - val_loss: 1.0317 - val_mae: 0.5471\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.9586 - mae: 0.5322 - val_loss: 1.0317 - val_mae: 0.5471\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.9502 - mae: 0.5340 - val_loss: 1.0522 - val_mae: 0.5849\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.9502 - mae: 0.5340 - val_loss: 1.0522 - val_mae: 0.5849\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9493 - mae: 0.5331 - val_loss: 1.0891 - val_mae: 0.5491\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9493 - mae: 0.5331 - val_loss: 1.0891 - val_mae: 0.5491\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.9479 - mae: 0.5323 - val_loss: 1.0594 - val_mae: 0.5417\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.9479 - mae: 0.5323 - val_loss: 1.0594 - val_mae: 0.5417\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9772 - mae: 0.5396 - val_loss: 1.0607 - val_mae: 0.5593\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9772 - mae: 0.5396 - val_loss: 1.0607 - val_mae: 0.5593\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9649 - mae: 0.5415 - val_loss: 1.0842 - val_mae: 0.5725\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9649 - mae: 0.5415 - val_loss: 1.0842 - val_mae: 0.5725\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9530 - mae: 0.5340 - val_loss: 1.0213 - val_mae: 0.5420\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9530 - mae: 0.5340 - val_loss: 1.0213 - val_mae: 0.5420\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9359 - mae: 0.5340 - val_loss: 1.1100 - val_mae: 0.5836\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9359 - mae: 0.5340 - val_loss: 1.1100 - val_mae: 0.5836\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1317 - mae: 0.5720 - val_loss: 1.1138 - val_mae: 0.5977\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1317 - mae: 0.5720 - val_loss: 1.1138 - val_mae: 0.5977\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9165 - mae: 0.5250 - val_loss: 1.0651 - val_mae: 0.5313\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9165 - mae: 0.5250 - val_loss: 1.0651 - val_mae: 0.5313\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9006 - mae: 0.5194 - val_loss: 1.1029 - val_mae: 0.5454\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9006 - mae: 0.5194 - val_loss: 1.1029 - val_mae: 0.5454\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8999 - mae: 0.5155 - val_loss: 1.8107 - val_mae: 0.6621\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8999 - mae: 0.5155 - val_loss: 1.8107 - val_mae: 0.6621\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "Advanced MLP (Bayesian) Evaluation: MAE=0.6621, RMSE=1.3456\n",
      "Advanced MLP (Bayesian) Evaluation: MAE=0.6621, RMSE=1.3456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_bayes_tuning\\tuner0.json\n",
      "Epoch 1/40\n",
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7113 - mae: 5.5656 - val_loss: 4.6096 - val_mae: 1.4736\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7113 - mae: 5.5656 - val_loss: 4.6096 - val_mae: 1.4736\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 2.7609 - mae: 1.0896 - val_loss: 2.0525 - val_mae: 0.9176\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 2.7609 - mae: 1.0896 - val_loss: 2.0525 - val_mae: 0.9176\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.7429 - mae: 0.8246 - val_loss: 1.8845 - val_mae: 0.8732\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.7429 - mae: 0.8246 - val_loss: 1.8845 - val_mae: 0.8732\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 1.5057 - mae: 0.7407 - val_loss: 1.3025 - val_mae: 0.6708\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 971us/step - loss: 1.5057 - mae: 0.7407 - val_loss: 1.3025 - val_mae: 0.6708\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.3841 - mae: 0.6933 - val_loss: 1.4468 - val_mae: 0.6830\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 1.3841 - mae: 0.6933 - val_loss: 1.4468 - val_mae: 0.6830\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.3258 - mae: 0.6680 - val_loss: 1.3531 - val_mae: 0.6430\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.3258 - mae: 0.6680 - val_loss: 1.3531 - val_mae: 0.6430\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 1.2919 - mae: 0.6600 - val_loss: 1.3166 - val_mae: 0.6600\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959us/step - loss: 1.2919 - mae: 0.6600 - val_loss: 1.3166 - val_mae: 0.6600\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.2960 - mae: 0.6561 - val_loss: 1.4172 - val_mae: 0.6903\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 966us/step - loss: 1.2960 - mae: 0.6561 - val_loss: 1.4172 - val_mae: 0.6903\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 1.2239 - mae: 0.6374 - val_loss: 1.2312 - val_mae: 0.6211\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 1.2239 - mae: 0.6374 - val_loss: 1.2312 - val_mae: 0.6211\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1.2174 - mae: 0.6312 - val_loss: 1.3238 - val_mae: 0.6273\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 985us/step - loss: 1.2174 - mae: 0.6312 - val_loss: 1.3238 - val_mae: 0.6273\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.2002 - mae: 0.6267 - val_loss: 1.3012 - val_mae: 0.6239\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 981us/step - loss: 1.2002 - mae: 0.6267 - val_loss: 1.3012 - val_mae: 0.6239\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1900 - mae: 0.6184 - val_loss: 1.5149 - val_mae: 0.7216\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1900 - mae: 0.6184 - val_loss: 1.5149 - val_mae: 0.7216\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 1.1514 - mae: 0.6105 - val_loss: 1.1925 - val_mae: 0.6194\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 1.1514 - mae: 0.6105 - val_loss: 1.1925 - val_mae: 0.6194\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.1522 - mae: 0.6134 - val_loss: 1.2523 - val_mae: 0.6655\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980us/step - loss: 1.1522 - mae: 0.6134 - val_loss: 1.2523 - val_mae: 0.6655\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 1.1583 - mae: 0.6090 - val_loss: 1.2846 - val_mae: 0.6168\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 1.1583 - mae: 0.6090 - val_loss: 1.2846 - val_mae: 0.6168\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1.0963 - mae: 0.5826 - val_loss: 1.1075 - val_mae: 0.5740\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 978us/step - loss: 1.0963 - mae: 0.5826 - val_loss: 1.1075 - val_mae: 0.5740\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0893 - mae: 0.5884 - val_loss: 1.1828 - val_mae: 0.6186\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0893 - mae: 0.5884 - val_loss: 1.1828 - val_mae: 0.6186\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 1.8539 - mae: 0.6946 - val_loss: 1.1109 - val_mae: 0.5955\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 1.8539 - mae: 0.6946 - val_loss: 1.1109 - val_mae: 0.5955\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1.0405 - mae: 0.5640 - val_loss: 1.1219 - val_mae: 0.5895\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996us/step - loss: 1.0405 - mae: 0.5640 - val_loss: 1.1219 - val_mae: 0.5895\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.0148 - mae: 0.5547 - val_loss: 1.1095 - val_mae: 0.5555\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.0148 - mae: 0.5547 - val_loss: 1.1095 - val_mae: 0.5555\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0125 - mae: 0.5539 - val_loss: 1.2913 - val_mae: 0.6607\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 969us/step - loss: 1.0125 - mae: 0.5539 - val_loss: 1.2913 - val_mae: 0.6607\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1.0081 - mae: 0.5576 - val_loss: 1.2125 - val_mae: 0.6430\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 1.0081 - mae: 0.5576 - val_loss: 1.2125 - val_mae: 0.6430\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 1.0064 - mae: 0.5519 - val_loss: 1.0716 - val_mae: 0.5506\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 1.0064 - mae: 0.5519 - val_loss: 1.0716 - val_mae: 0.5506\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - loss: 0.9944 - mae: 0.5483 - val_loss: 1.1268 - val_mae: 0.5905\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - loss: 0.9944 - mae: 0.5483 - val_loss: 1.1268 - val_mae: 0.5905\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0086 - mae: 0.5529 - val_loss: 1.0678 - val_mae: 0.5534\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 979us/step - loss: 1.0086 - mae: 0.5529 - val_loss: 1.0678 - val_mae: 0.5534\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 1.0654 - mae: 0.5684 - val_loss: 1.9083 - val_mae: 0.7470\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 973us/step - loss: 1.0654 - mae: 0.5684 - val_loss: 1.9083 - val_mae: 0.7470\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 1.5259 - mae: 0.6203 - val_loss: 1.0960 - val_mae: 0.5991\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983us/step - loss: 1.5259 - mae: 0.6203 - val_loss: 1.0960 - val_mae: 0.5991\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.9794 - mae: 0.5447 - val_loss: 1.0510 - val_mae: 0.5621\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 0.9794 - mae: 0.5447 - val_loss: 1.0510 - val_mae: 0.5621\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.9586 - mae: 0.5322 - val_loss: 1.0317 - val_mae: 0.5471\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 972us/step - loss: 0.9586 - mae: 0.5322 - val_loss: 1.0317 - val_mae: 0.5471\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.9502 - mae: 0.5340 - val_loss: 1.0522 - val_mae: 0.5849\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 975us/step - loss: 0.9502 - mae: 0.5340 - val_loss: 1.0522 - val_mae: 0.5849\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9493 - mae: 0.5331 - val_loss: 1.0891 - val_mae: 0.5491\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9493 - mae: 0.5331 - val_loss: 1.0891 - val_mae: 0.5491\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.9479 - mae: 0.5323 - val_loss: 1.0594 - val_mae: 0.5417\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.9479 - mae: 0.5323 - val_loss: 1.0594 - val_mae: 0.5417\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9772 - mae: 0.5396 - val_loss: 1.0607 - val_mae: 0.5593\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970us/step - loss: 0.9772 - mae: 0.5396 - val_loss: 1.0607 - val_mae: 0.5593\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9649 - mae: 0.5415 - val_loss: 1.0842 - val_mae: 0.5725\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9649 - mae: 0.5415 - val_loss: 1.0842 - val_mae: 0.5725\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9530 - mae: 0.5340 - val_loss: 1.0213 - val_mae: 0.5420\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9530 - mae: 0.5340 - val_loss: 1.0213 - val_mae: 0.5420\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9359 - mae: 0.5340 - val_loss: 1.1100 - val_mae: 0.5836\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9359 - mae: 0.5340 - val_loss: 1.1100 - val_mae: 0.5836\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1317 - mae: 0.5720 - val_loss: 1.1138 - val_mae: 0.5977\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1317 - mae: 0.5720 - val_loss: 1.1138 - val_mae: 0.5977\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9165 - mae: 0.5250 - val_loss: 1.0651 - val_mae: 0.5313\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9165 - mae: 0.5250 - val_loss: 1.0651 - val_mae: 0.5313\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9006 - mae: 0.5194 - val_loss: 1.1029 - val_mae: 0.5454\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9006 - mae: 0.5194 - val_loss: 1.1029 - val_mae: 0.5454\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8999 - mae: 0.5155 - val_loss: 1.8107 - val_mae: 0.6621\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8999 - mae: 0.5155 - val_loss: 1.8107 - val_mae: 0.6621\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step\n",
      "Advanced MLP (Bayesian) Evaluation: MAE=0.6621, RMSE=1.3456\n",
      "Advanced MLP (Bayesian) Evaluation: MAE=0.6621, RMSE=1.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Step 6: Advanced MLP (Bayesian Optimization)\n",
    "# This model uses Bayesian Optimization to search for optimal architecture and training parameters, aiming for best-in-class tabular regression.\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_advanced_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train.shape[1]\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    num_layers = hp.Int(\"num_layers\", 2, 6)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", 32, 512, step=32)\n",
    "        activation = hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\", \"rmsprop\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "bayes_tuner = BayesianOptimization(\n",
    "    build_advanced_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=20,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_bayes_tuning\"\n",
    "    )\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)\n",
    "bayes_tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=64,\n",
    "    epochs=40\n",
    "    )\n",
    "best_bayes_hps = bayes_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "advanced_model = bayes_tuner.hypermodel.build(best_bayes_hps)\n",
    "history_adv = advanced_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    "    )\n",
    "y_pred_adv = advanced_model.predict(X_test).flatten()\n",
    "mae_adv = mean_absolute_error(y_test, y_pred_adv)\n",
    "rmse_adv = np.sqrt(mean_squared_error(y_test, y_pred_adv))\n",
    "print(f\"Advanced MLP (Bayesian) Evaluation: MAE={mae_adv:.4f}, RMSE={rmse_adv:.4f}\")\n",
    "plot_training_metrics(history_adv, \"Advanced MLP (Bayesian)\", \"training_large_advanced_tuned.png\")\n",
    "advanced_model.save(os.path.join(MODEL_DIR, \"mlp_large_advanced_tuned.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e440d5",
   "metadata": {},
   "source": [
    "## Advanced Model Improvements\n",
    "Let's try more advanced deep learning techniques: residual connections, dropout, batch normalization, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156d30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 76.0410 - mae: 5.8400 - val_loss: 15.4982 - val_mae: 2.8000 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.3556 - mae: 2.6920 - val_loss: 6.5562 - val_mae: 1.7460 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.1529 - mae: 2.3747 - val_loss: 3.5976 - val_mae: 1.1456 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.9112 - mae: 2.2530 - val_loss: 4.2431 - val_mae: 1.2711 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.1976 - mae: 2.1566 - val_loss: 2.3209 - val_mae: 0.7822 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.2547 - mae: 2.0485 - val_loss: 2.8765 - val_mae: 0.9514 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.5234 - mae: 2.0852 - val_loss: 2.6161 - val_mae: 0.8879 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0785 - mae: 2.0223 - val_loss: 3.1689 - val_mae: 1.0983 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.0124 - mae: 2.0241 - val_loss: 2.9302 - val_mae: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.4687 - mae: 1.9473 - val_loss: 4.6637 - val_mae: 1.2961 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6812 - mae: 1.8401 - val_loss: 2.1244 - val_mae: 0.7372 - learning_rate: 5.0000e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6457 - mae: 1.8490 - val_loss: 2.4032 - val_mae: 0.7890 - learning_rate: 5.0000e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.6983 - mae: 1.8416 - val_loss: 2.3702 - val_mae: 0.8825 - learning_rate: 5.0000e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4917 - mae: 1.8204 - val_loss: 2.0213 - val_mae: 0.7152 - learning_rate: 5.0000e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.4076 - mae: 1.8264 - val_loss: 1.8929 - val_mae: 0.6281 - learning_rate: 5.0000e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.1037 - mae: 1.7741 - val_loss: 1.9180 - val_mae: 0.6571 - learning_rate: 5.0000e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.9061 - mae: 1.7433 - val_loss: 1.9933 - val_mae: 0.7076 - learning_rate: 5.0000e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7820 - mae: 1.7277 - val_loss: 1.9111 - val_mae: 0.6636 - learning_rate: 5.0000e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.8899 - mae: 1.7580 - val_loss: 1.8588 - val_mae: 0.7153 - learning_rate: 5.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7126 - mae: 1.7252 - val_loss: 2.0306 - val_mae: 0.7095 - learning_rate: 5.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6018 - mae: 1.7220 - val_loss: 1.8844 - val_mae: 0.6511 - learning_rate: 5.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.3602 - mae: 1.6651 - val_loss: 1.9894 - val_mae: 0.7576 - learning_rate: 5.0000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4451 - mae: 1.6982 - val_loss: 1.8970 - val_mae: 0.7141 - learning_rate: 5.0000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4748 - mae: 1.6861 - val_loss: 2.2690 - val_mae: 0.8148 - learning_rate: 5.0000e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2610 - mae: 1.6616 - val_loss: 1.7045 - val_mae: 0.6761 - learning_rate: 2.5000e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1698 - mae: 1.6545 - val_loss: 1.6785 - val_mae: 0.6175 - learning_rate: 2.5000e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0715 - mae: 1.6243 - val_loss: 1.6464 - val_mae: 0.6135 - learning_rate: 2.5000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9076 - mae: 1.6107 - val_loss: 1.6226 - val_mae: 0.6074 - learning_rate: 2.5000e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.0424 - mae: 1.6409 - val_loss: 1.6673 - val_mae: 0.6558 - learning_rate: 2.5000e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.1187 - mae: 1.6560 - val_loss: 1.6797 - val_mae: 0.6348 - learning_rate: 2.5000e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.8883 - mae: 1.6195 - val_loss: 1.6634 - val_mae: 0.6441 - learning_rate: 2.5000e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7703 - mae: 1.5888 - val_loss: 1.6048 - val_mae: 0.6167 - learning_rate: 2.5000e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9040 - mae: 1.6279 - val_loss: 1.5606 - val_mae: 0.5638 - learning_rate: 2.5000e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6378 - mae: 1.5730 - val_loss: 1.6609 - val_mae: 0.6587 - learning_rate: 2.5000e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7234 - mae: 1.5813 - val_loss: 1.5816 - val_mae: 0.6374 - learning_rate: 2.5000e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9924 - mae: 1.6441 - val_loss: 1.6396 - val_mae: 0.6723 - learning_rate: 2.5000e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4446 - mae: 1.5356 - val_loss: 1.7036 - val_mae: 0.6424 - learning_rate: 2.5000e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.6433 - mae: 1.5848 - val_loss: 1.6063 - val_mae: 0.6513 - learning_rate: 2.5000e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.4183 - mae: 1.5392 - val_loss: 1.5875 - val_mae: 0.6371 - learning_rate: 1.2500e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.3858 - mae: 1.5122 - val_loss: 1.5252 - val_mae: 0.6003 - learning_rate: 1.2500e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "Residual MLP Evaluation: MAE=0.6003, RMSE=1.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Step 5: Residual MLP Model\n",
    "# This model introduces residual connections, dropout, and batch normalization for improved generalization and deeper learning.\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_residual_mlp(input_dim, n_layers=4, units=128, dropout_rate=0.2, l2_reg=1e-4):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    for i in range(n_layers):\n",
    "        shortcut = x\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([x, shortcut])  # Residual connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "res_mlp = build_residual_mlp(X_train.shape[1], n_layers=3, units=256, dropout_rate=0.3, l2_reg=1e-3)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "history_res = res_mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    "    )\n",
    "res_pred = res_mlp.predict(X_test).flatten()\n",
    "mae_res = mean_absolute_error(y_test, res_pred)\n",
    "rmse_res = np.sqrt(mean_squared_error(y_test, res_pred))\n",
    "print(f\"Residual MLP Evaluation: MAE={mae_res:.4f}, RMSE={rmse_res:.4f}\")\n",
    "plot_training_metrics(history_res, \"Residual MLP\", \"training_residual_mlp.png\")\n",
    "res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_residual_tuned.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86780e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Evaluation: MAE=0.5744, RMSE=1.0644\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Ensemble Model\n",
    "# The ensemble averages predictions from all trained models to further improve accuracy and robustness.\n",
    "ensemble_preds = (y_pred_large + res_pred + y_pred_adv) / 3\n",
    "mae_ensemble = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_test, ensemble_preds))\n",
    "print(f\"Ensemble Model Evaluation: MAE={mae_ensemble:.4f}, RMSE={rmse_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944f8f2",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "- Data was robustly preprocessed based on EDA.\n",
    "- Three advanced neural network models were trained and tuned: Large MLP (Hyperband), Residual MLP, and Advanced MLP (Bayesian Optimization).\n",
    "- Each model was evaluated using RMSE and MAE, with training curves plotted and saved.\n",
    "- An ensemble model was created for improved accuracy.\n",
    "- All models are saved for future inference.\n",
    "\n",
    "**Next steps:**\n",
    "- Explore deeper architectures, regularization, and uncertainty estimation (MC Dropout, quantile regression).\n",
    "- Try feature engineering, PCA, and stacking ensembles for further gains.\n",
    "- Analyze model errors and refine training data or architecture as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b02e8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 86.7469 - mae: 6.1939 - mse: 82.4059 - val_loss: 31.0816 - val_mae: 3.5045 - val_mse: 26.7075 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 20.5677 - mae: 2.7786 - mse: 16.2067 - val_loss: 10.9460 - val_mae: 1.6049 - val_mse: 6.6035 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 15.2482 - mae: 2.3033 - mse: 10.9304 - val_loss: 7.3173 - val_mae: 1.0927 - val_mse: 3.0277 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 14.4865 - mae: 2.1818 - mse: 10.2171 - val_loss: 7.4064 - val_mae: 1.2364 - val_mse: 3.1495 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 11.5014 - mae: 1.9784 - mse: 7.2981 - val_loss: 6.0669 - val_mae: 0.8673 - val_mse: 1.9241 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.8983 - mae: 1.9192 - mse: 6.8215 - val_loss: 6.5967 - val_mae: 1.0689 - val_mse: 2.5895 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 11.2866 - mae: 1.9320 - mse: 7.3278 - val_loss: 5.9009 - val_mae: 0.8970 - val_mse: 1.9832 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.9361 - mae: 1.8252 - mse: 6.0965 - val_loss: 5.6704 - val_mae: 0.8392 - val_mse: 1.9195 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.3052 - mae: 1.8491 - mse: 6.5966 - val_loss: 6.8672 - val_mae: 1.2230 - val_mse: 3.1731 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.1340 - mae: 1.7529 - mse: 5.5527 - val_loss: 5.0828 - val_mae: 0.7893 - val_mse: 1.6159 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.9066 - mae: 1.7312 - mse: 5.5710 - val_loss: 5.9129 - val_mae: 1.0798 - val_mse: 2.6343 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.5133 - mae: 1.7404 - mse: 5.3288 - val_loss: 5.1670 - val_mae: 0.9319 - val_mse: 2.1248 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.0381 - mae: 1.7186 - mse: 5.1459 - val_loss: 4.4496 - val_mae: 0.9267 - val_mse: 1.6841 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.3717 - mae: 1.6372 - mse: 4.7046 - val_loss: 3.8824 - val_mae: 0.7140 - val_mse: 1.3433 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 8.3822 - mae: 1.7210 - mse: 5.6912 - val_loss: 4.1948 - val_mae: 0.7864 - val_mse: 1.4935 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.1872 - mae: 1.6364 - mse: 4.6486 - val_loss: 3.7912 - val_mae: 0.7735 - val_mse: 1.3903 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.7397 - mae: 1.6080 - mse: 4.4656 - val_loss: 3.7319 - val_mae: 0.8531 - val_mse: 1.5769 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 6.7210 - mae: 1.6430 - mse: 4.6511 - val_loss: 3.8673 - val_mae: 0.9741 - val_mse: 1.8024 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7.2419 - mae: 1.6672 - mse: 5.1053 - val_loss: 4.2436 - val_mae: 0.8825 - val_mse: 1.9522 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 7.0209 - mae: 1.6333 - mse: 4.7907 - val_loss: 4.0399 - val_mae: 1.0456 - val_mse: 1.9008 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.2246 - mae: 1.5571 - mse: 4.2008 - val_loss: 3.5547 - val_mae: 0.8358 - val_mse: 1.6278 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.0828 - mae: 1.5777 - mse: 4.2588 - val_loss: 3.4489 - val_mae: 0.8679 - val_mse: 1.7025 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.4838 - mae: 1.5547 - mse: 4.5372 - val_loss: 5.0917 - val_mae: 1.0699 - val_mse: 2.8673 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 6.3721 - mae: 1.5569 - mse: 4.2456 - val_loss: 3.6064 - val_mae: 0.8645 - val_mse: 1.5092 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.7901 - mae: 1.4830 - mse: 3.8252 - val_loss: 3.2865 - val_mae: 0.7619 - val_mse: 1.3843 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5.8158 - mae: 1.5138 - mse: 3.9757 - val_loss: 3.2623 - val_mae: 0.8346 - val_mse: 1.4794 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 5.4719 - mae: 1.4884 - mse: 3.7908 - val_loss: 3.2863 - val_mae: 0.9068 - val_mse: 1.6783 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.6554 - mae: 1.4708 - mse: 3.8987 - val_loss: 3.2670 - val_mae: 0.8055 - val_mse: 1.4769 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.3027 - mae: 1.4395 - mse: 3.6301 - val_loss: 2.8553 - val_mae: 0.6999 - val_mse: 1.2717 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.1130 - mae: 1.4217 - mse: 3.5456 - val_loss: 3.2178 - val_mae: 0.8805 - val_mse: 1.6994 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.7870 - mae: 1.4687 - mse: 4.0155 - val_loss: 3.3199 - val_mae: 0.7850 - val_mse: 1.5056 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.4136 - mae: 1.4080 - mse: 3.6189 - val_loss: 3.2363 - val_mae: 0.8282 - val_mse: 1.4788 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 5.0112 - mae: 1.3881 - mse: 3.3558 - val_loss: 3.2429 - val_mae: 0.8790 - val_mse: 1.6766 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.7102 - mae: 1.3463 - mse: 3.2111 - val_loss: 3.2314 - val_mae: 0.8612 - val_mse: 1.7689 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.2126 - mae: 1.2680 - mse: 2.8191 - val_loss: 2.4951 - val_mae: 0.6700 - val_mse: 1.1562 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 4.1091 - mae: 1.2776 - mse: 2.8135 - val_loss: 2.5737 - val_mae: 0.7361 - val_mse: 1.3241 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.9325 - mae: 1.2368 - mse: 2.7123 - val_loss: 2.5191 - val_mae: 0.6855 - val_mse: 1.2985 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.7154 - mae: 1.1872 - mse: 2.5025 - val_loss: 2.4507 - val_mae: 0.7724 - val_mse: 1.2779 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.6754 - mae: 1.1991 - mse: 2.5388 - val_loss: 2.3612 - val_mae: 0.6951 - val_mse: 1.2478 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.6389 - mae: 1.1933 - mse: 2.5320 - val_loss: 2.4826 - val_mae: 0.7539 - val_mse: 1.3314 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.5767 - mae: 1.1787 - mse: 2.4667 - val_loss: 2.4184 - val_mae: 0.7669 - val_mse: 1.3154 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.5356 - mae: 1.1767 - mse: 2.4594 - val_loss: 2.2532 - val_mae: 0.6394 - val_mse: 1.2043 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.3997 - mae: 1.1505 - mse: 2.3571 - val_loss: 2.2366 - val_mae: 0.6728 - val_mse: 1.2141 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.3475 - mae: 1.1303 - mse: 2.3068 - val_loss: 2.2816 - val_mae: 0.7117 - val_mse: 1.2347 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.2755 - mae: 1.1127 - mse: 2.2560 - val_loss: 2.4862 - val_mae: 0.7614 - val_mse: 1.4730 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.4026 - mae: 1.1301 - mse: 2.3575 - val_loss: 2.4489 - val_mae: 0.8426 - val_mse: 1.4276 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.1571 - mae: 1.0893 - mse: 2.1631 - val_loss: 2.4025 - val_mae: 0.7533 - val_mse: 1.4302 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.1354 - mae: 1.1018 - mse: 2.1765 - val_loss: 2.2158 - val_mae: 0.7318 - val_mse: 1.2556 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.1405 - mae: 1.0889 - mse: 2.1706 - val_loss: 2.1701 - val_mae: 0.6601 - val_mse: 1.1927 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.9922 - mae: 1.0611 - mse: 2.0403 - val_loss: 2.2418 - val_mae: 0.6659 - val_mse: 1.3103 - learning_rate: 5.0000e-04\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deeper Residual MLP Evaluation:\n",
      "  MAE: 0.6659\n",
      "  RMSE: 1.1447\n",
      "  R²: 0.9935\n"
     ]
    }
   ],
   "source": [
    "# Deeper Residual Network with Multi-Level Skip Connections and More Regularization\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers import AdamW, Nadam\n",
    "\n",
    "def build_deep_residual_mlp(input_dim, n_layers=6, units=128, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-4, noise_std=0.05):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = GaussianNoise(noise_std)(inputs)\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "    skip = x\n",
    "    for i in range(n_layers):\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        if i % 2 == 1:\n",
    "            x = layers.Add()([x, skip])  # Multi-level skip connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=AdamW(), loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Train deeper residual model\n",
    "advanced_res_mlp = build_deep_residual_mlp(X_train_fe.shape[1], n_layers=6, units=256, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-3, noise_std=0.05)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history_adv_res = advanced_res_mlp.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "adv_res_pred = advanced_res_mlp.predict(X_test_fe).flatten()\n",
    "mae_adv_res = mean_absolute_error(y_test, adv_res_pred)\n",
    "rmse_adv_res = np.sqrt(mean_squared_error(y_test, adv_res_pred))\n",
    "r2_adv_res = r2_score(y_test, adv_res_pred)\n",
    "\n",
    "print(f\"\\nDeeper Residual MLP Evaluation:\\n  MAE: {mae_adv_res:.4f}\\n  RMSE: {rmse_adv_res:.4f}\\n  R²: {r2_adv_res:.4f}\")\n",
    "advanced_res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_deep_residual_tuned.h5\"))\n",
    "plot_training_metrics(history_adv_res, \"Deep Residual MLP\", \"training_deep_residual_mlp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886424dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Learning Rate Callback\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=1e-4, max_lr=1e-2, step_size=2000):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.iterations = 0\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        cycle = np.floor(1 + self.iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.iterations / self.step_size - 2 * cycle + 1)\n",
    "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        self.iterations += 1\n",
    "\n",
    "# Example usage:\n",
    "# clr = CyclicLR(base_lr=1e-4, max_lr=1e-2, step_size=1000)\n",
    "# history = model.fit(..., callbacks=[clr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1fbc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reduced shape: X_train_pca (40000, 101), X_test_pca (10000, 101)\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection: Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_fe)\n",
    "X_test_pca = pca.transform(X_test_fe)\n",
    "\n",
    "print(f\"PCA reduced shape: X_train_pca {X_train_pca.shape}, X_test_pca {X_test_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "055af79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked Ensemble Evaluation:\n",
      "  MAE: 0.5756\n",
      "  RMSE: 1.0077\n",
      "  R²: 0.9949\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Stacking: Combine Multiple Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume you have predictions from several models\n",
    "# y_pred_adv, res_pred, adv_res_pred (from previous cells)\n",
    "\n",
    "stacked_preds = np.vstack([\n",
    "    y_pred_adv,\n",
    "    res_pred,\n",
    "    adv_res_pred\n",
    "]).T\n",
    "\n",
    "stacker = LinearRegression()\n",
    "stacker.fit(stacked_preds, y_test)\n",
    "ensemble_stacked = stacker.predict(stacked_preds)\n",
    "\n",
    "mae_stacked = mean_absolute_error(y_test, ensemble_stacked)\n",
    "rmse_stacked = np.sqrt(mean_squared_error(y_test, ensemble_stacked))\n",
    "r2_stacked = r2_score(y_test, ensemble_stacked)\n",
    "\n",
    "print(f\"\\nStacked Ensemble Evaluation:\\n  MAE: {mae_stacked:.4f}\\n  RMSE: {rmse_stacked:.4f}\\n  R²: {r2_stacked:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ebe60",
   "metadata": {},
   "source": [
    "### Advanced Deep Learning Improvements\n",
    "We'll now apply:\n",
    "- MC Dropout for uncertainty estimation\n",
    "- Quantile regression for predictive intervals\n",
    "- Mixup data augmentation for tabular data\n",
    "- Learning rate warmup/scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bc196ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Dropout mean prediction: [ 0.32455605 29.407522    1.2473353   0.40297022  0.15261444]\n",
      "MC Dropout std (uncertainty): [0.5068752  0.7433825  0.67587054 0.34708363 0.38833657]\n"
     ]
    }
   ],
   "source": [
    "# MC Dropout for Uncertainty Estimation\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def mc_dropout_predict(model, X, n_iter=100):\n",
    "    # Enable dropout at inference\n",
    "    f = Model(model.input, model.output)\n",
    "    preds = [f(X, training=True).numpy().flatten() for _ in range(n_iter)]\n",
    "    preds = np.array(preds)\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    std_pred = preds.std(axis=0)\n",
    "    return mean_pred, std_pred\n",
    "\n",
    "# Example usage:\n",
    "mean_pred, std_pred = mc_dropout_predict(advanced_res_mlp, X_test_fe, n_iter=100)\n",
    "print(f\"MC Dropout mean prediction: {mean_pred[:5]}\")\n",
    "print(f\"MC Dropout std (uncertainty): {std_pred[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6351b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step\n",
      "Quantile interval example: low=[-3.7380363e-05  1.7551762e+01  9.4530261e-01 -3.7380363e-05\n",
      " -3.7380363e-05], median=[2.2215044e-04 2.8520277e+01 8.9656967e-01 2.2215044e-04 2.2215044e-04], high=[ 0.10687125 32.447674    0.9467553   0.0888381  -0.13649786]\n"
     ]
    }
   ],
   "source": [
    "# Quantile Regression for Predictive Intervals\n",
    "# Use only tensorflow.keras for compatibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "\n",
    "# Quantile loss function for Keras\n",
    "# K.maximum and K.mean are from tensorflow.keras.backend\n",
    "\n",
    "def quantile_loss(q):\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * e, (q - 1) * e), axis=-1)\n",
    "    return loss\n",
    "\n",
    "# Build quantile model (e.g., 0.1, 0.5, 0.9)\n",
    "def build_quantile_mlp(input_dim, quantile):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss=quantile_loss(quantile))\n",
    "    return model\n",
    "\n",
    "# Train quantile models\n",
    "q_low, q_med, q_high = 0.1, 0.5, 0.9\n",
    "model_low = build_quantile_mlp(X_train_fe.shape[1], q_low)\n",
    "model_med = build_quantile_mlp(X_train_fe.shape[1], q_med)\n",
    "model_high = build_quantile_mlp(X_train_fe.shape[1], q_high)\n",
    "\n",
    "model_low.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_med.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_high.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "\n",
    "# Predict intervals\n",
    "pred_low = model_low.predict(X_test_fe).flatten()\n",
    "pred_med = model_med.predict(X_test_fe).flatten()\n",
    "pred_high = model_high.predict(X_test_fe).flatten()\n",
    "\n",
    "print(f\"Quantile interval example: low={pred_low[:5]}, median={pred_med[:5]}, high={pred_high[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0fe898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixup sample shapes: (40000, 230), (40000,)\n",
      "Epoch 1/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 89.8291 - mae: 6.6961 - mse: 87.1499 - val_loss: 32.4558 - val_mae: 3.7826 - val_mse: 29.7592\n",
      "Epoch 2/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 33.2316 - mae: 3.7542 - mse: 30.5304 - val_loss: 9.1704 - val_mae: 1.7785 - val_mse: 6.4664\n",
      "Epoch 3/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 26.0108 - mae: 3.2429 - mse: 23.3053 - val_loss: 9.5135 - val_mae: 1.7839 - val_mse: 6.8049\n",
      "Epoch 4/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 22.3727 - mae: 2.9739 - mse: 19.6619 - val_loss: 6.2404 - val_mae: 1.2138 - val_mse: 3.5274\n",
      "Epoch 5/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 20.3124 - mae: 2.8177 - mse: 17.5965 - val_loss: 5.9862 - val_mae: 1.2007 - val_mse: 3.2690\n",
      "Epoch 6/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 19.0015 - mae: 2.7285 - mse: 16.2797 - val_loss: 5.9490 - val_mae: 1.2214 - val_mse: 3.2235\n",
      "Epoch 7/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 17.6208 - mae: 2.5934 - mse: 14.8906 - val_loss: 8.2416 - val_mae: 1.4575 - val_mse: 5.5073\n",
      "Epoch 8/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 17.5826 - mae: 2.5797 - mse: 14.8381 - val_loss: 5.7809 - val_mae: 1.1379 - val_mse: 3.0308\n",
      "Epoch 9/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 15.9260 - mae: 2.4482 - mse: 13.1706 - val_loss: 5.9577 - val_mae: 1.1778 - val_mse: 3.2003\n",
      "Epoch 10/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 15.6699 - mae: 2.4374 - mse: 12.9117 - val_loss: 5.7961 - val_mae: 1.2414 - val_mse: 3.0283\n",
      "Epoch 11/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 14.8784 - mae: 2.3599 - mse: 12.1064 - val_loss: 5.8749 - val_mae: 1.2761 - val_mse: 3.1026\n",
      "Epoch 12/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 14.2282 - mae: 2.2991 - mse: 11.4566 - val_loss: 5.4293 - val_mae: 1.0499 - val_mse: 2.6497\n",
      "Epoch 13/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 13.9409 - mae: 2.2862 - mse: 11.1679 - val_loss: 5.4507 - val_mae: 1.1035 - val_mse: 2.6711\n",
      "Epoch 14/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 13.1820 - mae: 2.2131 - mse: 10.4062 - val_loss: 5.7230 - val_mae: 1.1151 - val_mse: 2.9407\n",
      "Epoch 15/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 12.9462 - mae: 2.2093 - mse: 10.1649 - val_loss: 4.7220 - val_mae: 0.9166 - val_mse: 1.9376\n",
      "Epoch 16/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 12.5741 - mae: 2.1652 - mse: 9.7923 - val_loss: 7.1769 - val_mae: 1.2610 - val_mse: 4.3774\n",
      "Epoch 17/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 12.7448 - mae: 2.1704 - mse: 9.9402 - val_loss: 5.2059 - val_mae: 1.0314 - val_mse: 2.3935\n",
      "Epoch 18/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 12.0204 - mae: 2.1280 - mse: 9.2164 - val_loss: 5.3209 - val_mae: 0.9646 - val_mse: 2.5075\n",
      "Epoch 19/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 11.6242 - mae: 2.0872 - mse: 8.8097 - val_loss: 4.7256 - val_mae: 0.9586 - val_mse: 1.9233\n",
      "Epoch 20/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 11.1602 - mae: 2.0371 - mse: 8.3644 - val_loss: 4.7439 - val_mae: 0.9272 - val_mse: 1.9586\n",
      "Epoch 21/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.9618 - mae: 2.0236 - mse: 8.1786 - val_loss: 5.5052 - val_mae: 1.0935 - val_mse: 2.7106\n",
      "Epoch 22/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 11.1224 - mae: 2.0432 - mse: 8.3171 - val_loss: 5.7049 - val_mae: 1.0876 - val_mse: 2.8919\n",
      "Epoch 23/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.5270 - mae: 1.9834 - mse: 7.7235 - val_loss: 5.3063 - val_mae: 1.0458 - val_mse: 2.5009\n",
      "Epoch 24/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.7148 - mae: 1.9921 - mse: 7.9020 - val_loss: 5.0145 - val_mae: 1.0603 - val_mse: 2.1966\n",
      "Epoch 25/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.5694 - mae: 1.9815 - mse: 7.7439 - val_loss: 5.5656 - val_mae: 0.9909 - val_mse: 2.7284\n",
      "Epoch 26/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 10.2631 - mae: 1.9293 - mse: 7.4284 - val_loss: 4.6278 - val_mae: 0.9037 - val_mse: 1.8027\n",
      "Epoch 27/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.6075 - mae: 1.8921 - mse: 6.8101 - val_loss: 4.7828 - val_mae: 0.9211 - val_mse: 2.0018\n",
      "Epoch 28/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.9536 - mae: 1.9148 - mse: 7.1634 - val_loss: 4.5935 - val_mae: 0.8874 - val_mse: 1.7971\n",
      "Epoch 29/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.6067 - mae: 1.8824 - mse: 6.8190 - val_loss: 4.9520 - val_mae: 0.9930 - val_mse: 2.1695\n",
      "Epoch 30/30\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.2785 - mae: 1.8426 - mse: 6.5093 - val_loss: 4.6849 - val_mae: 0.9922 - val_mse: 1.9292\n"
     ]
    }
   ],
   "source": [
    "# Mixup Data Augmentation for Tabular Data\n",
    "import numpy as np\n",
    "\n",
    "def mixup(X, y, alpha=0.2):\n",
    "    '''Mixup augmentation for tabular data.'''\n",
    "    n_samples = X.shape[0]\n",
    "    lam = np.random.beta(alpha, alpha, n_samples)\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X_mix = lam[:, None] * X + (1 - lam)[:, None] * X[idx]\n",
    "    y_mix = lam * y + (1 - lam) * y[idx]\n",
    "    return X_mix, y_mix\n",
    "\n",
    "# Example usage:\n",
    "X_train_mix, y_train_mix = mixup(X_train_fe, y_train, alpha=0.2)\n",
    "print(f\"Mixup sample shapes: {X_train_mix.shape}, {y_train_mix.shape}\")\n",
    "# Train model on mixup data\n",
    "mixup_model = build_deep_residual_mlp(X_train_mix.shape[1], n_layers=6, units=256)\n",
    "history_mixup = mixup_model.fit(X_train_mix, y_train_mix, validation_data=(X_test_fe, y_test), epochs=30, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c5b7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step\n",
      "\n",
      "Mixup Model Evaluation:\n",
      "  MAE: 0.9922\n",
      "  RMSE: 1.3890\n",
      "  R²: 0.9904\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Mixup Model Performance\n",
    "y_pred_mixup = mixup_model.predict(X_test_fe).flatten()\n",
    "mae_mixup = mean_absolute_error(y_test, y_pred_mixup)\n",
    "rmse_mixup = np.sqrt(mean_squared_error(y_test, y_pred_mixup))\n",
    "r2_mixup = r2_score(y_test, y_pred_mixup)\n",
    "\n",
    "print(f\"\\nMixup Model Evaluation:\\n  MAE: {mae_mixup:.4f}\\n  RMSE: {rmse_mixup:.4f}\\n  R²: {r2_mixup:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neural-pricer venv)",
   "language": "python",
   "name": "neural-pricer-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
