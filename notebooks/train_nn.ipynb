{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f79724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (6.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
      "Requirement already satisfied: keras_tuner in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (3.11.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (2.32.5)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras_tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (2.3.2)\n",
      "Requirement already satisfied: rich in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (3.14.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from keras->keras_tuner) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from requests->keras_tuner) (2025.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from optree->keras->keras_tuner) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from rich->keras->keras_tuner) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\onedrive\\desktop\\github\\neural-pricer\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras tensorflow --upgrade\n",
    "!{sys.executable} -m pip install keras_tuner\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f994c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/raw/training_data.npz\"\n",
    "SCALER_DIR = \"../data/processed/scalers\"\n",
    "MODEL_DIR = \"../src/models\"\n",
    "FIGURE_DIR = \"../src/visualization/plots\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17623456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: X shape (22680000, 25), y shape (22680000,)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path=DATA_PATH):\n",
    "    \"\"\"Load dataset from NPZ file.\"\"\"\n",
    "    data = np.load(path)\n",
    "    X, y = data[\"X\"], data[\"y\"]\n",
    "    print(f\"Loaded dataset: X shape {X.shape}, y shape {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X, y = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9110167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (18144000, 25), Test set: (4536000, 25)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits into train/test and normalizes features.\n",
    "    Prefix prices + numeric features are scaled.\n",
    "    opt_flag is kept as-is (categorical 0/1).\n",
    "    \"\"\"\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Separate opt_flag (last column)\n",
    "    X_train_prefix = X_train[:, :-1]\n",
    "    X_test_prefix = X_test[:, :-1]\n",
    "\n",
    "    opt_flag_train = X_train[:, -1].reshape(-1, 1)\n",
    "    opt_flag_test = X_test[:, -1].reshape(-1, 1)\n",
    "\n",
    "    # Scale everything except opt_flag\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_prefix)\n",
    "    X_test_scaled = scaler.transform(X_test_prefix)\n",
    "\n",
    "    # Reattach opt_flag\n",
    "    X_train_final = np.hstack([X_train_scaled, opt_flag_train])\n",
    "    X_test_final = np.hstack([X_test_scaled, opt_flag_test])\n",
    "\n",
    "    # Save scaler for later inference\n",
    "    os.makedirs(SCALER_DIR, exist_ok=True)\n",
    "    joblib.dump(scaler, os.path.join(SCALER_DIR, \"feature_scaler.pkl\"))\n",
    "\n",
    "    print(f\"Train set: {X_train_final.shape}, Test set: {X_test_final.shape}\")\n",
    "    return X_train_final, X_test_final, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cef6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f90b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history, title, filename):\n",
    "    \"\"\"Save training curves for loss + MAE + RMSE.\"\"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Val MAE\")\n",
    "    plt.title(f\"{title} - MAE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    # RMSE plot (computed from loss)\n",
    "    train_rmse = np.sqrt(history.history[\"loss\"])\n",
    "    val_rmse = np.sqrt(history.history[\"val_loss\"])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_rmse, label=\"Train RMSE\")\n",
    "    plt.plot(val_rmse, label=\"Val RMSE\")\n",
    "    plt.title(f\"{title} - RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Root MSE\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURE_DIR, filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0165c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 705us/step - loss: 96.1570 - mae: 4.4488 - val_loss: 94.6974 - val_mae: 4.3128\n",
      "Epoch 2/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 705us/step - loss: 96.1570 - mae: 4.4488 - val_loss: 94.6974 - val_mae: 4.3128\n",
      "Epoch 2/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 677us/step - loss: 94.2414 - mae: 4.3172 - val_loss: 94.2153 - val_mae: 4.2751\n",
      "Epoch 3/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 677us/step - loss: 94.2414 - mae: 4.3172 - val_loss: 94.2153 - val_mae: 4.2751\n",
      "Epoch 3/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 704us/step - loss: 93.9700 - mae: 4.2890 - val_loss: 94.3808 - val_mae: 4.3060\n",
      "Epoch 4/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 704us/step - loss: 93.9700 - mae: 4.2890 - val_loss: 94.3808 - val_mae: 4.3060\n",
      "Epoch 4/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 719us/step - loss: 93.8551 - mae: 4.2801 - val_loss: 94.1555 - val_mae: 4.2734\n",
      "Epoch 5/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 719us/step - loss: 93.8551 - mae: 4.2801 - val_loss: 94.1555 - val_mae: 4.2734\n",
      "Epoch 5/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 712us/step - loss: 93.7958 - mae: 4.2760 - val_loss: 94.1844 - val_mae: 4.2714\n",
      "Epoch 6/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 712us/step - loss: 93.7958 - mae: 4.2760 - val_loss: 94.1844 - val_mae: 4.2714\n",
      "Epoch 6/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 729us/step - loss: 93.7692 - mae: 4.2753 - val_loss: 94.3970 - val_mae: 4.2459\n",
      "Epoch 7/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 729us/step - loss: 93.7692 - mae: 4.2753 - val_loss: 94.3970 - val_mae: 4.2459\n",
      "Epoch 7/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 720us/step - loss: 93.7975 - mae: 4.2772 - val_loss: 94.3185 - val_mae: 4.3190\n",
      "Epoch 8/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 720us/step - loss: 93.7975 - mae: 4.2772 - val_loss: 94.3185 - val_mae: 4.3190\n",
      "Epoch 8/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 764us/step - loss: 93.7612 - mae: 4.2750 - val_loss: 94.1736 - val_mae: 4.3086\n",
      "Epoch 9/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 764us/step - loss: 93.7612 - mae: 4.2750 - val_loss: 94.1736 - val_mae: 4.3086\n",
      "Epoch 9/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 768us/step - loss: 93.7596 - mae: 4.2770 - val_loss: 96.1284 - val_mae: 4.3412\n",
      "Epoch 10/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 768us/step - loss: 93.7596 - mae: 4.2770 - val_loss: 96.1284 - val_mae: 4.3412\n",
      "Epoch 10/10\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 757us/step - loss: 93.7393 - mae: 4.2756 - val_loss: 94.0619 - val_mae: 4.2367\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 757us/step - loss: 93.7393 - mae: 4.2756 - val_loss: 94.0619 - val_mae: 4.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 377us/step\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 377us/step\n",
      "Evaluation on Test Set -> MAE: 4.2367, RMSE: 9.6985\n",
      "Evaluation on Test Set -> MAE: 4.2367, RMSE: 9.6985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(4.236670574655875), np.float64(9.698543360537787))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_quick_mlp(input_dim):\n",
    "    \"\"\"Small, quick baseline MLP.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Compute MAE and RMSE on test set.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = np.mean(np.abs(y_test - y_pred.flatten()))\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred.flatten()) ** 2))\n",
    "    print(f\"Evaluation on Test Set -> MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "quick_mlp = build_quick_mlp(input_dim)\n",
    "history_quick = quick_mlp.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "quick_mlp.save(os.path.join(MODEL_DIR, \"mlp_quick.h5\"))\n",
    "plot_training(history_quick, \"Quick MLP\", \"training_quick_run_2.png\")\n",
    "evaluate_model(quick_mlp, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc39133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_large_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Number of hidden layers\n",
    "    num_layers = hp.Int(\"num_layers\", min_value=2, max_value=6, step=1)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32)\n",
    "        activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\", \"mse\", \"accuracy\"]  # include accuracy for monitoring\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed55014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_tuning\\tuner0.json\n",
      "  - num_layers: 3\n",
      "  - units_0: 96\n",
      "  - activation: relu\n",
      "  - units_1: 192\n",
      "  - optimizer: sgd\n",
      "  - lr: 0.0005822751269209618\n",
      "  - units_2: 352\n",
      "  - units_3: 320\n",
      "  - units_4: 384\n",
      "  - units_5: 352\n",
      "  - tuner/epochs: 6\n",
      "  - tuner/initial_epoch: 0\n",
      "  - tuner/bracket: 2\n",
      "  - tuner/round: 0\n",
      "  - num_layers: 3\n",
      "  - units_0: 96\n",
      "  - activation: relu\n",
      "  - units_1: 192\n",
      "  - optimizer: sgd\n",
      "  - lr: 0.0005822751269209618\n",
      "  - units_2: 352\n",
      "  - units_3: 320\n",
      "  - units_4: 384\n",
      "  - units_5: 352\n",
      "  - tuner/epochs: 6\n",
      "  - tuner/initial_epoch: 0\n",
      "  - tuner/bracket: 2\n",
      "  - tuner/round: 0\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_large_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_epochs=50,  # upper bound for epochs\n",
    "    factor=3,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_tuning\"\n",
    ")\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=kt.HyperParameters().Int(\"batch_size\", min_value=32, max_value=256, step=32),\n",
    "    epochs=50\n",
    ")\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "for hp in best_hps.values.keys():\n",
    "    print(f\"  - {hp}: {best_hps.get(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f0db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1ms/step - accuracy: 0.3077 - loss: 97.4116 - mae: 4.4431 - mse: 97.4116 - val_accuracy: 0.3284 - val_loss: 94.9298 - val_mae: 4.2647 - val_mse: 94.9298\n",
      "Epoch 2/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1ms/step - accuracy: 0.3077 - loss: 97.4116 - mae: 4.4431 - mse: 97.4116 - val_accuracy: 0.3284 - val_loss: 94.9298 - val_mae: 4.2647 - val_mse: 94.9298\n",
      "Epoch 2/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 1ms/step - accuracy: 0.3010 - loss: 95.3363 - mae: 4.3945 - mse: 95.3363 - val_accuracy: 0.2511 - val_loss: 94.9959 - val_mae: 4.4549 - val_mse: 94.9959\n",
      "Epoch 3/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 1ms/step - accuracy: 0.3010 - loss: 95.3363 - mae: 4.3945 - mse: 95.3363 - val_accuracy: 0.2511 - val_loss: 94.9959 - val_mae: 4.4549 - val_mse: 94.9959\n",
      "Epoch 3/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2958 - loss: 94.9430 - mae: 4.4000 - mse: 94.9430 - val_accuracy: 0.3565 - val_loss: 95.5808 - val_mae: 4.4020 - val_mse: 95.5808\n",
      "Epoch 4/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2958 - loss: 94.9430 - mae: 4.4000 - mse: 94.9430 - val_accuracy: 0.3565 - val_loss: 95.5808 - val_mae: 4.4020 - val_mse: 95.5808\n",
      "Epoch 4/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.8342 - mae: 4.4184 - mse: 94.8342 - val_accuracy: 0.3624 - val_loss: 95.9922 - val_mae: 4.4516 - val_mse: 95.9922\n",
      "Epoch 5/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.8342 - mae: 4.4184 - mse: 94.8342 - val_accuracy: 0.3624 - val_loss: 95.9922 - val_mae: 4.4516 - val_mse: 95.9922\n",
      "Epoch 5/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2866 - loss: 94.7671 - mae: 4.4285 - mse: 94.7671 - val_accuracy: 0.2670 - val_loss: 94.9676 - val_mae: 4.4321 - val_mse: 94.9676\n",
      "Epoch 6/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2866 - loss: 94.7671 - mae: 4.4285 - mse: 94.7671 - val_accuracy: 0.2670 - val_loss: 94.9676 - val_mae: 4.4321 - val_mse: 94.9676\n",
      "Epoch 6/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2857 - loss: 94.7734 - mae: 4.4342 - mse: 94.7734 - val_accuracy: 0.3364 - val_loss: 94.7174 - val_mae: 4.2996 - val_mse: 94.7174\n",
      "Epoch 7/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2857 - loss: 94.7734 - mae: 4.4342 - mse: 94.7734 - val_accuracy: 0.3364 - val_loss: 94.7174 - val_mae: 4.2996 - val_mse: 94.7174\n",
      "Epoch 7/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2829 - loss: 94.8329 - mae: 4.4530 - mse: 94.8329 - val_accuracy: 0.3556 - val_loss: 94.7954 - val_mae: 4.4125 - val_mse: 94.7954\n",
      "Epoch 8/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2829 - loss: 94.8329 - mae: 4.4530 - mse: 94.8329 - val_accuracy: 0.3556 - val_loss: 94.7954 - val_mae: 4.4125 - val_mse: 94.7954\n",
      "Epoch 8/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1ms/step - accuracy: 0.2814 - loss: 94.7695 - mae: 4.4574 - mse: 94.7695 - val_accuracy: 0.3556 - val_loss: 95.2367 - val_mae: 4.3987 - val_mse: 95.2367\n",
      "Epoch 9/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1ms/step - accuracy: 0.2814 - loss: 94.7695 - mae: 4.4574 - mse: 94.7695 - val_accuracy: 0.3556 - val_loss: 95.2367 - val_mae: 4.3987 - val_mse: 95.2367\n",
      "Epoch 9/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2839 - loss: 94.7083 - mae: 4.4467 - mse: 94.7083 - val_accuracy: 0.2939 - val_loss: 94.9467 - val_mae: 4.4549 - val_mse: 94.9467\n",
      "Epoch 10/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2839 - loss: 94.7083 - mae: 4.4467 - mse: 94.7083 - val_accuracy: 0.2939 - val_loss: 94.9467 - val_mae: 4.4549 - val_mse: 94.9467\n",
      "Epoch 10/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.7739 - mae: 4.4650 - mse: 94.7739 - val_accuracy: 0.2346 - val_loss: 95.1904 - val_mae: 4.5046 - val_mse: 95.1904\n",
      "Epoch 11/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.7739 - mae: 4.4650 - mse: 94.7739 - val_accuracy: 0.2346 - val_loss: 95.1904 - val_mae: 4.5046 - val_mse: 95.1904\n",
      "Epoch 11/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2800 - loss: 94.7282 - mae: 4.4651 - mse: 94.7282 - val_accuracy: 0.3036 - val_loss: 94.7908 - val_mae: 4.3842 - val_mse: 94.7908\n",
      "Epoch 12/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2800 - loss: 94.7282 - mae: 4.4651 - mse: 94.7282 - val_accuracy: 0.3036 - val_loss: 94.7908 - val_mae: 4.3842 - val_mse: 94.7908\n",
      "Epoch 12/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2805 - loss: 94.7353 - mae: 4.4639 - mse: 94.7353 - val_accuracy: 0.3326 - val_loss: 94.7666 - val_mae: 4.2643 - val_mse: 94.7666\n",
      "Epoch 13/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2805 - loss: 94.7353 - mae: 4.4639 - mse: 94.7353 - val_accuracy: 0.3326 - val_loss: 94.7666 - val_mae: 4.2643 - val_mse: 94.7666\n",
      "Epoch 13/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2806 - loss: 94.6695 - mae: 4.4626 - mse: 94.6695 - val_accuracy: 0.2060 - val_loss: 95.8419 - val_mae: 4.4792 - val_mse: 95.8419\n",
      "Epoch 14/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2806 - loss: 94.6695 - mae: 4.4626 - mse: 94.6695 - val_accuracy: 0.2060 - val_loss: 95.8419 - val_mae: 4.4792 - val_mse: 95.8419\n",
      "Epoch 14/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2822 - loss: 94.6411 - mae: 4.4573 - mse: 94.6411 - val_accuracy: 0.3365 - val_loss: 95.7161 - val_mae: 4.4438 - val_mse: 95.7161\n",
      "Epoch 15/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 1ms/step - accuracy: 0.2822 - loss: 94.6411 - mae: 4.4573 - mse: 94.6411 - val_accuracy: 0.3365 - val_loss: 95.7161 - val_mae: 4.4438 - val_mse: 95.7161\n",
      "Epoch 15/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2817 - loss: 94.6972 - mae: 4.4634 - mse: 94.6972 - val_accuracy: 0.2829 - val_loss: 95.0396 - val_mae: 4.4724 - val_mse: 95.0396\n",
      "Epoch 16/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2817 - loss: 94.6972 - mae: 4.4634 - mse: 94.6972 - val_accuracy: 0.2829 - val_loss: 95.0396 - val_mae: 4.4724 - val_mse: 95.0396\n",
      "Epoch 16/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2817 - loss: 94.7092 - mae: 4.4650 - mse: 94.7092 - val_accuracy: 0.2783 - val_loss: 95.8453 - val_mae: 4.5624 - val_mse: 95.8453\n",
      "Epoch 17/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2817 - loss: 94.7092 - mae: 4.4650 - mse: 94.7092 - val_accuracy: 0.2783 - val_loss: 95.8453 - val_mae: 4.5624 - val_mse: 95.8453\n",
      "Epoch 17/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2831 - loss: 94.6735 - mae: 4.4604 - mse: 94.6735 - val_accuracy: 0.3336 - val_loss: 95.3402 - val_mae: 4.4063 - val_mse: 95.3402\n",
      "Epoch 18/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2831 - loss: 94.6735 - mae: 4.4604 - mse: 94.6735 - val_accuracy: 0.3336 - val_loss: 95.3402 - val_mae: 4.4063 - val_mse: 95.3402\n",
      "Epoch 18/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2804 - loss: 94.6373 - mae: 4.4694 - mse: 94.6373 - val_accuracy: 0.2112 - val_loss: 95.8251 - val_mae: 4.6194 - val_mse: 95.8251\n",
      "Epoch 19/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2804 - loss: 94.6373 - mae: 4.4694 - mse: 94.6373 - val_accuracy: 0.2112 - val_loss: 95.8251 - val_mae: 4.6194 - val_mse: 95.8251\n",
      "Epoch 19/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2794 - loss: 94.6662 - mae: 4.4722 - mse: 94.6662 - val_accuracy: 0.3788 - val_loss: 94.7525 - val_mae: 4.4164 - val_mse: 94.7525\n",
      "Epoch 20/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2794 - loss: 94.6662 - mae: 4.4722 - mse: 94.6662 - val_accuracy: 0.3788 - val_loss: 94.7525 - val_mae: 4.4164 - val_mse: 94.7525\n",
      "Epoch 20/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.6874 - mae: 4.4688 - mse: 94.6874 - val_accuracy: 0.2143 - val_loss: 95.0948 - val_mae: 4.5221 - val_mse: 95.0948\n",
      "Epoch 21/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.6874 - mae: 4.4688 - mse: 94.6874 - val_accuracy: 0.2143 - val_loss: 95.0948 - val_mae: 4.5221 - val_mse: 95.0948\n",
      "Epoch 21/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2881 - loss: 94.6300 - mae: 4.4490 - mse: 94.6300 - val_accuracy: 0.2053 - val_loss: 95.3367 - val_mae: 4.6449 - val_mse: 95.3367\n",
      "Epoch 22/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2881 - loss: 94.6300 - mae: 4.4490 - mse: 94.6300 - val_accuracy: 0.2053 - val_loss: 95.3367 - val_mae: 4.6449 - val_mse: 95.3367\n",
      "Epoch 22/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2821 - loss: 94.6454 - mae: 4.4658 - mse: 94.6454 - val_accuracy: 0.3086 - val_loss: 94.7359 - val_mae: 4.4171 - val_mse: 94.7359\n",
      "Epoch 23/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2821 - loss: 94.6454 - mae: 4.4658 - mse: 94.6454 - val_accuracy: 0.3086 - val_loss: 94.7359 - val_mae: 4.4171 - val_mse: 94.7359\n",
      "Epoch 23/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.5650 - mae: 4.4617 - mse: 94.5650 - val_accuracy: 0.3014 - val_loss: 96.1341 - val_mae: 4.7390 - val_mse: 96.1341\n",
      "Epoch 24/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1ms/step - accuracy: 0.2810 - loss: 94.5650 - mae: 4.4617 - mse: 94.5650 - val_accuracy: 0.3014 - val_loss: 96.1341 - val_mae: 4.7390 - val_mse: 96.1341\n",
      "Epoch 24/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2797 - loss: 94.6213 - mae: 4.4664 - mse: 94.6213 - val_accuracy: 0.2799 - val_loss: 95.0241 - val_mae: 4.3993 - val_mse: 95.0241\n",
      "Epoch 25/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2797 - loss: 94.6213 - mae: 4.4664 - mse: 94.6213 - val_accuracy: 0.2799 - val_loss: 95.0241 - val_mae: 4.3993 - val_mse: 95.0241\n",
      "Epoch 25/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2780 - loss: 94.6714 - mae: 4.4741 - mse: 94.6714 - val_accuracy: 0.3168 - val_loss: 94.9686 - val_mae: 4.3420 - val_mse: 94.9686\n",
      "Epoch 26/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2780 - loss: 94.6714 - mae: 4.4741 - mse: 94.6714 - val_accuracy: 0.3168 - val_loss: 94.9686 - val_mae: 4.3420 - val_mse: 94.9686\n",
      "Epoch 26/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.5916 - mae: 4.4369 - mse: 94.5916 - val_accuracy: 0.1972 - val_loss: 94.9833 - val_mae: 4.5128 - val_mse: 94.9833\n",
      "Epoch 27/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.5916 - mae: 4.4369 - mse: 94.5916 - val_accuracy: 0.1972 - val_loss: 94.9833 - val_mae: 4.5128 - val_mse: 94.9833\n",
      "Epoch 27/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2938 - loss: 94.6229 - mae: 4.4319 - mse: 94.6229 - val_accuracy: 0.2531 - val_loss: 95.1162 - val_mae: 4.3992 - val_mse: 95.1162\n",
      "Epoch 28/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2938 - loss: 94.6229 - mae: 4.4319 - mse: 94.6229 - val_accuracy: 0.2531 - val_loss: 95.1162 - val_mae: 4.3992 - val_mse: 95.1162\n",
      "Epoch 28/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2928 - loss: 94.6315 - mae: 4.4331 - mse: 94.6315 - val_accuracy: 0.3909 - val_loss: 94.9715 - val_mae: 4.3644 - val_mse: 94.9715\n",
      "Epoch 29/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2928 - loss: 94.6315 - mae: 4.4331 - mse: 94.6315 - val_accuracy: 0.3909 - val_loss: 94.9715 - val_mae: 4.3644 - val_mse: 94.9715\n",
      "Epoch 29/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2935 - loss: 94.5758 - mae: 4.4290 - mse: 94.5758 - val_accuracy: 0.2141 - val_loss: 95.0569 - val_mae: 4.5748 - val_mse: 95.0569\n",
      "Epoch 30/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2935 - loss: 94.5758 - mae: 4.4290 - mse: 94.5758 - val_accuracy: 0.2141 - val_loss: 95.0569 - val_mae: 4.5748 - val_mse: 95.0569\n",
      "Epoch 30/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2938 - loss: 94.5739 - mae: 4.4323 - mse: 94.5739 - val_accuracy: 0.2993 - val_loss: 94.6494 - val_mae: 4.3587 - val_mse: 94.6494\n",
      "Epoch 31/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 1ms/step - accuracy: 0.2938 - loss: 94.5739 - mae: 4.4323 - mse: 94.5739 - val_accuracy: 0.2993 - val_loss: 94.6494 - val_mae: 4.3587 - val_mse: 94.6494\n",
      "Epoch 31/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2949 - loss: 94.5542 - mae: 4.4269 - mse: 94.5542 - val_accuracy: 0.3694 - val_loss: 94.7281 - val_mae: 4.3531 - val_mse: 94.7281\n",
      "Epoch 32/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2949 - loss: 94.5542 - mae: 4.4269 - mse: 94.5542 - val_accuracy: 0.3694 - val_loss: 94.7281 - val_mae: 4.3531 - val_mse: 94.7281\n",
      "Epoch 32/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2961 - loss: 94.5500 - mae: 4.4246 - mse: 94.5500 - val_accuracy: 0.2406 - val_loss: 95.8310 - val_mae: 4.6460 - val_mse: 95.8310\n",
      "Epoch 33/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2961 - loss: 94.5500 - mae: 4.4246 - mse: 94.5500 - val_accuracy: 0.2406 - val_loss: 95.8310 - val_mae: 4.6460 - val_mse: 95.8310\n",
      "Epoch 33/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2969 - loss: 94.5429 - mae: 4.4239 - mse: 94.5429 - val_accuracy: 0.2472 - val_loss: 94.6703 - val_mae: 4.3890 - val_mse: 94.6703\n",
      "Epoch 34/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2969 - loss: 94.5429 - mae: 4.4239 - mse: 94.5429 - val_accuracy: 0.2472 - val_loss: 94.6703 - val_mae: 4.3890 - val_mse: 94.6703\n",
      "Epoch 34/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2948 - loss: 94.5521 - mae: 4.4257 - mse: 94.5521 - val_accuracy: 0.2988 - val_loss: 94.5620 - val_mae: 4.3368 - val_mse: 94.5620\n",
      "Epoch 35/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2948 - loss: 94.5521 - mae: 4.4257 - mse: 94.5521 - val_accuracy: 0.2988 - val_loss: 94.5620 - val_mae: 4.3368 - val_mse: 94.5620\n",
      "Epoch 35/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2898 - loss: 94.5409 - mae: 4.4398 - mse: 94.5409 - val_accuracy: 0.0589 - val_loss: 96.4424 - val_mae: 5.0290 - val_mse: 96.4424\n",
      "Epoch 36/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2898 - loss: 94.5409 - mae: 4.4398 - mse: 94.5409 - val_accuracy: 0.0589 - val_loss: 96.4424 - val_mae: 5.0290 - val_mse: 96.4424\n",
      "Epoch 36/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2826 - loss: 94.5072 - mae: 4.4555 - mse: 94.5072 - val_accuracy: 0.3722 - val_loss: 94.4877 - val_mae: 4.3109 - val_mse: 94.4877\n",
      "Epoch 37/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 1ms/step - accuracy: 0.2826 - loss: 94.5072 - mae: 4.4555 - mse: 94.5072 - val_accuracy: 0.3722 - val_loss: 94.4877 - val_mae: 4.3109 - val_mse: 94.4877\n",
      "Epoch 37/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2814 - loss: 94.5338 - mae: 4.4623 - mse: 94.5338 - val_accuracy: 0.3875 - val_loss: 94.9291 - val_mae: 4.3863 - val_mse: 94.9291\n",
      "Epoch 38/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 1ms/step - accuracy: 0.2814 - loss: 94.5338 - mae: 4.4623 - mse: 94.5338 - val_accuracy: 0.3875 - val_loss: 94.9291 - val_mae: 4.3863 - val_mse: 94.9291\n",
      "Epoch 38/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2850 - loss: 94.4901 - mae: 4.4486 - mse: 94.4901 - val_accuracy: 0.3518 - val_loss: 95.1226 - val_mae: 4.3386 - val_mse: 95.1226\n",
      "Epoch 39/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2850 - loss: 94.4901 - mae: 4.4486 - mse: 94.4901 - val_accuracy: 0.3518 - val_loss: 95.1226 - val_mae: 4.3386 - val_mse: 95.1226\n",
      "Epoch 39/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2947 - loss: 94.4964 - mae: 4.4264 - mse: 94.4964 - val_accuracy: 0.2123 - val_loss: 94.5178 - val_mae: 4.4204 - val_mse: 94.5178\n",
      "Epoch 40/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2947 - loss: 94.4964 - mae: 4.4264 - mse: 94.4964 - val_accuracy: 0.2123 - val_loss: 94.5178 - val_mae: 4.4204 - val_mse: 94.5178\n",
      "Epoch 40/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2966 - loss: 94.4652 - mae: 4.4186 - mse: 94.4652 - val_accuracy: 0.3697 - val_loss: 94.8653 - val_mae: 4.3414 - val_mse: 94.8653\n",
      "Epoch 41/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2966 - loss: 94.4652 - mae: 4.4186 - mse: 94.4652 - val_accuracy: 0.3697 - val_loss: 94.8653 - val_mae: 4.3414 - val_mse: 94.8653\n",
      "Epoch 41/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2806 - loss: 94.6160 - mae: 4.4615 - mse: 94.6160 - val_accuracy: 0.3421 - val_loss: 94.9590 - val_mae: 4.3726 - val_mse: 94.9590\n",
      "Epoch 42/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2806 - loss: 94.6160 - mae: 4.4615 - mse: 94.6160 - val_accuracy: 0.3421 - val_loss: 94.9590 - val_mae: 4.3726 - val_mse: 94.9590\n",
      "Epoch 42/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2786 - loss: 94.6427 - mae: 4.4713 - mse: 94.6427 - val_accuracy: 0.3267 - val_loss: 95.5547 - val_mae: 4.3353 - val_mse: 95.5547\n",
      "Epoch 43/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2786 - loss: 94.6427 - mae: 4.4713 - mse: 94.6427 - val_accuracy: 0.3267 - val_loss: 95.5547 - val_mae: 4.3353 - val_mse: 95.5547\n",
      "Epoch 43/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2767 - loss: 94.7212 - mae: 4.4876 - mse: 94.7212 - val_accuracy: 0.3699 - val_loss: 95.0044 - val_mae: 4.3102 - val_mse: 95.0044\n",
      "Epoch 44/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 1ms/step - accuracy: 0.2767 - loss: 94.7212 - mae: 4.4876 - mse: 94.7212 - val_accuracy: 0.3699 - val_loss: 95.0044 - val_mae: 4.3102 - val_mse: 95.0044\n",
      "Epoch 44/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2783 - loss: 94.5693 - mae: 4.4724 - mse: 94.5693 - val_accuracy: 0.3748 - val_loss: 94.9706 - val_mae: 4.2873 - val_mse: 94.9706\n",
      "Epoch 45/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2783 - loss: 94.5693 - mae: 4.4724 - mse: 94.5693 - val_accuracy: 0.3748 - val_loss: 94.9706 - val_mae: 4.2873 - val_mse: 94.9706\n",
      "Epoch 45/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2825 - loss: 94.5719 - mae: 4.4565 - mse: 94.5719 - val_accuracy: 0.3627 - val_loss: 94.7776 - val_mae: 4.3110 - val_mse: 94.7776\n",
      "Epoch 46/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2825 - loss: 94.5719 - mae: 4.4565 - mse: 94.5719 - val_accuracy: 0.3627 - val_loss: 94.7776 - val_mae: 4.3110 - val_mse: 94.7776\n",
      "Epoch 46/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.5320 - mae: 4.4338 - mse: 94.5320 - val_accuracy: 0.3716 - val_loss: 94.7266 - val_mae: 4.3282 - val_mse: 94.7266\n",
      "Epoch 47/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 1ms/step - accuracy: 0.2896 - loss: 94.5320 - mae: 4.4338 - mse: 94.5320 - val_accuracy: 0.3716 - val_loss: 94.7266 - val_mae: 4.3282 - val_mse: 94.7266\n",
      "Epoch 47/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2819 - loss: 94.6056 - mae: 4.4697 - mse: 94.6056 - val_accuracy: 0.3557 - val_loss: 94.7746 - val_mae: 4.3595 - val_mse: 94.7746\n",
      "Epoch 48/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2819 - loss: 94.6056 - mae: 4.4697 - mse: 94.6056 - val_accuracy: 0.3557 - val_loss: 94.7746 - val_mae: 4.3595 - val_mse: 94.7746\n",
      "Epoch 48/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2791 - loss: 94.5982 - mae: 4.4795 - mse: 94.5982 - val_accuracy: 0.3165 - val_loss: 95.0577 - val_mae: 4.3504 - val_mse: 95.0577\n",
      "Epoch 49/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2791 - loss: 94.5982 - mae: 4.4795 - mse: 94.5982 - val_accuracy: 0.3165 - val_loss: 95.0577 - val_mae: 4.3504 - val_mse: 95.0577\n",
      "Epoch 49/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2780 - loss: 94.6311 - mae: 4.4841 - mse: 94.6311 - val_accuracy: 0.2957 - val_loss: 94.4034 - val_mae: 4.3587 - val_mse: 94.4034\n",
      "Epoch 50/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1ms/step - accuracy: 0.2780 - loss: 94.6311 - mae: 4.4841 - mse: 94.6311 - val_accuracy: 0.2957 - val_loss: 94.4034 - val_mae: 4.3587 - val_mse: 94.4034\n",
      "Epoch 50/50\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 1ms/step - accuracy: 0.2778 - loss: 94.6151 - mae: 4.4867 - mse: 94.6151 - val_accuracy: 0.3831 - val_loss: 94.9983 - val_mae: 4.5013 - val_mse: 94.9983\n",
      "\u001b[1m283500/283500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 1ms/step - accuracy: 0.2778 - loss: 94.6151 - mae: 4.4867 - mse: 94.6151 - val_accuracy: 0.3831 - val_loss: 94.9983 - val_mae: 4.5013 - val_mse: 94.9983\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeeaee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 437us/step\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 437us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation:\n",
      "  MAE: 4.5012\n",
      "  RMSE: 9.7467\n",
      "  Accuracy: 0.2132\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test).flatten()\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "acc = np.mean(np.isclose(np.round(y_test), np.round(y_pred)))  # crude regression accuracy\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation:\\n  MAE: {mae:.4f}\\n  RMSE: {rmse:.4f}\\n  Accuracy: {acc:.4f}\")\n",
    "model.save(os.path.join(MODEL_DIR, \"mlp_large_tuned.h5\"))\n",
    "plot_training(history, \"Large MLP (Tuned)\", \"training_large_tuned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab703a",
   "metadata": {},
   "source": [
    "Not the best numbers, next few steps - \n",
    "1. Baseline from mean of y_train -> check wif the NN is beating this\n",
    "2. Try different batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8f394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (mean predictor):\n",
      "  MAE: 0.7519\n",
      "  RMSE: 1.0001\n",
      "  R²: -0.0000\n"
     ]
    }
   ],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "y_baseline = np.full_like(y_test_scaled, fill_value=np.mean(y_train_scaled))\n",
    "baseline_mae = mean_absolute_error(y_test_scaled, y_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_scaled, y_baseline))\n",
    "baseline_r2 = r2_score(y_test_scaled, y_baseline)\n",
    "\n",
    "print(f\"\\nBaseline (mean predictor):\\n\"\n",
    "      f\"  MAE: {baseline_mae:.4f}\\n\"\n",
    "      f\"  RMSE: {baseline_rmse:.4f}\\n\"\n",
    "      f\"  R²: {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf5d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e3b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 5.5115e-08 - loss: 0.4700 - mae: 0.3875 - mse: 0.4700 - val_accuracy: 0.0000e+00 - val_loss: 0.3378 - val_mae: 0.2744 - val_mse: 0.3378\n",
      "Epoch 2/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 5.5115e-08 - loss: 0.4700 - mae: 0.3875 - mse: 0.4700 - val_accuracy: 0.0000e+00 - val_loss: 0.3378 - val_mae: 0.2744 - val_mse: 0.3378\n",
      "Epoch 2/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3323 - mae: 0.2679 - mse: 0.3323 - val_accuracy: 0.0000e+00 - val_loss: 0.3324 - val_mae: 0.2637 - val_mse: 0.3324\n",
      "Epoch 3/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3323 - mae: 0.2679 - mse: 0.3323 - val_accuracy: 0.0000e+00 - val_loss: 0.3324 - val_mae: 0.2637 - val_mse: 0.3324\n",
      "Epoch 3/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3296 - mae: 0.2619 - mse: 0.3296 - val_accuracy: 0.0000e+00 - val_loss: 0.3308 - val_mae: 0.2601 - val_mse: 0.3308\n",
      "Epoch 4/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3296 - mae: 0.2619 - mse: 0.3296 - val_accuracy: 0.0000e+00 - val_loss: 0.3308 - val_mae: 0.2601 - val_mse: 0.3308\n",
      "Epoch 4/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3286 - mae: 0.2595 - mse: 0.3286 - val_accuracy: 0.0000e+00 - val_loss: 0.3302 - val_mae: 0.2578 - val_mse: 0.3302\n",
      "Epoch 5/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3286 - mae: 0.2595 - mse: 0.3286 - val_accuracy: 0.0000e+00 - val_loss: 0.3302 - val_mae: 0.2578 - val_mse: 0.3302\n",
      "Epoch 5/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3281 - mae: 0.2581 - mse: 0.3281 - val_accuracy: 0.0000e+00 - val_loss: 0.3298 - val_mae: 0.2574 - val_mse: 0.3298\n",
      "Epoch 6/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3281 - mae: 0.2581 - mse: 0.3281 - val_accuracy: 0.0000e+00 - val_loss: 0.3298 - val_mae: 0.2574 - val_mse: 0.3298\n",
      "Epoch 6/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3277 - mae: 0.2572 - mse: 0.3277 - val_accuracy: 0.0000e+00 - val_loss: 0.3293 - val_mae: 0.2558 - val_mse: 0.3293\n",
      "Epoch 7/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3277 - mae: 0.2572 - mse: 0.3277 - val_accuracy: 0.0000e+00 - val_loss: 0.3293 - val_mae: 0.2558 - val_mse: 0.3293\n",
      "Epoch 7/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3275 - mae: 0.2564 - mse: 0.3275 - val_accuracy: 0.0000e+00 - val_loss: 0.3293 - val_mae: 0.2582 - val_mse: 0.3293\n",
      "Epoch 8/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3275 - mae: 0.2564 - mse: 0.3275 - val_accuracy: 0.0000e+00 - val_loss: 0.3293 - val_mae: 0.2582 - val_mse: 0.3293\n",
      "Epoch 8/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3273 - mae: 0.2559 - mse: 0.3273 - val_accuracy: 0.0000e+00 - val_loss: 0.3292 - val_mae: 0.2553 - val_mse: 0.3292\n",
      "Epoch 9/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3273 - mae: 0.2559 - mse: 0.3273 - val_accuracy: 0.0000e+00 - val_loss: 0.3292 - val_mae: 0.2553 - val_mse: 0.3292\n",
      "Epoch 9/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3271 - mae: 0.2555 - mse: 0.3271 - val_accuracy: 0.0000e+00 - val_loss: 0.3292 - val_mae: 0.2544 - val_mse: 0.3292\n",
      "Epoch 10/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3271 - mae: 0.2555 - mse: 0.3271 - val_accuracy: 0.0000e+00 - val_loss: 0.3292 - val_mae: 0.2544 - val_mse: 0.3292\n",
      "Epoch 10/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3270 - mae: 0.2551 - mse: 0.3270 - val_accuracy: 0.0000e+00 - val_loss: 0.3291 - val_mae: 0.2578 - val_mse: 0.3291\n",
      "Epoch 11/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3270 - mae: 0.2551 - mse: 0.3270 - val_accuracy: 0.0000e+00 - val_loss: 0.3291 - val_mae: 0.2578 - val_mse: 0.3291\n",
      "Epoch 11/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3269 - mae: 0.2548 - mse: 0.3269 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2531 - val_mse: 0.3287\n",
      "Epoch 12/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3269 - mae: 0.2548 - mse: 0.3269 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2531 - val_mse: 0.3287\n",
      "Epoch 12/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3268 - mae: 0.2545 - mse: 0.3268 - val_accuracy: 0.0000e+00 - val_loss: 0.3290 - val_mae: 0.2527 - val_mse: 0.3290\n",
      "Epoch 13/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3268 - mae: 0.2545 - mse: 0.3268 - val_accuracy: 0.0000e+00 - val_loss: 0.3290 - val_mae: 0.2527 - val_mse: 0.3290\n",
      "Epoch 13/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3267 - mae: 0.2543 - mse: 0.3267 - val_accuracy: 0.0000e+00 - val_loss: 0.3289 - val_mae: 0.2535 - val_mse: 0.3289\n",
      "Epoch 14/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3267 - mae: 0.2543 - mse: 0.3267 - val_accuracy: 0.0000e+00 - val_loss: 0.3289 - val_mae: 0.2535 - val_mse: 0.3289\n",
      "Epoch 14/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3267 - mae: 0.2541 - mse: 0.3267 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2529 - val_mse: 0.3287\n",
      "Epoch 15/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3267 - mae: 0.2541 - mse: 0.3267 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2529 - val_mse: 0.3287\n",
      "Epoch 15/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3266 - mae: 0.2539 - mse: 0.3266 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2516 - val_mse: 0.3285\n",
      "Epoch 16/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3266 - mae: 0.2539 - mse: 0.3266 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2516 - val_mse: 0.3285\n",
      "Epoch 16/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2537 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2561 - val_mse: 0.3287\n",
      "Epoch 17/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2537 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2561 - val_mse: 0.3287\n",
      "Epoch 17/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2536 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2515 - val_mse: 0.3285\n",
      "Epoch 18/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2536 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2515 - val_mse: 0.3285\n",
      "Epoch 18/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2535 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2550 - val_mse: 0.3285\n",
      "Epoch 19/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3265 - mae: 0.2535 - mse: 0.3265 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2550 - val_mse: 0.3285\n",
      "Epoch 19/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3264 - mae: 0.2533 - mse: 0.3264 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2532 - val_mse: 0.3284\n",
      "Epoch 20/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3264 - mae: 0.2533 - mse: 0.3264 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2532 - val_mse: 0.3284\n",
      "Epoch 20/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3264 - mae: 0.2533 - mse: 0.3264 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2517 - val_mse: 0.3287\n",
      "Epoch 21/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3264 - mae: 0.2533 - mse: 0.3264 - val_accuracy: 0.0000e+00 - val_loss: 0.3287 - val_mae: 0.2517 - val_mse: 0.3287\n",
      "Epoch 21/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2531 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2534 - val_mse: 0.3284\n",
      "Epoch 22/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2531 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2534 - val_mse: 0.3284\n",
      "Epoch 22/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2530 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2517 - val_mse: 0.3285\n",
      "Epoch 23/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2530 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2517 - val_mse: 0.3285\n",
      "Epoch 23/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2529 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3286 - val_mae: 0.2516 - val_mse: 0.3286\n",
      "Epoch 24/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2529 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3286 - val_mae: 0.2516 - val_mse: 0.3286\n",
      "Epoch 24/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2529 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2526 - val_mse: 0.3284\n",
      "Epoch 25/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3263 - mae: 0.2529 - mse: 0.3263 - val_accuracy: 0.0000e+00 - val_loss: 0.3284 - val_mae: 0.2526 - val_mse: 0.3284\n",
      "Epoch 25/25\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3262 - mae: 0.2528 - mse: 0.3262 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2509 - val_mse: 0.3285\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1ms/step - accuracy: 1.1023e-07 - loss: 0.3262 - mae: 0.2528 - mse: 0.3262 - val_accuracy: 0.0000e+00 - val_loss: 0.3285 - val_mae: 0.2509 - val_mse: 0.3285\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_test_scaled, y_test_scaled),\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87addde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 424us/step\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 424us/step\n",
      "\n",
      "Final Model Evaluation (scaled):\n",
      "  MAE: 0.2509\n",
      "  RMSE: 0.5731\n",
      "  R²: 0.6716\n",
      "\n",
      "Final Model Evaluation (scaled):\n",
      "  MAE: 0.2509\n",
      "  RMSE: 0.5731\n",
      "  R²: 0.6716\n"
     ]
    }
   ],
   "source": [
    "y_pred_scaled = model.predict(X_test_scaled).flatten()\n",
    "mae = mean_absolute_error(y_test_scaled, y_pred_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_scaled, y_pred_scaled))\n",
    "r2 = r2_score(y_test_scaled, y_pred_scaled)\n",
    "\n",
    "print(f\"\\nFinal Model Evaluation (scaled):\\n\"\n",
    "      f\"  MAE: {mae:.4f}\\n\"\n",
    "      f\"  RMSE: {rmse:.4f}\\n\"\n",
    "      f\"  R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7fc00",
   "metadata": {},
   "source": [
    "- Model seems to be learning from data, with drops in RMSE and MAE\n",
    "- Still improvements to be made\n",
    "- Explore feature engineering, more complex models regularization, CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3cc30",
   "metadata": {},
   "source": [
    "## Further Improvements: Feature Engineering & Advanced Tuning\n",
    "We'll now try to improve the large MLP by adding feature engineering and using more advanced hyperparameter search methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b28edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineered shapes: X_train_fe (18144000, 350), X_test_fe (4536000, 350)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Add polynomial and interaction features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_fe = poly.fit_transform(X_train)\n",
    "X_test_fe = poly.transform(X_test)\n",
    "\n",
    "print(f\"Feature engineered shapes: X_train_fe {X_train_fe.shape}, X_test_fe {X_test_fe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f6164bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_logs\\large_mlp_bayes_tuning\\tuner0.json\n",
      "  - num_layers: 3\n",
      "  - units_0: 64\n",
      "  - activation_0: relu\n",
      "  - units_1: 352\n",
      "  - activation_1: relu\n",
      "  - optimizer: adam\n",
      "  - lr: 0.0016580818738343003\n",
      "  - units_2: 160\n",
      "  - activation_2: tanh\n",
      "  - units_3: 64\n",
      "  - activation_3: tanh\n",
      "  - units_4: 64\n",
      "  - activation_4: relu\n",
      "  - num_layers: 3\n",
      "  - units_0: 64\n",
      "  - activation_0: relu\n",
      "  - units_1: 352\n",
      "  - activation_1: relu\n",
      "  - optimizer: adam\n",
      "  - lr: 0.0016580818738343003\n",
      "  - units_2: 160\n",
      "  - activation_2: tanh\n",
      "  - units_3: 64\n",
      "  - activation_3: tanh\n",
      "  - units_4: 64\n",
      "  - activation_4: relu\n"
     ]
    }
   ],
   "source": [
    "# Advanced Hyperparameter Tuning: Bayesian Optimization\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_advanced_mlp(hp):\n",
    "    model = keras.Sequential()\n",
    "    input_dim = X_train_fe.shape[1]\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    num_layers = hp.Int(\"num_layers\", 2, 6)\n",
    "    for i in range(num_layers):\n",
    "        units = hp.Int(f\"units_{i}\", 32, 512, step=32)\n",
    "        activation = hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\"])\n",
    "        model.add(layers.Dense(units=units, activation=activation))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    optimizer_choice = hp.Choice(\"optimizer\", [\"adam\", \"sgd\", \"rmsprop\"])\n",
    "    learning_rate = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "    if optimizer_choice == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"mse\"])\n",
    "    return model\n",
    "\n",
    "bayes_tuner = BayesianOptimization(\n",
    "    build_advanced_mlp,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=20,\n",
    "    directory=\"tuner_logs\",\n",
    "    project_name=\"large_mlp_bayes_tuning\"\n",
    ")\n",
    "\n",
    "stop_early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)\n",
    "bayes_tuner.search(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    callbacks=[stop_early],\n",
    "    batch_size=64,\n",
    "    epochs=40\n",
    ")\n",
    "best_bayes_hps = bayes_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "for hp in best_bayes_hps.values.keys():\n",
    "    print(f\"  - {hp}: {best_bayes_hps.get(hp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c883fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 253.3592 - mae: 11.2618 - mse: 253.3592 - val_loss: 228.3281 - val_mae: 9.7021 - val_mse: 228.3281\n",
      "Epoch 2/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - loss: 253.3592 - mae: 11.2618 - mse: 253.3592 - val_loss: 228.3281 - val_mae: 9.7021 - val_mse: 228.3281\n",
      "Epoch 2/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 579ms/step - loss: 194.8995 - mae: 8.7755 - mse: 194.8995 - val_loss: 207.9048 - val_mae: 9.1993 - val_mse: 207.9048\n",
      "Epoch 3/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 579ms/step - loss: 194.8995 - mae: 8.7755 - mse: 194.8995 - val_loss: 207.9048 - val_mae: 9.1993 - val_mse: 207.9048\n",
      "Epoch 3/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 570ms/step - loss: 171.0040 - mae: 7.7339 - mse: 171.0040 - val_loss: 200.5439 - val_mae: 8.1629 - val_mse: 200.5439\n",
      "Epoch 4/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 570ms/step - loss: 171.0040 - mae: 7.7339 - mse: 171.0040 - val_loss: 200.5439 - val_mae: 8.1629 - val_mse: 200.5439\n",
      "Epoch 4/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 148.6569 - mae: 6.9066 - mse: 148.6569 - val_loss: 200.4576 - val_mae: 8.7684 - val_mse: 200.4576\n",
      "Epoch 5/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 148.6569 - mae: 6.9066 - mse: 148.6569 - val_loss: 200.4576 - val_mae: 8.7684 - val_mse: 200.4576\n",
      "Epoch 5/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 568ms/step - loss: 131.4825 - mae: 6.3733 - mse: 131.4825 - val_loss: 190.0231 - val_mae: 7.5333 - val_mse: 190.0231\n",
      "Epoch 6/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 568ms/step - loss: 131.4825 - mae: 6.3733 - mse: 131.4825 - val_loss: 190.0231 - val_mae: 7.5333 - val_mse: 190.0231\n",
      "Epoch 6/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 113.7953 - mae: 5.5434 - mse: 113.7953 - val_loss: 175.5450 - val_mae: 7.1481 - val_mse: 175.5450\n",
      "Epoch 7/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 113.7953 - mae: 5.5434 - mse: 113.7953 - val_loss: 175.5450 - val_mae: 7.1481 - val_mse: 175.5450\n",
      "Epoch 7/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 577ms/step - loss: 97.0644 - mae: 5.0223 - mse: 97.0644 - val_loss: 174.5285 - val_mae: 6.8787 - val_mse: 174.5285\n",
      "Epoch 8/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 577ms/step - loss: 97.0644 - mae: 5.0223 - mse: 97.0644 - val_loss: 174.5285 - val_mae: 6.8787 - val_mse: 174.5285\n",
      "Epoch 8/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 571ms/step - loss: 84.3346 - mae: 4.5719 - mse: 84.3346 - val_loss: 173.5096 - val_mae: 6.8836 - val_mse: 173.5096\n",
      "Epoch 9/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 571ms/step - loss: 84.3346 - mae: 4.5719 - mse: 84.3346 - val_loss: 173.5096 - val_mae: 6.8836 - val_mse: 173.5096\n",
      "Epoch 9/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 84.9857 - mae: 4.5287 - mse: 84.9857 - val_loss: 171.3681 - val_mae: 6.6792 - val_mse: 171.3681\n",
      "Epoch 10/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 84.9857 - mae: 4.5287 - mse: 84.9857 - val_loss: 171.3681 - val_mae: 6.6792 - val_mse: 171.3681\n",
      "Epoch 10/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 570ms/step - loss: 74.3973 - mae: 4.1901 - mse: 74.3973 - val_loss: 169.7611 - val_mae: 6.7375 - val_mse: 169.7611\n",
      "Epoch 11/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 570ms/step - loss: 74.3973 - mae: 4.1901 - mse: 74.3973 - val_loss: 169.7611 - val_mae: 6.7375 - val_mse: 169.7611\n",
      "Epoch 11/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 573ms/step - loss: 69.5527 - mae: 3.9436 - mse: 69.5527 - val_loss: 172.0658 - val_mae: 6.6210 - val_mse: 172.0658\n",
      "Epoch 12/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 573ms/step - loss: 69.5527 - mae: 3.9436 - mse: 69.5527 - val_loss: 172.0658 - val_mae: 6.6210 - val_mse: 172.0658\n",
      "Epoch 12/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 69.0574 - mae: 3.9740 - mse: 69.0574 - val_loss: 168.6104 - val_mae: 6.4038 - val_mse: 168.6104\n",
      "Epoch 13/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 69.0574 - mae: 3.9740 - mse: 69.0574 - val_loss: 168.6104 - val_mae: 6.4038 - val_mse: 168.6104\n",
      "Epoch 13/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 62.3846 - mae: 3.7124 - mse: 62.3846 - val_loss: 184.2425 - val_mae: 6.9415 - val_mse: 184.2425\n",
      "Epoch 14/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 62.3846 - mae: 3.7124 - mse: 62.3846 - val_loss: 184.2425 - val_mae: 6.9415 - val_mse: 184.2425\n",
      "Epoch 14/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 563ms/step - loss: 61.6427 - mae: 3.5928 - mse: 61.6427 - val_loss: 168.0833 - val_mae: 6.7003 - val_mse: 168.0833\n",
      "Epoch 15/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 563ms/step - loss: 61.6427 - mae: 3.5928 - mse: 61.6427 - val_loss: 168.0833 - val_mae: 6.7003 - val_mse: 168.0833\n",
      "Epoch 15/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 54.9345 - mae: 3.4439 - mse: 54.9345 - val_loss: 180.2302 - val_mae: 6.4523 - val_mse: 180.2302\n",
      "Epoch 16/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 54.9345 - mae: 3.4439 - mse: 54.9345 - val_loss: 180.2302 - val_mae: 6.4523 - val_mse: 180.2302\n",
      "Epoch 16/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 56.6299 - mae: 3.5340 - mse: 56.6299 - val_loss: 182.2452 - val_mae: 6.3002 - val_mse: 182.2452\n",
      "Epoch 17/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 56.6299 - mae: 3.5340 - mse: 56.6299 - val_loss: 182.2452 - val_mae: 6.3002 - val_mse: 182.2452\n",
      "Epoch 17/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 59.6007 - mae: 3.6095 - mse: 59.6007 - val_loss: 167.9564 - val_mae: 6.3735 - val_mse: 167.9564\n",
      "Epoch 18/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 59.6007 - mae: 3.6095 - mse: 59.6007 - val_loss: 167.9564 - val_mae: 6.3735 - val_mse: 167.9564\n",
      "Epoch 18/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 57.2866 - mae: 3.5327 - mse: 57.2866 - val_loss: 177.5159 - val_mae: 6.4429 - val_mse: 177.5159\n",
      "Epoch 19/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 57.2866 - mae: 3.5327 - mse: 57.2866 - val_loss: 177.5159 - val_mae: 6.4429 - val_mse: 177.5159\n",
      "Epoch 19/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 46.9150 - mae: 3.1503 - mse: 46.9150 - val_loss: 168.2527 - val_mae: 6.2392 - val_mse: 168.2527\n",
      "Epoch 20/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 46.9150 - mae: 3.1503 - mse: 46.9150 - val_loss: 168.2527 - val_mae: 6.2392 - val_mse: 168.2527\n",
      "Epoch 20/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 560ms/step - loss: 39.3008 - mae: 2.7804 - mse: 39.3008 - val_loss: 167.4614 - val_mae: 6.1672 - val_mse: 167.4614\n",
      "Epoch 21/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 560ms/step - loss: 39.3008 - mae: 2.7804 - mse: 39.3008 - val_loss: 167.4614 - val_mae: 6.1672 - val_mse: 167.4614\n",
      "Epoch 21/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 38.3900 - mae: 2.7791 - mse: 38.3900 - val_loss: 166.2535 - val_mae: 6.1538 - val_mse: 166.2535\n",
      "Epoch 22/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 38.3900 - mae: 2.7791 - mse: 38.3900 - val_loss: 166.2535 - val_mae: 6.1538 - val_mse: 166.2535\n",
      "Epoch 22/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 34.4352 - mae: 2.5762 - mse: 34.4352 - val_loss: 169.5033 - val_mae: 6.2391 - val_mse: 169.5033\n",
      "Epoch 23/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 34.4352 - mae: 2.5762 - mse: 34.4352 - val_loss: 169.5033 - val_mae: 6.2391 - val_mse: 169.5033\n",
      "Epoch 23/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 568ms/step - loss: 41.1480 - mae: 2.8995 - mse: 41.1480 - val_loss: 170.9942 - val_mae: 6.2262 - val_mse: 170.9942\n",
      "Epoch 24/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 568ms/step - loss: 41.1480 - mae: 2.8995 - mse: 41.1480 - val_loss: 170.9942 - val_mae: 6.2262 - val_mse: 170.9942\n",
      "Epoch 24/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 36.9797 - mae: 2.6597 - mse: 36.9797 - val_loss: 175.0755 - val_mae: 6.5812 - val_mse: 175.0755\n",
      "Epoch 25/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 36.9797 - mae: 2.6597 - mse: 36.9797 - val_loss: 175.0755 - val_mae: 6.5812 - val_mse: 175.0755\n",
      "Epoch 25/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 33.2227 - mae: 2.6087 - mse: 33.2227 - val_loss: 172.4585 - val_mae: 6.3783 - val_mse: 172.4585\n",
      "Epoch 26/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 33.2227 - mae: 2.6087 - mse: 33.2227 - val_loss: 172.4585 - val_mae: 6.3783 - val_mse: 172.4585\n",
      "Epoch 26/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 26.9011 - mae: 2.2116 - mse: 26.9011 - val_loss: 167.7881 - val_mae: 6.4443 - val_mse: 167.7881\n",
      "Epoch 27/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 26.9011 - mae: 2.2116 - mse: 26.9011 - val_loss: 167.7881 - val_mae: 6.4443 - val_mse: 167.7881\n",
      "Epoch 27/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 563ms/step - loss: 25.2127 - mae: 2.1677 - mse: 25.2127 - val_loss: 170.1526 - val_mae: 6.2623 - val_mse: 170.1526\n",
      "Epoch 28/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 563ms/step - loss: 25.2127 - mae: 2.1677 - mse: 25.2127 - val_loss: 170.1526 - val_mae: 6.2623 - val_mse: 170.1526\n",
      "Epoch 28/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 562ms/step - loss: 27.5211 - mae: 2.2033 - mse: 27.5211 - val_loss: 174.4436 - val_mae: 6.4126 - val_mse: 174.4436\n",
      "Epoch 29/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 562ms/step - loss: 27.5211 - mae: 2.2033 - mse: 27.5211 - val_loss: 174.4436 - val_mae: 6.4126 - val_mse: 174.4436\n",
      "Epoch 29/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 24.5759 - mae: 2.0831 - mse: 24.5759 - val_loss: 174.1720 - val_mae: 6.3919 - val_mse: 174.1720\n",
      "Epoch 30/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 24.5759 - mae: 2.0831 - mse: 24.5759 - val_loss: 174.1720 - val_mae: 6.3919 - val_mse: 174.1720\n",
      "Epoch 30/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.4033 - mae: 1.9390 - mse: 22.4033 - val_loss: 180.8772 - val_mae: 6.4000 - val_mse: 180.8772\n",
      "Epoch 31/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.4033 - mae: 1.9390 - mse: 22.4033 - val_loss: 180.8772 - val_mae: 6.4000 - val_mse: 180.8772\n",
      "Epoch 31/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.9944 - mae: 2.0304 - mse: 22.9944 - val_loss: 182.1433 - val_mae: 6.5324 - val_mse: 182.1433\n",
      "Epoch 32/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.9944 - mae: 2.0304 - mse: 22.9944 - val_loss: 182.1433 - val_mae: 6.5324 - val_mse: 182.1433\n",
      "Epoch 32/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 27.3672 - mae: 2.2253 - mse: 27.3672 - val_loss: 175.2163 - val_mae: 6.3423 - val_mse: 175.2163\n",
      "Epoch 33/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 27.3672 - mae: 2.2253 - mse: 27.3672 - val_loss: 175.2163 - val_mae: 6.3423 - val_mse: 175.2163\n",
      "Epoch 33/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 25.7272 - mae: 2.2823 - mse: 25.7272 - val_loss: 171.1455 - val_mae: 6.2399 - val_mse: 171.1455\n",
      "Epoch 34/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 25.7272 - mae: 2.2823 - mse: 25.7272 - val_loss: 171.1455 - val_mae: 6.2399 - val_mse: 171.1455\n",
      "Epoch 34/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.0461 - mae: 2.1129 - mse: 22.0461 - val_loss: 169.5595 - val_mae: 6.2709 - val_mse: 169.5595\n",
      "Epoch 35/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 566ms/step - loss: 22.0461 - mae: 2.1129 - mse: 22.0461 - val_loss: 169.5595 - val_mae: 6.2709 - val_mse: 169.5595\n",
      "Epoch 35/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 19.1839 - mae: 1.8413 - mse: 19.1839 - val_loss: 174.8255 - val_mae: 6.3256 - val_mse: 174.8255\n",
      "Epoch 36/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 19.1839 - mae: 1.8413 - mse: 19.1839 - val_loss: 174.8255 - val_mae: 6.3256 - val_mse: 174.8255\n",
      "Epoch 36/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 16.7569 - mae: 1.6864 - mse: 16.7569 - val_loss: 171.7759 - val_mae: 6.1820 - val_mse: 171.7759\n",
      "Epoch 37/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 567ms/step - loss: 16.7569 - mae: 1.6864 - mse: 16.7569 - val_loss: 171.7759 - val_mae: 6.1820 - val_mse: 171.7759\n",
      "Epoch 37/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 562ms/step - loss: 14.1790 - mae: 1.4417 - mse: 14.1790 - val_loss: 173.5490 - val_mae: 6.2385 - val_mse: 173.5490\n",
      "Epoch 38/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 562ms/step - loss: 14.1790 - mae: 1.4417 - mse: 14.1790 - val_loss: 173.5490 - val_mae: 6.2385 - val_mse: 173.5490\n",
      "Epoch 38/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 14.3649 - mae: 1.4750 - mse: 14.3649 - val_loss: 169.9578 - val_mae: 6.2635 - val_mse: 169.9578\n",
      "Epoch 39/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 565ms/step - loss: 14.3649 - mae: 1.4750 - mse: 14.3649 - val_loss: 169.9578 - val_mae: 6.2635 - val_mse: 169.9578\n",
      "Epoch 39/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 569ms/step - loss: 15.9465 - mae: 1.6271 - mse: 15.9465 - val_loss: 173.2628 - val_mae: 6.2819 - val_mse: 173.2628\n",
      "Epoch 40/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 569ms/step - loss: 15.9465 - mae: 1.6271 - mse: 15.9465 - val_loss: 173.2628 - val_mae: 6.2819 - val_mse: 173.2628\n",
      "Epoch 40/40\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 13.2053 - mae: 1.4505 - mse: 13.2053 - val_loss: 172.4449 - val_mae: 6.2851 - val_mse: 172.4449\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 564ms/step - loss: 13.2053 - mae: 1.4505 - mse: 13.2053 - val_loss: 172.4449 - val_mae: 6.2851 - val_mse: 172.4449\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 451us/step\n",
      "\u001b[1m141750/141750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 451us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Model Evaluation:\n",
      "  MAE: 6.2850\n",
      "  RMSE: 13.1318\n",
      "  R²: 0.3983\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate improved model\n",
    "# Fix: Reduce sample size further to avoid MemoryError\n",
    "max_samples = 5000  # Lower this if you still get MemoryError\n",
    "X_train_fe_small = X_train_fe[:max_samples]\n",
    "y_train_small = y_train[:max_samples]\n",
    "# Optional: Use PCA to reduce feature dimensionality if needed\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)  # or set to retain 95% variance\n",
    "# X_train_fe_small = pca.fit_transform(X_train_fe_small)\n",
    "# X_test_fe_pca = pca.transform(X_test_fe)\n",
    "advanced_model = bayes_tuner.hypermodel.build(best_bayes_hps)\n",
    "history_adv = advanced_model.fit(\n",
    "    X_train_fe_small, y_train_small,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_adv = advanced_model.predict(X_test_fe).flatten()\n",
    "mae_adv = mean_absolute_error(y_test, y_pred_adv)\n",
    "rmse_adv = np.sqrt(mean_squared_error(y_test, y_pred_adv))\n",
    "r2_adv = r2_score(y_test, y_pred_adv)\n",
    "\n",
    "print(f\"\\nImproved Model Evaluation:\\n  MAE: {mae_adv:.4f}\\n  RMSE: {rmse_adv:.4f}\\n  R²: {r2_adv:.4f}\")\n",
    "advanced_model.save(os.path.join(MODEL_DIR, \"mlp_large_advanced_tuned.h5\"))\n",
    "plot_training(history_adv, \"Large MLP (Advanced)\", \"training_large_advanced_tuned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e440d5",
   "metadata": {},
   "source": [
    "## Advanced Model Improvements\n",
    "Let's try more advanced deep learning techniques: residual connections, dropout, batch normalization, and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156d30dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 23.7 GiB for an array with shape (18144000, 350) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m res_mlp = build_residual_mlp(X_train_fe.shape[\u001b[32m1\u001b[39m], n_layers=\u001b[32m3\u001b[39m, units=\u001b[32m256\u001b[39m, dropout_rate=\u001b[32m0.3\u001b[39m, l2_reg=\u001b[32m1e-3\u001b[39m)\n\u001b[32m     19\u001b[39m lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m, min_lr=\u001b[32m1e-5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m history_res = \u001b[43mres_mlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_fe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_fe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     31\u001b[39m res_pred = res_mlp.predict(X_test_fe).flatten()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aisha\\OneDrive\\Desktop\\GitHub\\neural-pricer\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aisha\\OneDrive\\Desktop\\GitHub\\neural-pricer\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:96\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np.ndarray):\n\u001b[32m     93\u001b[39m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[32m     94\u001b[39m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[32m     95\u001b[39m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m   value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops.EagerTensor):\n\u001b[32m     98\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value.dtype != dtype:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 23.7 GiB for an array with shape (18144000, 350) and data type float32"
     ]
    }
   ],
   "source": [
    "# Residual MLP with Dropout and BatchNorm\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def build_residual_mlp(input_dim, n_layers=4, units=128, dropout_rate=0.2, l2_reg=1e-4):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    for i in range(n_layers):\n",
    "        shortcut = x\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([x, shortcut])  # Residual connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "res_mlp = build_residual_mlp(X_train_fe.shape[1], n_layers=3, units=256, dropout_rate=0.3, l2_reg=1e-3)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history_res = res_mlp.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "res_pred = res_mlp.predict(X_test_fe).flatten()\n",
    "mae_res = mean_absolute_error(y_test, res_pred)\n",
    "rmse_res = np.sqrt(mean_squared_error(y_test, res_pred))\n",
    "r2_res = r2_score(y_test, res_pred)\n",
    "\n",
    "print(f\"\\nResidual MLP Evaluation:\\n  MAE: {mae_res:.4f}\\n  RMSE: {rmse_res:.4f}\\n  R²: {r2_res:.4f}\")\n",
    "res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_residual_tuned.h5\"))\n",
    "plot_training(history_res, \"Residual MLP\", \"training_residual_mlp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86780e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Model Evaluation:\n",
      "  MAE: 5.9232\n",
      "  RMSE: 9.6243\n",
      "  R²: 0.4447\n"
     ]
    }
   ],
   "source": [
    "# Ensemble: Average predictions from best models\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predict with advanced and residual models\n",
    "ensemble_preds = (y_pred_adv + res_pred) / 2\n",
    "mae_ensemble = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse_ensemble = np.sqrt(mean_squared_error(y_test, ensemble_preds))\n",
    "r2_ensemble = r2_score(y_test, ensemble_preds)\n",
    "\n",
    "print(f\"\\nEnsemble Model Evaluation:\\n  MAE: {mae_ensemble:.4f}\\n  RMSE: {rmse_ensemble:.4f}\\n  R²: {r2_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944f8f2",
   "metadata": {},
   "source": [
    "## Next-Level Model Improvements\n",
    "We'll now try several advanced deep learning and ML techniques to further boost performance:\n",
    "- Deeper residual networks with multi-level skip connections\n",
    "- More regularization: Dropout, L1/L2, Gaussian noise\n",
    "- Advanced optimizers: AdamW, Nadam, Lookahead\n",
    "- Learning rate warmup and cyclical schedules\n",
    "- Feature selection/dimensionality reduction (PCA)\n",
    "- Ensemble stacking (combine multiple models)\n",
    "- Data augmentation (if feasible for tabular data)\n",
    "- Model uncertainty estimation (MC Dropout, quantile regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e8f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 99.8228 - mae: 6.0948 - mse: 95.7783 - val_loss: 95.8885 - val_mae: 5.9922 - val_mse: 92.3501 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 94.7495 - mae: 5.9057 - mse: 92.1526 - val_loss: 94.2381 - val_mae: 5.9298 - val_mse: 92.2835 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 93.3804 - mae: 5.8713 - mse: 91.7001 - val_loss: 93.5348 - val_mae: 5.8081 - val_mse: 92.0518 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 92.7688 - mae: 5.8654 - mse: 91.4322 - val_loss: 93.1828 - val_mae: 5.8160 - val_mse: 91.8870 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 92.4510 - mae: 5.8504 - mse: 91.2488 - val_loss: 93.3022 - val_mae: 5.9720 - val_mse: 92.2163 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 92.2102 - mae: 5.8473 - mse: 91.1684 - val_loss: 92.6732 - val_mae: 5.9574 - val_mse: 91.6410 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.9738 - mae: 5.8437 - mse: 91.0684 - val_loss: 92.6029 - val_mae: 5.8323 - val_mse: 91.7949 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.7903 - mae: 5.8422 - mse: 91.0468 - val_loss: 92.3787 - val_mae: 5.8954 - val_mse: 91.5691 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.6538 - mae: 5.8403 - mse: 90.9701 - val_loss: 92.2698 - val_mae: 5.8432 - val_mse: 91.6619 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.4867 - mae: 5.8374 - mse: 90.9115 - val_loss: 91.9189 - val_mae: 5.8768 - val_mse: 91.4147 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 91.3705 - mae: 5.8332 - mse: 90.8123 - val_loss: 91.9394 - val_mae: 5.8184 - val_mse: 91.4568 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.2590 - mae: 5.8318 - mse: 90.7734 - val_loss: 92.0443 - val_mae: 5.8769 - val_mse: 91.5817 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.2243 - mae: 5.8299 - mse: 90.7779 - val_loss: 91.9199 - val_mae: 5.8142 - val_mse: 91.4795 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 91.0932 - mae: 5.8291 - mse: 90.6608 - val_loss: 92.3505 - val_mae: 5.8537 - val_mse: 91.9236 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0771 - mae: 5.8305 - mse: 90.6405 - val_loss: 91.7918 - val_mae: 5.8488 - val_mse: 91.3731 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0469 - mae: 5.8283 - mse: 90.6400 - val_loss: 91.9302 - val_mae: 5.8494 - val_mse: 91.5212 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 91.0550 - mae: 5.8286 - mse: 90.6329 - val_loss: 92.6815 - val_mae: 5.8003 - val_mse: 92.2657 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.9794 - mae: 5.8255 - mse: 90.5882 - val_loss: 92.0936 - val_mae: 5.8497 - val_mse: 91.6808 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.9652 - mae: 5.8258 - mse: 90.5800 - val_loss: 91.8916 - val_mae: 5.8489 - val_mse: 91.5127 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.9206 - mae: 5.8250 - mse: 90.5436 - val_loss: 91.8778 - val_mae: 5.8699 - val_mse: 91.5219 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.5216 - mae: 5.8073 - mse: 90.1888 - val_loss: 91.7146 - val_mae: 5.8184 - val_mse: 91.3898 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.4718 - mae: 5.8043 - mse: 90.1488 - val_loss: 91.7472 - val_mae: 5.8467 - val_mse: 91.4325 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.4629 - mae: 5.8047 - mse: 90.1487 - val_loss: 91.6612 - val_mae: 5.8342 - val_mse: 91.3434 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.4034 - mae: 5.8032 - mse: 90.0940 - val_loss: 92.0491 - val_mae: 5.8698 - val_mse: 91.7452 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3867 - mae: 5.8033 - mse: 90.0812 - val_loss: 91.7574 - val_mae: 5.8759 - val_mse: 91.4517 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3542 - mae: 5.8014 - mse: 90.0484 - val_loss: 91.8765 - val_mae: 5.8342 - val_mse: 91.5735 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.3272 - mae: 5.8032 - mse: 90.0264 - val_loss: 91.7452 - val_mae: 5.8469 - val_mse: 91.4528 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 90.3457 - mae: 5.8009 - mse: 90.0519 - val_loss: 92.0543 - val_mae: 5.8166 - val_mse: 91.7579 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0788 - mae: 5.7910 - mse: 89.7942 - val_loss: 91.6858 - val_mae: 5.8530 - val_mse: 91.4043 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0472 - mae: 5.7909 - mse: 89.7666 - val_loss: 91.6735 - val_mae: 5.8186 - val_mse: 91.3946 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0227 - mae: 5.7900 - mse: 89.7430 - val_loss: 91.7160 - val_mae: 5.8241 - val_mse: 91.4404 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0209 - mae: 5.7884 - mse: 89.7454 - val_loss: 91.6743 - val_mae: 5.8582 - val_mse: 91.3991 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 90.0281 - mae: 5.7909 - mse: 89.7550 - val_loss: 91.7191 - val_mae: 5.8431 - val_mse: 91.4483 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8660 - mae: 5.7825 - mse: 89.5978 - val_loss: 91.6468 - val_mae: 5.8159 - val_mse: 91.3794 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8371 - mae: 5.7804 - mse: 89.5712 - val_loss: 91.7240 - val_mae: 5.8037 - val_mse: 91.4591 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8425 - mae: 5.7804 - mse: 89.5782 - val_loss: 91.6936 - val_mae: 5.8360 - val_mse: 91.4287 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8249 - mae: 5.7804 - mse: 89.5614 - val_loss: 91.6772 - val_mae: 5.8195 - val_mse: 91.4144 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.8216 - mae: 5.7810 - mse: 89.5588 - val_loss: 91.6856 - val_mae: 5.8170 - val_mse: 91.4242 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.8219 - mae: 5.7817 - mse: 89.5605 - val_loss: 91.6556 - val_mae: 5.8246 - val_mse: 91.3946 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7228 - mae: 5.7761 - mse: 89.4631 - val_loss: 91.6753 - val_mae: 5.8217 - val_mse: 91.4170 - learning_rate: 6.2500e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7001 - mae: 5.7765 - mse: 89.4419 - val_loss: 91.6795 - val_mae: 5.8244 - val_mse: 91.4217 - learning_rate: 6.2500e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7079 - mae: 5.7769 - mse: 89.4500 - val_loss: 91.6992 - val_mae: 5.8156 - val_mse: 91.4416 - learning_rate: 6.2500e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.7070 - mae: 5.7765 - mse: 89.4493 - val_loss: 91.7197 - val_mae: 5.8024 - val_mse: 91.4626 - learning_rate: 6.2500e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6715 - mae: 5.7757 - mse: 89.4144 - val_loss: 91.6938 - val_mae: 5.8337 - val_mse: 91.4371 - learning_rate: 6.2500e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.6555 - mae: 5.7786 - mse: 89.4000 - val_loss: 91.7080 - val_mae: 5.8045 - val_mse: 91.4532 - learning_rate: 3.1250e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.6257 - mae: 5.7741 - mse: 89.3705 - val_loss: 91.6895 - val_mae: 5.8232 - val_mse: 91.4347 - learning_rate: 3.1250e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6201 - mae: 5.7728 - mse: 89.3657 - val_loss: 91.7196 - val_mae: 5.8160 - val_mse: 91.4655 - learning_rate: 3.1250e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 89.6351 - mae: 5.7756 - mse: 89.3811 - val_loss: 91.6984 - val_mae: 5.8398 - val_mse: 91.4444 - learning_rate: 3.1250e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.5880 - mae: 5.7716 - mse: 89.3343 - val_loss: 91.7016 - val_mae: 5.8337 - val_mse: 91.4479 - learning_rate: 3.1250e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 89.5943 - mae: 5.7743 - mse: 89.3414 - val_loss: 91.7121 - val_mae: 5.8221 - val_mse: 91.4594 - learning_rate: 1.5625e-05\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 720us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deeper Residual MLP Evaluation:\n",
      "  MAE: 5.8221\n",
      "  RMSE: 9.5634\n",
      "  R²: 0.4517\n"
     ]
    }
   ],
   "source": [
    "# Deeper Residual Network with Multi-Level Skip Connections and More Regularization\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers import AdamW, Nadam\n",
    "\n",
    "def build_deep_residual_mlp(input_dim, n_layers=6, units=128, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-4, noise_std=0.05):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = GaussianNoise(noise_std)(inputs)\n",
    "    x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "    skip = x\n",
    "    for i in range(n_layers):\n",
    "        x = layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        if i % 2 == 1:\n",
    "            x = layers.Add()([x, skip])  # Multi-level skip connection\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=AdamW(), loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Train deeper residual model\n",
    "advanced_res_mlp = build_deep_residual_mlp(X_train_fe.shape[1], n_layers=6, units=256, dropout_rate=0.3, l1_reg=1e-4, l2_reg=1e-3, noise_std=0.05)\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "history_adv_res = advanced_res_mlp.fit(\n",
    "    X_train_fe, y_train,\n",
    "    validation_data=(X_test_fe, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "adv_res_pred = advanced_res_mlp.predict(X_test_fe).flatten()\n",
    "mae_adv_res = mean_absolute_error(y_test, adv_res_pred)\n",
    "rmse_adv_res = np.sqrt(mean_squared_error(y_test, adv_res_pred))\n",
    "r2_adv_res = r2_score(y_test, adv_res_pred)\n",
    "\n",
    "print(f\"\\nDeeper Residual MLP Evaluation:\\n  MAE: {mae_adv_res:.4f}\\n  RMSE: {rmse_adv_res:.4f}\\n  R²: {r2_adv_res:.4f}\")\n",
    "advanced_res_mlp.save(os.path.join(MODEL_DIR, \"mlp_large_deep_residual_tuned.h5\"))\n",
    "plot_training(history_adv_res, \"Deep Residual MLP\", \"training_deep_residual_mlp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886424dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Learning Rate Callback\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=1e-4, max_lr=1e-2, step_size=2000):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.iterations = 0\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        cycle = np.floor(1 + self.iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.iterations / self.step_size - 2 * cycle + 1)\n",
    "        lr = self.base_lr + (self.max_lr - self.base_lr) * max(0, (1 - x))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        self.iterations += 1\n",
    "\n",
    "# Example usage:\n",
    "# clr = CyclicLR(base_lr=1e-4, max_lr=1e-2, step_size=1000)\n",
    "# history = model.fit(..., callbacks=[clr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reduced shape: X_train_pca (360000, 38), X_test_pca (90000, 38)\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection: Principal Component Analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, svd_solver='full')  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_fe)\n",
    "X_test_pca = pca.transform(X_test_fe)\n",
    "\n",
    "print(f\"PCA reduced shape: X_train_pca {X_train_pca.shape}, X_test_pca {X_test_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055af79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked Ensemble Evaluation:\n",
      "  MAE: 5.8465\n",
      "  RMSE: 9.5624\n",
      "  R²: 0.4518\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Stacking: Combine Multiple Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume you have predictions from several models\n",
    "# y_pred_adv, res_pred, adv_res_pred (from previous cells)\n",
    "\n",
    "stacked_preds = np.vstack([\n",
    "    y_pred_adv,\n",
    "    res_pred,\n",
    "    adv_res_pred\n",
    "]).T\n",
    "\n",
    "stacker = LinearRegression()\n",
    "stacker.fit(stacked_preds, y_test)\n",
    "ensemble_stacked = stacker.predict(stacked_preds)\n",
    "\n",
    "mae_stacked = mean_absolute_error(y_test, ensemble_stacked)\n",
    "rmse_stacked = np.sqrt(mean_squared_error(y_test, ensemble_stacked))\n",
    "r2_stacked = r2_score(y_test, ensemble_stacked)\n",
    "\n",
    "print(f\"\\nStacked Ensemble Evaluation:\\n  MAE: {mae_stacked:.4f}\\n  RMSE: {rmse_stacked:.4f}\\n  R²: {r2_stacked:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf889bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce8ebe60",
   "metadata": {},
   "source": [
    "### Advanced Deep Learning Improvements\n",
    "We'll now apply:\n",
    "- MC Dropout for uncertainty estimation\n",
    "- Quantile regression for predictive intervals\n",
    "- Mixup data augmentation for tabular data\n",
    "- Learning rate warmup/scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc196ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Dropout mean prediction: [0.06772503 0.17885922 7.824484   2.438419   0.09934095]\n",
      "MC Dropout std (uncertainty): [0.22354934 0.2279192  0.34892038 0.3731246  0.2561742 ]\n"
     ]
    }
   ],
   "source": [
    "# MC Dropout for Uncertainty Estimation\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def mc_dropout_predict(model, X, n_iter=100):\n",
    "    # Enable dropout at inference\n",
    "    f = Model(model.input, model.output)\n",
    "    preds = [f(X, training=True).numpy().flatten() for _ in range(n_iter)]\n",
    "    preds = np.array(preds)\n",
    "    mean_pred = preds.mean(axis=0)\n",
    "    std_pred = preds.std(axis=0)\n",
    "    return mean_pred, std_pred\n",
    "\n",
    "# Example usage:\n",
    "mean_pred, std_pred = mc_dropout_predict(advanced_res_mlp, X_test_fe, n_iter=100)\n",
    "print(f\"MC Dropout mean prediction: {mean_pred[:5]}\")\n",
    "print(f\"MC Dropout std (uncertainty): {std_pred[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 395us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 391us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step\n",
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step\n",
      "Quantile interval example: low=[1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06], median=[-1.5293405e-04 -1.5293405e-04  4.4251113e+00 -1.5293405e-04\n",
      " -1.5293405e-04], high=[1.2684676e-03 1.2684676e-03 1.9968006e+01 9.7142715e+00 1.2684676e-03]\n",
      "Quantile interval example: low=[1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06 1.869812e-06], median=[-1.5293405e-04 -1.5293405e-04  4.4251113e+00 -1.5293405e-04\n",
      " -1.5293405e-04], high=[1.2684676e-03 1.2684676e-03 1.9968006e+01 9.7142715e+00 1.2684676e-03]\n"
     ]
    }
   ],
   "source": [
    "# Quantile Regression for Predictive Intervals\n",
    "# Use only tensorflow.keras for compatibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "\n",
    "# Quantile loss function for Keras\n",
    "# K.maximum and K.mean are from tensorflow.keras.backend\n",
    "\n",
    "def quantile_loss(q):\n",
    "    def loss(y_true, y_pred):\n",
    "        e = y_true - y_pred\n",
    "        return K.mean(K.maximum(q * e, (q - 1) * e), axis=-1)\n",
    "    return loss\n",
    "\n",
    "# Build quantile model (e.g., 0.1, 0.5, 0.9)\n",
    "def build_quantile_mlp(input_dim, quantile):\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss=quantile_loss(quantile))\n",
    "    return model\n",
    "\n",
    "# Train quantile models\n",
    "q_low, q_med, q_high = 0.1, 0.5, 0.9\n",
    "model_low = build_quantile_mlp(X_train_fe.shape[1], q_low)\n",
    "model_med = build_quantile_mlp(X_train_fe.shape[1], q_med)\n",
    "model_high = build_quantile_mlp(X_train_fe.shape[1], q_high)\n",
    "\n",
    "model_low.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_med.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "model_high.fit(X_train_fe, y_train, epochs=20, batch_size=64, verbose=0)\n",
    "\n",
    "# Predict intervals\n",
    "pred_low = model_low.predict(X_test_fe).flatten()\n",
    "pred_med = model_med.predict(X_test_fe).flatten()\n",
    "pred_high = model_high.predict(X_test_fe).flatten()\n",
    "\n",
    "print(f\"Quantile interval example: low={pred_low[:5]}, median={pred_med[:5]}, high={pred_high[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixup sample shapes: (360000, 135), (360000,)\n",
      "Epoch 1/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 85.3159 - mae: 5.7766 - mse: 82.7681 - val_loss: 95.4293 - val_mae: 6.0405 - val_mse: 93.0159\n",
      "Epoch 2/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - loss: 85.3159 - mae: 5.7766 - mse: 82.7681 - val_loss: 95.4293 - val_mae: 6.0405 - val_mse: 93.0159\n",
      "Epoch 2/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 81.4110 - mae: 5.5964 - mse: 79.4167 - val_loss: 97.4179 - val_mae: 6.1284 - val_mse: 95.8057\n",
      "Epoch 3/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 81.4110 - mae: 5.5964 - mse: 79.4167 - val_loss: 97.4179 - val_mae: 6.1284 - val_mse: 95.8057\n",
      "Epoch 3/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 80.3429 - mae: 5.5628 - mse: 78.9570 - val_loss: 94.6987 - val_mae: 6.0738 - val_mse: 93.4098\n",
      "Epoch 4/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 80.3429 - mae: 5.5628 - mse: 78.9570 - val_loss: 94.6987 - val_mae: 6.0738 - val_mse: 93.4098\n",
      "Epoch 4/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.8443 - mae: 5.5474 - mse: 78.6696 - val_loss: 92.9281 - val_mae: 5.9283 - val_mse: 91.8011\n",
      "Epoch 5/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.8443 - mae: 5.5474 - mse: 78.6696 - val_loss: 92.9281 - val_mae: 5.9283 - val_mse: 91.8011\n",
      "Epoch 5/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.5674 - mae: 5.5401 - mse: 78.5042 - val_loss: 92.8102 - val_mae: 5.8325 - val_mse: 91.7410\n",
      "Epoch 6/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 79.5674 - mae: 5.5401 - mse: 78.5042 - val_loss: 92.8102 - val_mae: 5.8325 - val_mse: 91.7410\n",
      "Epoch 6/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.2646 - mae: 5.5338 - mse: 78.3859 - val_loss: 92.4000 - val_mae: 5.8472 - val_mse: 91.5401\n",
      "Epoch 7/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.2646 - mae: 5.5338 - mse: 78.3859 - val_loss: 92.4000 - val_mae: 5.8472 - val_mse: 91.5401\n",
      "Epoch 7/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.0615 - mae: 5.5290 - mse: 78.2568 - val_loss: 92.4259 - val_mae: 5.8737 - val_mse: 91.6549\n",
      "Epoch 8/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 79.0615 - mae: 5.5290 - mse: 78.2568 - val_loss: 92.4259 - val_mae: 5.8737 - val_mse: 91.6549\n",
      "Epoch 8/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.8362 - mae: 5.5277 - mse: 78.1400 - val_loss: 92.6366 - val_mae: 5.8481 - val_mse: 91.9183\n",
      "Epoch 9/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.8362 - mae: 5.5277 - mse: 78.1400 - val_loss: 92.6366 - val_mae: 5.8481 - val_mse: 91.9183\n",
      "Epoch 9/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.7466 - mae: 5.5276 - mse: 78.0876 - val_loss: 92.6769 - val_mae: 5.9848 - val_mse: 92.0112\n",
      "Epoch 10/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.7466 - mae: 5.5276 - mse: 78.0876 - val_loss: 92.6769 - val_mae: 5.9848 - val_mse: 92.0112\n",
      "Epoch 10/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.5818 - mae: 5.5225 - mse: 77.9399 - val_loss: 92.3640 - val_mae: 5.9420 - val_mse: 91.7404\n",
      "Epoch 11/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.5818 - mae: 5.5225 - mse: 77.9399 - val_loss: 92.3640 - val_mae: 5.9420 - val_mse: 91.7404\n",
      "Epoch 11/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.4267 - mae: 5.5189 - mse: 77.8541 - val_loss: 92.7150 - val_mae: 5.9120 - val_mse: 92.2120\n",
      "Epoch 12/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 78.4267 - mae: 5.5189 - mse: 77.8541 - val_loss: 92.7150 - val_mae: 5.9120 - val_mse: 92.2120\n",
      "Epoch 12/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.2213 - mae: 5.5132 - mse: 77.7259 - val_loss: 92.6047 - val_mae: 5.8272 - val_mse: 92.1385\n",
      "Epoch 13/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.2213 - mae: 5.5132 - mse: 77.7259 - val_loss: 92.6047 - val_mae: 5.8272 - val_mse: 92.1385\n",
      "Epoch 13/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.1383 - mae: 5.5126 - mse: 77.6697 - val_loss: 92.6841 - val_mae: 5.8005 - val_mse: 92.2377\n",
      "Epoch 14/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 78.1383 - mae: 5.5126 - mse: 77.6697 - val_loss: 92.6841 - val_mae: 5.8005 - val_mse: 92.2377\n",
      "Epoch 14/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.9559 - mae: 5.5095 - mse: 77.4966 - val_loss: 92.5615 - val_mae: 5.8853 - val_mse: 92.0937\n",
      "Epoch 15/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.9559 - mae: 5.5095 - mse: 77.4966 - val_loss: 92.5615 - val_mae: 5.8853 - val_mse: 92.0937\n",
      "Epoch 15/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.8510 - mae: 5.5076 - mse: 77.4042 - val_loss: 92.5182 - val_mae: 5.8942 - val_mse: 92.0935\n",
      "Epoch 16/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.8510 - mae: 5.5076 - mse: 77.4042 - val_loss: 92.5182 - val_mae: 5.8942 - val_mse: 92.0935\n",
      "Epoch 16/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.7806 - mae: 5.5064 - mse: 77.3441 - val_loss: 92.4383 - val_mae: 5.9071 - val_mse: 91.9713\n",
      "Epoch 17/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.7806 - mae: 5.5064 - mse: 77.3441 - val_loss: 92.4383 - val_mae: 5.9071 - val_mse: 91.9713\n",
      "Epoch 17/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6870 - mae: 5.5052 - mse: 77.2584 - val_loss: 92.5702 - val_mae: 5.8549 - val_mse: 92.1644\n",
      "Epoch 18/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6870 - mae: 5.5052 - mse: 77.2584 - val_loss: 92.5702 - val_mae: 5.8549 - val_mse: 92.1644\n",
      "Epoch 18/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6162 - mae: 5.5032 - mse: 77.1851 - val_loss: 92.8932 - val_mae: 5.9707 - val_mse: 92.4700\n",
      "Epoch 19/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.6162 - mae: 5.5032 - mse: 77.1851 - val_loss: 92.8932 - val_mae: 5.9707 - val_mse: 92.4700\n",
      "Epoch 19/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.4621 - mae: 5.4967 - mse: 77.0256 - val_loss: 92.9643 - val_mae: 5.9739 - val_mse: 92.5410\n",
      "Epoch 20/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.4621 - mae: 5.4967 - mse: 77.0256 - val_loss: 92.9643 - val_mae: 5.9739 - val_mse: 92.5410\n",
      "Epoch 20/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.4020 - mae: 5.4954 - mse: 76.9423 - val_loss: 92.9174 - val_mae: 5.9209 - val_mse: 92.4612\n",
      "Epoch 21/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.4020 - mae: 5.4954 - mse: 76.9423 - val_loss: 92.9174 - val_mae: 5.9209 - val_mse: 92.4612\n",
      "Epoch 21/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.3191 - mae: 5.4932 - mse: 76.8789 - val_loss: 93.3338 - val_mae: 5.8462 - val_mse: 92.9052\n",
      "Epoch 22/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.3191 - mae: 5.4932 - mse: 76.8789 - val_loss: 93.3338 - val_mae: 5.8462 - val_mse: 92.9052\n",
      "Epoch 22/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.2090 - mae: 5.4923 - mse: 76.7815 - val_loss: 92.9022 - val_mae: 6.0293 - val_mse: 92.4709\n",
      "Epoch 23/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.2090 - mae: 5.4923 - mse: 76.7815 - val_loss: 92.9022 - val_mae: 6.0293 - val_mse: 92.4709\n",
      "Epoch 23/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.1022 - mae: 5.4887 - mse: 76.6751 - val_loss: 92.9234 - val_mae: 5.8257 - val_mse: 92.5149\n",
      "Epoch 24/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.1022 - mae: 5.4887 - mse: 76.6751 - val_loss: 92.9234 - val_mae: 5.8257 - val_mse: 92.5149\n",
      "Epoch 24/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.0270 - mae: 5.4900 - mse: 76.6172 - val_loss: 92.9699 - val_mae: 5.8482 - val_mse: 92.5620\n",
      "Epoch 25/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 77.0270 - mae: 5.4900 - mse: 76.6172 - val_loss: 92.9699 - val_mae: 5.8482 - val_mse: 92.5620\n",
      "Epoch 25/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.0064 - mae: 5.4867 - mse: 76.5931 - val_loss: 93.3832 - val_mae: 5.8459 - val_mse: 92.9711\n",
      "Epoch 26/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 77.0064 - mae: 5.4867 - mse: 76.5931 - val_loss: 93.3832 - val_mae: 5.8459 - val_mse: 92.9711\n",
      "Epoch 26/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 76.9397 - mae: 5.4835 - mse: 76.5212 - val_loss: 93.3414 - val_mae: 5.9000 - val_mse: 92.9221\n",
      "Epoch 27/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 76.9397 - mae: 5.4835 - mse: 76.5212 - val_loss: 93.3414 - val_mae: 5.9000 - val_mse: 92.9221\n",
      "Epoch 27/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.8812 - mae: 5.4833 - mse: 76.4612 - val_loss: 93.3887 - val_mae: 5.9511 - val_mse: 92.9733\n",
      "Epoch 28/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.8812 - mae: 5.4833 - mse: 76.4612 - val_loss: 93.3887 - val_mae: 5.9511 - val_mse: 92.9733\n",
      "Epoch 28/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7457 - mae: 5.4796 - mse: 76.3395 - val_loss: 93.1661 - val_mae: 5.9000 - val_mse: 92.7570\n",
      "Epoch 29/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7457 - mae: 5.4796 - mse: 76.3395 - val_loss: 93.1661 - val_mae: 5.9000 - val_mse: 92.7570\n",
      "Epoch 29/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7468 - mae: 5.4783 - mse: 76.3430 - val_loss: 93.2295 - val_mae: 5.9006 - val_mse: 92.8319\n",
      "Epoch 30/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 76.7468 - mae: 5.4783 - mse: 76.3430 - val_loss: 93.2295 - val_mae: 5.9006 - val_mse: 92.8319\n",
      "Epoch 30/30\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 76.7161 - mae: 5.4782 - mse: 76.3096 - val_loss: 93.8141 - val_mae: 5.9384 - val_mse: 93.4154\n",
      "\u001b[1m5625/5625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 76.7161 - mae: 5.4782 - mse: 76.3096 - val_loss: 93.8141 - val_mae: 5.9384 - val_mse: 93.4154\n"
     ]
    }
   ],
   "source": [
    "# Mixup Data Augmentation for Tabular Data\n",
    "import numpy as np\n",
    "\n",
    "def mixup(X, y, alpha=0.2):\n",
    "    '''Mixup augmentation for tabular data.'''\n",
    "    n_samples = X.shape[0]\n",
    "    lam = np.random.beta(alpha, alpha, n_samples)\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    X_mix = lam[:, None] * X + (1 - lam)[:, None] * X[idx]\n",
    "    y_mix = lam * y + (1 - lam) * y[idx]\n",
    "    return X_mix, y_mix\n",
    "\n",
    "# Example usage:\n",
    "X_train_mix, y_train_mix = mixup(X_train_fe, y_train, alpha=0.2)\n",
    "print(f\"Mixup sample shapes: {X_train_mix.shape}, {y_train_mix.shape}\")\n",
    "# Train model on mixup data\n",
    "mixup_model = build_deep_residual_mlp(X_train_mix.shape[1], n_layers=6, units=256)\n",
    "history_mixup = mixup_model.fit(X_train_mix, y_train_mix, validation_data=(X_test_fe, y_test), epochs=30, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2813/2813\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 716us/step\n",
      "\n",
      "Mixup Model Evaluation:\n",
      "  MAE: 5.9384\n",
      "  RMSE: 9.6652\n",
      "  R²: 0.4400\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Mixup Model Performance\n",
    "y_pred_mixup = mixup_model.predict(X_test_fe).flatten()\n",
    "mae_mixup = mean_absolute_error(y_test, y_pred_mixup)\n",
    "rmse_mixup = np.sqrt(mean_squared_error(y_test, y_pred_mixup))\n",
    "r2_mixup = r2_score(y_test, y_pred_mixup)\n",
    "\n",
    "print(f\"\\nMixup Model Evaluation:\\n  MAE: {mae_mixup:.4f}\\n  RMSE: {rmse_mixup:.4f}\\n  R²: {r2_mixup:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9e462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24c3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neural-pricer venv)",
   "language": "python",
   "name": "neural-pricer-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
